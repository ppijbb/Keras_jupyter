{"cells":[{"cell_type":"markdown","metadata":{"id":"3nhCjcehGczs"},"source":["<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/scikit/Simple_Scikit_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","<!--- @wandbcode{simple-sklearn} -->"]},{"cell_type":"markdown","metadata":{"id":"a7qGIaxwGczv"},"source":["<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n","\n","<!--- @wandbcode{simple-sklearn} -->\n","\n","# üèãÔ∏è‚Äç‚ôÄÔ∏è W&B + üß™ Scikit-learn\n","Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n","\n","\n","<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n","\n","\n","## What this notebook covers:\n","* Easy integration of Weights and Biases with Scikit.\n","* W&B Scikit plots for model interpretation and diagnostics for regression, classification, and clustering.\n","\n","**Note**: Sections starting with _Step_ are all you need to integrate W&B to existing code.\n","\n","\n","## The interactive W&B Dashboard will look like this:\n","\n","![](https://i.imgur.com/F1ZgR4A.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PH3vj99Gczv"},"outputs":[],"source":["import warnings\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.cluster import KMeans\n","from sklearn import datasets, cluster\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from sklearn.exceptions import ConvergenceWarning\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"]},{"cell_type":"markdown","metadata":{"id":"LDLf86--Gczw"},"source":["## Step 0: Install W&B"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vf3kmOMZGczx"},"outputs":[],"source":["!pip install wandb -qU"]},{"cell_type":"markdown","metadata":{"id":"NFOYvwrSGczx"},"source":["## Step 1: Import W&B and Login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PL5q3RIaGczx"},"outputs":[],"source":["import wandb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZskR-jPGczy"},"outputs":[],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"by-HTtHZGczy"},"source":["# Regression\n","\n","**Let's check out a quick example**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtIfiUbpGczy"},"outputs":[],"source":["# Load data\n","housing = datasets.fetch_california_housing()\n","X = pd.DataFrame(housing.data, columns=housing.feature_names)\n","y = housing.target\n","X, y = X[::2], y[::2]  # subsample for faster demo\n","wandb.errors.term._show_warnings = False\n","# ignore warnings about charts being built from subset of data\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","# Train model, get predictions\n","reg = Ridge()\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"bEfYCS5yGczz"},"source":["## Step 2: Initialize W&B run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yP3kUZc7Gczz"},"outputs":[],"source":["run = wandb.init(project='my-scikit-integration', name=\"regression\")"]},{"cell_type":"markdown","metadata":{"id":"YZ-ng9sKGczz"},"source":["## Step 3: Visualize model performance"]},{"cell_type":"markdown","metadata":{"id":"h-P2chtHGczz"},"source":["### Residual Plot\n","\n","Measures and plots the predicted target values (y-axis) vs the difference between actual and predicted target values (x-axis), as well as the distribution of the residual error.\n","\n","Generally, the residuals of a well-fit model should be randomly distributed because good models will account for most phenomena in a data set, except for random error.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#residuals-plot)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6XdM7ZiGczz"},"outputs":[],"source":["wandb.sklearn.plot_residuals(reg, X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"P1qL0QFOGcz0"},"source":["### Outlier Candidate\n","\n","Measures a datapoint's influence on regression model via Cook's distance. Instances with heavily skewed influences could potentially be outliers. Useful for outlier detection.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#outlier-candidates-plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbeloWlHGcz0"},"outputs":[],"source":["wandb.sklearn.plot_outlier_candidates(reg, X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"5ro1QBZIGcz0"},"source":["## All-in-one: Regression plot\n","\n","Using this all in one API one can:\n","* Log summary of metrics\n","* Log learning curve\n","* Log outlier candidates\n","* Log residual plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzZroiK9Gcz0"},"outputs":[],"source":["wandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, model_name='Ridge')\n","\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"2d7PYIH3Gcz0"},"source":["# Classification\n","\n","**Let's check out a quick example.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECtiBZuWGcz0"},"outputs":[],"source":["# Load data\n","wbcd = wisconsin_breast_cancer_data = datasets.load_breast_cancer()\n","feature_names = wbcd.feature_names\n","labels = wbcd.target_names\n","\n","X_train, X_test, y_train, y_test = train_test_split(wbcd.data, wbcd.target, test_size=0.2)\n","\n","\n","# Train model, get predictions\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","y_probas = model.predict_proba(X_test)\n","importances = model.feature_importances_\n","indices = np.argsort(importances)[::-1]"]},{"cell_type":"markdown","metadata":{"id":"bBDVVLyiGcz0"},"source":["## Step 2: Initialize W&B run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kebr8fMZGcz0"},"outputs":[],"source":["run = wandb.init(project='my-scikit-integration', name=\"classification\")"]},{"cell_type":"markdown","metadata":{"id":"OYuNH32DGcz0"},"source":["## Step 3: Visualize model performance"]},{"cell_type":"markdown","metadata":{"id":"QWvQyq3lGcz0"},"source":["### Class Proportions\n","\n","Plots the distribution of target classes in training and test sets. Useful for detecting imbalanced classes and ensuring that one class doesn't have a disproportionate influence on the model.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#class-proportions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gEmv33aGcz0"},"outputs":[],"source":["wandb.sklearn.plot_class_proportions(y_train, y_test, labels)"]},{"cell_type":"markdown","metadata":{"id":"dwi2d8jFGcz1"},"source":["### Learning Curve\n","\n","Trains model on datasets of varying lengths and generates a plot of cross validated scores vs dataset size, for both training and test sets.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#learning-curve)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wW3EA1OXGcz1"},"outputs":[],"source":["wandb.sklearn.plot_learning_curve(model, X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"Ie25Rdo2Gcz1"},"source":["### ROC\n","\n","ROC curves plot true positive rate (y-axis) vs false positive rate (x-axis). The ideal score is a `TPR = 1` and `FPR = 0`, which is the point on the top left. Typically we calculate the area under the ROC curve (AUC-ROC), and the greater the AUC-ROC the better.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#roc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m0ra0O9Gcz1"},"outputs":[],"source":["wandb.sklearn.plot_roc(y_test, y_probas, labels)"]},{"cell_type":"markdown","metadata":{"id":"UEOOdCzlGcz1"},"source":["### Precision Recall Curve\n","\n","Computes the tradeoff between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#precision-recall-curve)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FUPbXQdGcz1"},"outputs":[],"source":["wandb.sklearn.plot_precision_recall(y_test, y_probas, labels)"]},{"cell_type":"markdown","metadata":{"id":"7ZYQ4rvRGcz1"},"source":["### Feature Importances\n","\n","Evaluates and plots the importance of each feature for the classification task. Only works with classifiers that have a `feature_importances_` attribute, like trees.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#feature-importances)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-sRNNyVGcz2"},"outputs":[],"source":["wandb.sklearn.plot_feature_importances(model);"]},{"cell_type":"markdown","metadata":{"id":"IpuLcxdbGcz2"},"source":["## All-in-one: Classifier Plot\n","\n","Using this all in one API one can:\n","* Log feature importance\n","* Log learning curve\n","* Log confusion matrix\n","* Log summary metrics\n","* Log class proportions\n","* Log calibration curve\n","* Log roc curve\n","* Log precision recall curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqVJro4HGcz2"},"outputs":[],"source":["wandb.sklearn.plot_classifier(model,\n","                              X_train, X_test,\n","                              y_train, y_test,\n","                              y_pred, y_probas,\n","                              labels,\n","                              is_binary=True,\n","                              model_name='RandomForest')\n","\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"7_id4Jg1Gcz2"},"source":["# Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t347ZzhGcz2"},"outputs":[],"source":["# Load data\n","iris = datasets.load_iris()\n","X, y = iris.data, iris.target\n","names = iris.target_names\n","\n","def get_label_ids(classes):\n","    return np.array([names[aclass] for aclass in classes])\n","labels = get_label_ids(y)\n","\n","# Train model\n","kmeans = KMeans(n_clusters=4, random_state=1)\n","cluster_labels = kmeans.fit_predict(X)"]},{"cell_type":"markdown","metadata":{"id":"EQ7l2FayGcz2"},"source":["## Step 2: Initialize W&B run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtMFf-8yGcz6"},"outputs":[],"source":["run = wandb.init(project='my-scikit-integration', name=\"clustering\")"]},{"cell_type":"markdown","metadata":{"id":"vbt5OyO3Gcz6"},"source":["## Step 3: Visualize model performance"]},{"cell_type":"markdown","metadata":{"id":"YuxCddupGcz6"},"source":["### Elbow Plot\n","\n","Measures and plots the percentage of variance explained as a function of the number of clusters, along with training times. Useful in picking the optimal number of clusters.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#elbow-plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEzPDFR3Gcz6"},"outputs":[],"source":["wandb.sklearn.plot_elbow_curve(kmeans, X)"]},{"cell_type":"markdown","metadata":{"id":"LST2BrVJGcz6"},"source":["### Silhouette Plot\n","\n","Measures & plots how close each point in one cluster is to points in the neighboring clusters. The thickness of the clusters corresponds to the cluster size. The vertical line represents the average silhouette score of all the points.\n","\n","[Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/integrations/scikit#silhouette-plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG7w99dQGcz6"},"outputs":[],"source":["wandb.sklearn.plot_silhouette(kmeans, X, labels)"]},{"cell_type":"markdown","metadata":{"id":"8Pru0PDRGcz6"},"source":["## All in one: Clusterer Plot\n","\n","Using this all-in-one API you can:\n","* Log elbow curve\n","* Log silhouette plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSBRrTSPGcz6"},"outputs":[],"source":["wandb.sklearn.plot_clusterer(kmeans, X, cluster_labels, labels, 'KMeans')\n","\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"JGE3z9I1Gcz7"},"source":["# Sweep 101\n","\n","Use Weights & Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.\n","\n","## [Check out Hyperparameter Optimization in PyTorch using W&B Sweeps $\\rightarrow$](http://wandb.me/sweeps-colab)\n","\n","Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n","\n","1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n","\n","2. **Initialize the sweep:**\n","`sweep_id = wandb.sweep(sweep_config)`\n","\n","3. **Run the sweep agent:**\n","`wandb.agent(sweep_id, function=train)`\n","\n","And voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.\n","\n","<img src=\"https://imgur.com/sdQXdDz.png\" alt=\"Sweep Result\" />\n"]},{"cell_type":"markdown","metadata":{"id":"j1hoQy2yGcz7"},"source":["# Example Gallery\n","\n","See examples of projects tracked and visualized with W&B in our gallery, [Fully Connected ‚Üí](https://wandb.me/fc)"]},{"cell_type":"markdown","metadata":{"id":"kXUiAEuqGcz7"},"source":["# Basic Setup\n","1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n","2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group='experiment-1')`\n","3. **Tags**: Add tags to track your current baseline or production model.\n","4. **Notes**: Type notes in the table to track the changes between runs.\n","5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects."]},{"cell_type":"markdown","metadata":{"id":"cZPaSLi3Gcz7"},"source":["# Advanced Setup\n","1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n","2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n","3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/wandb/examples/blob/master/colabs/scikit/Simple_Scikit_Integration.ipynb","timestamp":1720153038365}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}