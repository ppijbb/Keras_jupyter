{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n"],"metadata":{"id":"6x0QOAp4v7jd"}},{"cell_type":"markdown","metadata":{"id":"VcMoz_CLlWcW"},"source":["TabNet 사용시 제 성능을 내지 못하는 이유를 찾기 위해 처음부터 훑어보기"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nTo6loxkTyd9","executionInfo":{"status":"ok","timestamp":1698805382740,"user_tz":-540,"elapsed":134502,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["%%capture\n","!pip install -U pytorch-tabnet lightning wget jsonargparse[signatures]>=4.18.0\n","!pip install -U \\\n","    cloud-tpu-client==0.10 \\\n","    https://storage.googleapis.com/tpu-pytorch/tmp/colab_tmp_whl/torch_xla-2.0.0.dev20230516+colab-cp310-cp310-linux_x86_64.whl\n","!pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n","# !pip install -U \\\n","#     cloud-tpu-client==0.10 \\\n","#     https://storage.googleapis.com/tpu-pytorch/lsiyuan-experiment/wheel/torch_xla-2.1.0-cp310-cp310-linux_x86_64.whl"]},{"cell_type":"markdown","metadata":{"id":"PUhx1vM5jCme"},"source":["TabNet Tutorial 따라하기"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5274,"status":"ok","timestamp":1698805642434,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6xFqRi5zbvqk","outputId":"6cb3b522-148b-480d-e6d0-fd868913f7d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0+cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","import torch\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import roc_auc_score\n","\n","import pandas as pd\n","import numpy as np\n","np.random.seed(0)\n","\n","import scipy\n","\n","import os\n","import wget\n","from pathlib import Path\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import os\n","import torch\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = f\"1\"\n","torch.__version__"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":661,"status":"ok","timestamp":1698805643093,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"WetUcZlRjqnJ","outputId":"ce032efa-439e-43e9-8526-e69fcab9237f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading file...\n"]}],"source":["url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n","dataset_name = 'census-income'\n","out = Path(os.getcwd()+'/data/'+dataset_name+'.csv')\n","\n","out.parent.mkdir(parents=True, exist_ok=True)\n","if out.exists():\n","    print(\"File already exists.\")\n","else:\n","    print(\"Downloading file...\")\n","    wget.download(url, out.as_posix())"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1698805643513,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"4YZTjMPkjxzx","outputId":"9e6ec400-ae59-47f3-cd1c-84265723582b"},"outputs":[{"output_type":"stream","name":"stdout","text":["32560\n","39 73\n"," State-gov 9\n"," Bachelors 16\n"," 13 16\n"," Never-married 7\n"," Adm-clerical 15\n"," Not-in-family 6\n"," White 5\n"," Male 2\n"," 2174 119\n"," 0 92\n"," 40 94\n"," United-States 42\n"," <=50K 2\n","Set 3\n"]}],"source":["train = pd.read_csv(out)\n","print(len(train))\n","target = ' <=50K'\n","if \"Set\" not in train.columns:\n","    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n","\n","train_indices = train[train.Set==\"train\"].index\n","valid_indices = train[train.Set==\"valid\"].index\n","test_indices = train[train.Set==\"test\"].index\n","\n","nunique = train.nunique()\n","types = train.dtypes\n","\n","categorical_columns = []\n","categorical_dims =  {}\n","for col in train.columns:\n","    if types[col] == 'object' or nunique[col] < 200:\n","        print(col, train[col].nunique())\n","        l_enc = LabelEncoder()\n","        train[col] = train[col].fillna(\"VV_likely\")\n","        train[col] = l_enc.fit_transform(train[col].values)\n","        categorical_columns.append(col)\n","        categorical_dims[col] = len(l_enc.classes_)\n","    else:\n","        train.fillna(train.loc[train_indices, col].mean(), inplace=True)\n","# check that pipeline accepts strings\n","# train.loc[train[target]==0, target] = \"wealthy\"\n","# train.loc[train[target]==1, target] = \"not_wealthy\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1698805643851,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6v-oLA6Mj36p","outputId":"838ec1da-0df4-4edc-d030-59aaa7774698"},"outputs":[{"output_type":"stream","name":"stdout","text":["             all features 14\n","             categoricals [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","   categorical dimensions [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42]\n"]}],"source":["unused_feat = ['Set']\n","\n","features = [ col for col in train.columns if col not in unused_feat+[target]]\n","# 카테고리 index\n","cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","# 카테고리별 class 개수\n","cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","\n","print(\"all features\".rjust(25,\" \"), len(features))\n","print(\"categoricals\".rjust(25,\" \"), cat_idxs)\n","print(\"categorical dimensions\".rjust(25,\" \") , cat_dims)\n","grouped_features = [[0, 1, 2], [8, 9, 10]]\n","\n","train.loc[train_indices, features+[target]].to_csv(\"train.csv\", index=False)\n","train.loc[valid_indices, features+[target]].to_csv(\"valid.csv\", index=False)\n","train.loc[test_indices, features+[target]].to_csv(\"test.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698627040857,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"h_WpyooYj9EK","outputId":"8d80e5bd-ef30-4c53-f351-cb28261a7bb8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]}],"source":["# steps, n_shared, n_independce 수정하지 않았음\n","tabnet_params = {\n","    \"cat_idxs\":cat_idxs,\n","    \"cat_dims\":cat_dims,\n","    \"cat_emb_dim\":2,\n","    \"n_d\": 16,\n","    \"n_a\": 16,\n","    \"n_independent\": 9,\n","    \"n_shared\": 4,\n","    \"n_steps\": 2,\n","    \"gamma\": 1.4690246460970766,\n","    \"lambda_sparse\": 0,\n","    \"optimizer_fn\": torch.optim.Adam,\n","    \"optimizer_params\":dict(lr=2e-2),\n","    \"scheduler_params\":{\n","        \"step_size\":50, # how to use learning rate scheduler\n","        \"gamma\":0.9},\n","    \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n","    \"mask_type\": 'entmax', # \"sparsemax\"\n","    \"grouped_features\" : grouped_features\n","   }\n","\n","clf = TabNetClassifier(**tabnet_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpJ4E0TUkHfN"},"outputs":[],"source":["X_train = train[features].values[train_indices]\n","y_train = train[target].values[train_indices]\n","\n","X_valid = train[features].values[valid_indices]\n","y_valid = train[target].values[valid_indices]\n","\n","X_test = train[features].values[test_indices]\n","y_test = train[target].values[test_indices]"]},{"cell_type":"markdown","metadata":{"id":"_9_oHkNflf9R"},"source":["놀랍게도 agumentations를 지원하고 있었다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ-Nl1pikIg5"},"outputs":[],"source":["max_epochs = 50 if not os.getenv(\"CI\", False) else 2\n","\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","aug = ClassificationSMOTE(p=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617648,"status":"ok","timestamp":1698627658503,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"CT0D2yDCkNt-","outputId":"8cdeb641-f53d-459e-a399-589d962baf49"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:25s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:46s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:58s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:01:09s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:01:22s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:01:32s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:01:45s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:01:55s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:02:12s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:02:25s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:02:37s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:02:48s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:02:59s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:03:11s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:03:22s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:03:35s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:03:45s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:03:59s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:04:08s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:04:22s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:04:31s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:04:54s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:05:08s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:05:17s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:05:30s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:05:39s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:05:53s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:06:02s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:06:15s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:06:24s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:06:38s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:06:47s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:07:00s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:07:10s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:07:22s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:07:32s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:07:44s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:07:55s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:08:07s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:08:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:08:29s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:08:41s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:08:52s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:09:04s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:09:15s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:09:27s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:09:37s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:09:50s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:10:00s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:10:13s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the behaviour of the model's fit method using Compressed Sparse Row matrices\n","sparse_X_train = scipy.sparse.csr_matrix(X_train)  # Create a CSR matrix from X_train\n","sparse_X_valid = scipy.sparse.csr_matrix(X_valid)  # Create a CSR matrix from X_valid\n","\n","# Fitting the model\n","clf.fit(\n","    X_train=sparse_X_train, y_train=y_train,\n","    eval_set=[(sparse_X_train, y_train), (sparse_X_valid, y_valid)],\n","    eval_name=['train', 'valid'],\n","    eval_metric=['auc'],\n","    max_epochs=max_epochs , patience=20,\n","    batch_size=1024, virtual_batch_size=128,\n","    num_workers=0,\n","    weights=1,\n","    drop_last=False,\n","    augmentations=aug, #aug, None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":979090,"status":"ok","timestamp":1698628637591,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jTvaq1hVkRou","outputId":"bf6ffce5-f571-40a4-d2ce-a0b8822a404c"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:14s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:20s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:33s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:39s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:58s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:03s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:16s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:22s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:40s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:46s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:05s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:23s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:29s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:41s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:54s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:01s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:12s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:21s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:30s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:37s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:55s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:01s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:38s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:48s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:52s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:56s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:01:02s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:08s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:12s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:16s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:21s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:27s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:32s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:36s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:40s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:46s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:52s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:56s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:02:00s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:02:05s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:11s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:16s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:20s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:24s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:29s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:36s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:40s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:44s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:48s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:54s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:03:00s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:04s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:12s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:27s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:31s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:37s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:43s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:47s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:51s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:55s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:04:01s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:07s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:13s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:19s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:37s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:56s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:02s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:21s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:39s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:45s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:55s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:04s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:15s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:19s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:28s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:42s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:53s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:02s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:11s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:22s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:29s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:35s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:54s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:00s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:04s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:36s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:42s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:55s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:00s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:06s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:19s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:25s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:38s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:43s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:50s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:58s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:02s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:08s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:26s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:32s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:46s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:52s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:58s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:03s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:11s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:17s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:26s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:30s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:35s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:41s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:46s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:50s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:54s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:59s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:06s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the warm_start=False behaviour\n","save_history = []\n","\n","# Fitting the model without starting from a warm start nor computing the feature importance\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=False\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))\n","\n","save_history = []  # Resetting the list to show that it also works when computing feature importance\n","\n","# Fitting the model without starting from a warm start but with the computing of the feature importance activated\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=True # True by default so not needed\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uq7dwumBlr7z"},"outputs":[],"source":["# plot losses\n","plt.plot(clf.history['loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A52swQzcls_U"},"outputs":[],"source":["# plot auc\n","plt.plot(clf.history['train_auc'])\n","plt.plot(clf.history['valid_auc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qY0wTE3lt5k"},"outputs":[],"source":["# plot learning rates\n","plt.plot(clf.history['lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbZ5ydVIlvs4"},"outputs":[],"source":["preds = clf.predict_proba(X_test)\n","test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n","\n","\n","preds_valid = clf.predict_proba(X_valid)\n","valid_auc = roc_auc_score(y_score=preds_valid[:,1], y_true=y_valid)\n","\n","print(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")\n","# check that best weights are used\n","assert np.isclose(valid_auc, np.max(clf.history['valid_auc']), atol=1e-6)\n","clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBrjfLhbl02o"},"outputs":[],"source":["# save tabnet model\n","saving_path_name = \"./tabnet_model_test_1\"\n","saved_filepath = clf.save_model(saving_path_name)\n","\n","# define new model with basic parameters and load state dict weights\n","loaded_clf = TabNetClassifier()\n","loaded_clf.load_model(saved_filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9kfTTcAl4RY"},"outputs":[],"source":["loaded_preds = loaded_clf.predict_proba(X_test)\n","loaded_test_auc = roc_auc_score(y_score=loaded_preds[:,1], y_true=y_test)\n","\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_auc}\")\n","\n","assert(test_auc == loaded_test_auc)\n","loaded_clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IalieiADl8SZ"},"outputs":[],"source":["# Global explainability : feat importance summing to 1\n","clf.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3sio5i_l-XO"},"outputs":[],"source":["explain_matrix, masks = clf.explain(X_test)\n","fig, axs = plt.subplots(1, 3, figsize=(20,20))\n","\n","for i in range(3):\n","    axs[i].imshow(masks[i][:50])\n","    axs[i].set_title(f\"mask {i}\")\n","    axs[i].set_xticklabels(labels = features, rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"Xcqx5Qygjm_E"},"source":["Lightning 으로 네트워크 학습하기"]},{"cell_type":"code","execution_count":23,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836,"status":"ok","timestamp":1698814828108,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jizvAZvn1Ppo","outputId":"260daf19-f784-48f2-c32b-a8c0e1a6a585"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting tabnet.py\n"]}],"source":["%%writefile tabnet.py\n","# @title <b><i>Training 파일 저장<i/></b>\n","import argparse\n","import logging\n","import os\n","import gc\n","import sys\n","import time\n","import pickle\n","from typing import Any, Dict, Optional, Type\n","# import dgl\n","import numpy as np\n","import pandas as pd\n","# import deepchem as dc\n","\n","import torch\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import LabelEncoder\n","from torchmetrics import F1Score, AUROC\n","\n","from torch.optim import SGD, Adam, NAdam, RAdam, SparseAdam, LBFGS, RMSprop\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CyclicLR\n","# from torchmetrics.text.bleu import BLEUScore\n","# from dgl.dataloading import GraphDataLoader\n","from pytorch_tabnet.tab_network import TabNetPretraining, TabNet\n","from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n","from pytorch_tabnet.utils import create_group_matrix, create_explain_matrix\n","from lightning.fabric.utilities import apply_func\n","from torchsummary import summary\n","\n","import lightning.pytorch as pl\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from lightning.pytorch.callbacks import (\n","    TQDMProgressBar, EarlyStopping, LearningRateMonitor,\n","    ModelCheckpoint, StochasticWeightAveraging, DeviceStatsMonitor)\n","from lightning.pytorch.plugins.io import XLACheckpointIO\n","from lightning.pytorch.cli import LightningCLI, ArgsType\n","from lightning_utilities.core.imports import module_available\n","\n","sys.setrecursionlimit(10**7)\n","sys.set_int_max_str_digits(0)\n","isxla = module_available(\"torch_xla\")\n","print(f'Can Use Torch_XLA? {isxla}')\n","if isxla:\n","    import torch_xla\n","    import torch_xla.core.xla_model as xm\n","    import torch_xla.distributed.xla_multiprocessing as xmp\n","    import torch_xla.utils.serialization as xser\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.WARNING)\n","\n","\n","class ArgsCLI(LightningCLI):\n","    def add_arguments_to_parser(self, parser):\n","        parser.add_argument('--train_file',\n","                            type=str,\n","                            default='dev_t.csv',\n","                            help='train file')\n","\n","        parser.add_argument('--test_file',\n","                            type=str,\n","                            default='dev_v.csv',\n","                            help='test file')\n","\n","\n","        parser.add_argument('--accumulate_grad_batches',\n","                            type=int,\n","                            default=1,\n","                            help='accumulate_grad_batches')\n","\n","        parser.add_argument('--num_workers',\n","                            type=int,\n","                            default=2,\n","                            help='num of worker for dataloader')\n","\n","        parser.add_argument('--batch_size',\n","                            type=int,\n","                            default=32,\n","                            help='batch size for training (default: 96)')\n","\n","        parser.add_argument('--lr',\n","                            type=float,\n","                            default=5e-7,\n","                            help='The initial learning rate')\n","\n","        parser.add_argument('--warmup_ratio',\n","                            type=float,\n","                            default=0.1,\n","                            help='warmup ratio')\n","\n","\n","        parser.add_argument('--checkpoint_path',\n","                            type=str,\n","                            help='checkpoint path')\n","\n","        parser.add_argument('--default_root_dir',\n","                            type=str,\n","                            help='default_root_dir')\n","\n","\n","class CensusDataset(Dataset):\n","    def __init__(self,\n","                 filepath: str,) -> None:\n","        super().__init__()\n","        self.data = pd.read_csv(filepath)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def update_attributes(self, features, idxs, dims):\n","        self.features=features\n","        self.cat_ids = idxs\n","        self.cat_dims = dims\n","\n","    def make_input_id_mask(self,\n","                           tokens):\n","        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = [1] * len(input_id)\n","        if len(input_id) < self.max_seq_len:\n","            while len(input_id) < self.max_seq_len:\n","                input_id += [self.tokenizer.pad_token_id]\n","                attention_mask += [0]\n","        else:\n","            input_id = input_id[:self.max_seq_len - 1] + [self.tokenizer.eos_token_id]\n","            attention_mask = attention_mask[:self.max_seq_len]\n","        return input_id, attention_mask\n","\n","    def masking(self,\n","                input,\n","                mask):\n","        input = torch.tensor([input])\n","        rand = torch.rand(input.shape)\n","        mask_arr = (rand < 0.15) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n","        mask_ids = torch.flatten((mask_arr[0]).nonzero()).tolist()\n","        input[0, mask_ids] = mask\n","        del rand, mask_arr\n","        return input.tolist()[0]\n","\n","    def _labeling(self,\n","                  label):\n","        tokens = [self.tokenizer.bos_token]+self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n","        label_ids = self.tokenizer.convert_tokens_to_ids(tokens[1:])\n","        if len(label_ids) < self.max_seq_len:\n","            while len(label_ids)<self.max_seq_len:\n","                label_ids+=[-100]\n","        else:\n","            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n","        del tokens\n","        return label_ids\n","\n","    def _scale_transform(self, values):\n","        return self.scaler.transform(values.reshape(1,-1))\n","\n","    def __getitem__(self,\n","                    index:int):\n","        assert self.features\n","        record = self.data.iloc[index]\n","        dod  = record[' <=50K']\n","        tabs = record[self.features].values\n","        return {\n","            'tabnet_input': tabs.astype('float32'),\n","            'labels': np.array(dod, dtype=np.float32)\n","        }\n","\n","\n","class CensusDataModule(pl.core.LightningDataModule):\n","    def __init__(self,\n","                 train_file: str,\n","                 valid_file: str,\n","                 test_file: str,\n","                 batch_size: int = 32,\n","                 num_workers: int = 8):\n","        super().__init__()\n","        self.batch_size = batch_size # batch_size\n","        self.train_file_path = train_file\n","        self.valid_file_path = valid_file\n","        self.test_file_path = test_file\n","        self.num_workers = num_workers\n","        self.prepare_data_pre_node = True\n","    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n","\n","    def setup(self,\n","              stage):\n","        # split dataset\n","        self.train = CensusDataset(self.train_file_path)\n","        self.valid = CensusDataset(self.valid_file_path)\n","        self.test = CensusDataset(self.test_file_path)\n","        features, idxs, dims = self.get_features()\n","        self.train.update_attributes(features, idxs, dims)\n","        self.valid.update_attributes(features, idxs, dims)\n","        self.test.update_attributes(features, idxs, dims)\n","\n","    def get_features(self):\n","        df = pd.concat([self.train.data, self.valid.data, self.test.data])\n","        target = ' <=50K'\n","        nunique = df.nunique()\n","        types = df.dtypes\n","        categorical_columns = []\n","        categorical_dims =  {}\n","        for col in df.columns:\n","            if types[col] == 'object' or nunique[col] < 200:\n","                l_enc = LabelEncoder()\n","                df[col] = df[col].fillna(\"VV_likely\")\n","                df[col] = l_enc.fit_transform(df[col].values)\n","                categorical_columns.append(col)\n","                categorical_dims[col] = len(l_enc.classes_)\n","            else:\n","                df.fillna(df.median(), inplace=True)\n","        unused_feat = ['Set']\n","        features = [ col for col in df.columns if col not in unused_feat+[target]]\n","        cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","        cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","        return features, cat_idxs, cat_dims\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=True,\n","                          drop_last=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.valid,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","\n","\n","class Base(pl.core.LightningModule):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs) -> None:\n","        super().__init__()\n","        self.lr = lr\n","        self.warmup_ratio = warmup\n","        self.batch_size = batch_size\n","\n","    def decay_params(self, named_params):\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'BatchNorm1d.bias', 'BatchNorm1d.weight']\n","        return [\n","            {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","    def configure_optimizers(self):\n","        # Prepare optimizer\n","\n","        # optimizer_grouped_parameters = self.network.net.parameters()\n","        optimizer_grouped_parameters = self.decay_params(self.network.net.named_parameters())\n","\n","        optimizer = Adam(params=self.parameters(),\n","                         lr=self.lr,)\n","        # optimizer = RMSprop(params=optimizer_grouped_parameters,\n","        #                     momentum=0.9,\n","        #                     lr=self.lr)\n","\n","        # warm up lr\n","        num_nodes = self.trainer.strategy.num_nodes if not \"Single\" in str(type(self.trainer.strategy)) else 1\n","        devices = len(self.trainer.strategy.parallel_devices) if \"parallel_devices\" in self.trainer.strategy.__dict__.keys() else 1\n","        num_devices = int(devices) * int(num_nodes)\n","        data_len = len(self.trainer.fit_loop._data_source.instance.train)\n","       # data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n","        logging.info(f'number of devices {num_devices}, data length {data_len}')\n","        first = (data_len / (self.batch_size * num_devices))\n","        num_train_steps = int(first * self.trainer.max_epochs)\n","        logging.info(f'num_train_steps : {num_train_steps}')\n","\n","        num_warmup_steps = int(num_train_steps * 0.01)\n","        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n","\n","        scheduler = CyclicLR(\n","                 optimizer=optimizer,\n","                 base_lr=5e-5,\n","                 max_lr=self.lr,\n","                 cycle_momentum=False,\n","                 step_size_up=num_warmup_steps,)\n","\n","        self.lr_scheduler = {\n","            \"scheduler\": scheduler,\n","            \"name\": \"lr_log\",\n","            \"monitor\": \"loss\",\n","            \"interval\": \"step\",\n","            \"frequency\": 1}\n","        # return optimizer\n","        return [optimizer], [self.lr_scheduler]\n","\n","   # def backward(self, loss):\n","       # with torch.autograd.set_detect_anomaly(True):\n","       # loss.to(self.device)\n","       # loss.requires_grad_()\n","       # loss.backward()\n","\n","\n","\n","class Tabnet(torch.nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 output_dim: int,\n","                 batch_size: int,\n","                 cat_idx:list,\n","                 cat_dim:list,\n","                 device: str = \"cpu\"):\n","        super(Tabnet, self).__init__()\n","        self.cat_idx = cat_idx\n","        self.cat_dim = cat_dim\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.batch_size = batch_size\n","        self.net = TabNet(\n","            cat_idxs=cat_idx,\n","            cat_dims=cat_dim,\n","            cat_emb_dim=[3]*len(cat_idx),\n","            input_dim=input_dim,\n","            output_dim=output_dim,\n","            n_a=2, # Dimension of the prediction layer\n","            n_d=2, # Dimension of the attention layer\n","            n_steps=3, # Output까지의 Attentive-Feature transformer layer\n","            n_independent=1, # Decision step dependent 단계 GLU layer\n","            n_shared=1, # Shared across decision steps 단계 GLU layer\n","            epsilon=1e-5,\n","            gamma=1.3,\n","            mask_type=\"sparsemax\", # sparsemax / entmax\n","            momentum=0.01,\n","            group_attention_matrix=torch.eye(input_dim, device=device))\n","        self.reducing_matrix = create_explain_matrix(\n","                    self.net.input_dim,\n","                    self.net.cat_emb_dim,\n","                    self.net.cat_idxs,\n","                    self.net.post_embed_dim,\n","                )\n","\n","    def _re_init(self, device, state_dict=None):\n","       # if \"xla\" in device:\n","        del self.net, self.reducing_matrix\n","        self.__init__(\n","                input_dim=self.input_dim,\n","                output_dim=self.output_dim,\n","                batch_size=self.batch_size,\n","                cat_idx=self.cat_idx,\n","                cat_dim=self.cat_dim,\n","                device=device)\n","        if state_dict:\n","            self.load_state_dict(state_dict)\n","        self.to(device)\n","\n","    def forward(self, x):\n","        output, loss = self.net(x)\n","        return output, loss\n","\n","\n","class LightningTabNet(Base):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs):\n","        super().__init__(lr, warmup, batch_size, **kwargs)\n","        model_dim = 2 # @param {type:\"integer\"}\n","        cat_input_dim = 14 # @param {type:\"integer\"}\n","        self.network = Tabnet(\n","            input_dim=cat_input_dim,\n","            output_dim=model_dim,\n","            batch_size=batch_size,\n","            cat_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n","            cat_dim =  [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42])\n","        self.f1 = F1Score(task=\"binary\", num_classes=2)\n","        self.roc = AUROC(task=\"binary\", num_classes=2)\n","       # self.concat = torch.nn.Bilinear(model_dim, model_dim, model_dim,)\n","\n","    def _loss(self, yhat, y, M_loss):\n","        pred = yhat[:, 1].gt(0.5).float().to(self.device)\n","       # pred = F.log_softmax(yhat, dim=1).to(self.device)\n","        acc = torch.sum(torch.eq(pred, y))/ pred.shape[0]\n","        f1 = self.f1(pred, y)\n","        roc = self.roc(pred, y)\n","        clf_loss = F.cross_entropy(yhat, y, )\n","        clf_loss = clf_loss - (1.3 * M_loss)\n","        clf_loss.requires_grad_()\n","        clf_loss.to(self.device)\n","        return clf_loss, acc, f1, roc\n","\n","    def initialize(self, m):\n","        for layer in m.modules():\n","            if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.Bilinear):\n","                torch.nn.init.xavier_uniform_(layer.weight, gain=1.0)\n","\n","   # def teardown(self,\n","   #              trainer: pl.Trainer,\n","   #              pl_module: pl.LightningModule,\n","   #              stage: str) -> None:\n","   #     print(f\"{stage} Done\")\n","\n","    def on_fit_start(self,) -> None:\n","        self.network._re_init(self.device) # Multiprocessing 하는 경우 device에 올라가는 문제\n","       # self.lm.init_weights()\n","       # self.initialize(self.tabnet.net)\n","       # print(\"Training with Lightning Module\\n\")\n","\n","    def on_predict_start(self,) -> None:\n","        state_dict = self.network.state_dict()\n","       # self.tabnet._re_init(self.device, state_dict)\n","\n","   # def on_fit_end(self,) -> None:\n","   #     if xm.is_master_ordinal():\n","   #         print(\"\\nTrain Done\\n\")\n","   #         # save_hf_repo(self.model.cpu(), self.tokenizer)\n","\n","    def forward(self,\n","                inputs):\n","        output, M_loss = self.network(inputs[\"tabnet_input\"])\n","       # output = torch.sum(torch.nan_to_num(torch.stack(output)), dim=0)\n","        return output, M_loss\n","\n","   # def on_train_epoch_start(self):\n","   #     self.lm.train()\n","   #     self.tabnet.train()\n","\n","    def training_step(self,\n","                      batch,\n","                      batch_idx: int):\n","        outs, M_loss = self(batch)\n","        loss, dod_acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","\n","        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_dod_acc\", dod_acc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        return loss\n","\n","    def validation_step(self,\n","                        batch,\n","                        batch_idx: int):\n","        outs, M_loss = self(batch)\n","        loss, dod_acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_dod_acc\", dod_acc, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"val_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","\n","\n","    # def on_training_batch_end(self,\n","    #                           outputs,\n","    #                           batch,\n","    #                           batch_idx) -> None:\n","    #     print(\"Train Batch End\")\n","\n","    # def on_training_epoch_end(self,) -> None:\n","    #     print(\"Train Epoch End\")\n","\n","    # def validation_batch_end(self,\n","    #                          outputs,\n","    #                          batch,\n","    #                          batch_idx,\n","    #                          dataloader_idx=0) -> None:\n","    #     print(\"Valid Batch End\")\n","\n","    # def validation_epoch_end(self,) -> None:\n","    #     print(\"Valid Epoch End\")\n","\n","    # def on_save_checkpoint(self,\n","    #                        trainer: pl.Trainer,\n","    #                        pl_module: pl.LightningModule=None,\n","    #                        checkpoint: Dict[str,Any]=None) -> None:\n","    #     checkpoint[\"lighitning_logs/checkpoint\"]=self\n","    #     if xm.is_master_ordinal():\n","    #         print(f\"Trainer save checkpoint\")\n","    #         save_hf_repo(self.model.cpu(), self.model.tokenizer)\n","    #         print(\"\\nCheckpoint saved\\n\")\n","\n","\n","def cli_main() -> None: # args:ArgsType=None\n","    callbacks = [\n","        # ModelCheckpoint(monitor=\"val_loss\",\n","        #                dirpath=\"logs/\",\n","        #                filename=\"best-checkpoint\",\n","        #                verbose=True,\n","        #                save_last=True,\n","        #                mode=\"min\",\n","        #                save_top_k=1),\n","        # TensorBoardLogger(save_dir=\"lightning_logs/\"),\n","        # StochasticWeightAveraging(swa_lrs=1e-2), # 23/04/24 기준 lightning에서 오류 있는 callback\n","        # DeviceStatsMonitor(),\n","        # EarlyStopping(monitor=\"val_loss\",\n","        #               mode=\"min\",\n","        #               stopping_threshold=1e-4,\n","        #               min_delta=0.00,\n","        #               patience=1,\n","        #               divergence_threshold=9.0),\n","        LearningRateMonitor(logging_interval=\"step\"),\n","        TQDMProgressBar(refresh_rate=3)]\n","\n","    cli = ArgsCLI(model_class=LightningTabNet,\n","                  datamodule_class=CensusDataModule,\n","                  save_config_kwargs={\"overwrite\": True},\n","                  trainer_defaults={\n","                    # \"plugins\": [XLACheckpointIO(),],\n","                      \"callbacks\": callbacks,\n","                      \"reload_dataloaders_every_n_epochs\": 1,\n","                      \"detect_anomaly\": True,\n","                      },)\n","\n","if __name__ == \"__main__\":\n","    pl.cli_lightning_logo()\n","    cli_main()\n","\n","   # save_path = \"/content/drive/MyDrive/data_kevin/ChemGPT_finetuned.pth\"\n","   # torch.save(model.model.state_dict(),save_path)\n","    print(\"\\nMain Done...\\n\")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1698814838158,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"ibzfCMq_Mhhe","outputId":"c538be10-7427-40bd-f37c-5649ced9ac5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting trainer.sh\n"]}],"source":["%%writefile trainer.sh\n","TRAIN_FILE=\"train.csv\" #@param ['\"train.csv\"'] {type:\"raw\"}\n","VAL_FILE=\"valid.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","TEST_FILE=\"test.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","PERCISION=32 #@param [\"bf16\", \"16\", \"32\"] {type:\"raw\", allow-input: true}\n","LEARNING_RATE=2e-02 #@param {type:\"raw\"}\n","GRADIENT_CLIP=1.0 #@param {type:\"raw\"}\n","ACCUMULATE_GRAD_BATCHES=1 #@param {type:\"raw\"}\n","BATCH_SIZE=64 #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"512\", \"4096\", \"32768\", \"262144\", \"2097152\", \"16777216\", \"134217728\", \"1073741824\", \"8589934592\", \"68719476736\"] {type:\"raw\", allow-input: true}\n","MAXEPOCHS=100 #@param {type:\"raw\"}\n","STRATEGY=xla #@param [\"auto\", \"bagua\", \"colossalai\", \"collaborative\", \"ddp\", \"ddp2\", \"ddp_find_unused_parameters_false\", \"ddp_fully_sharded\", \"ddp_sharded\", \"ddp_sharded_find_unused_parameters_false\", \"ddp_sharded_spawn\", \"ddp_sharded_spawn_find_unused_parameters_false\", \"ddp_spawn\", \"ddp_spawn_find_unused_parameters_false\", \"ddp_fork\" ,\"deepspeed\", \"deepspeed_stage_1\", \"deepspeed_stage_2\", \"deepspeed_stage_2_offload\", \"deepspeed_stage_3\", \"deepspeed_stage_3_offload\", \"deepspeed_stage_3_offload_nvme\", \"dp\", \"fsdp\", \"fsdp_native\", \"horovod\", \"hpu_parallel\", \"hpu_single\", \"ipu_strategy\", \"single_device\", \"single_tpu\", \"tpu_spawn\", \"tpu_spawn_debug\", \"xla\", \"xla_debug\"]{type:\"raw\", allow-input: false}\n","NUMWORKERS=0 #@param {type:\"raw\"}\n","WARMUP=0.1 #@param {type:\"raw\"}\n","PROFILE=xla #@param [\"simple\", \"advanced\", \"pytorch\", \"xla\"]  {type:\"raw\", allow-input: false}\n","FAST_DEV_RUN=false #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","SYNC_BATCHNORM=true #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","ACCELERATOR=tpu #@param [\"gpu\", \"tpu\"] {type:\"raw\", allow-input: true}\n","CORES=8 #@param {type:\"raw\"}\n","\n","python tabnet.py fit\\\n","    --trainer.accumulate_grad_batches $ACCUMULATE_GRAD_BATCHES\\\n","    --trainer.gradient_clip_val $GRADIENT_CLIP\\\n","    --trainer.max_epochs $MAXEPOCHS\\\n","    --trainer.precision $PERCISION\\\n","    --trainer.accelerator $ACCELERATOR\\\n","    --trainer.devices $CORES\\\n","    --trainer.use_distributed_sampler false\\\n","    --trainer.sync_batchnorm $SYNC_BATCHNORM\\\n","    --trainer.profiler $PROFILE\\\n","    --trainer.strategy $STRATEGY\\\n","    --trainer.num_sanity_val_steps 3\\\n","    --trainer.log_every_n_steps 2\\\n","    --trainer.fast_dev_run $FAST_DEV_RUN\\\n","    --model.lr $LEARNING_RATE\\\n","    --model.batch_size $BATCH_SIZE\\\n","    --data.num_workers $NUMWORKERS\\\n","    --data.batch_size $BATCH_SIZE\\\n","    --model.warmup $WARMUP\\\n","    --data.train_file $TRAIN_FILE\\\n","    --data.valid_file $VAL_FILE\\\n","    --data.test_file $TEST_FILE\n","\n","exit 0"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRMpyxzTP6tn","executionInfo":{"status":"ok","timestamp":1698819410165,"user_tz":-540,"elapsed":664308,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"d422cb5e-be9e-4b03-f04a-92c5299a8643"},"outputs":[{"output_type":"stream","name":"stdout","text":["Can Use Torch_XLA? True\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 3897126576\n","Seed set to 3897126576\n","You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n","GPU available: False, used: False\n","TPU available: True, using: 8 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","2023-11-01 06:06:34.514355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","  | Name    | Type          | Params\n","------------------------------------------\n","0 | network | Tabnet        | 2.4 K \n","1 | f1      | BinaryF1Score | 0     \n","2 | roc     | BinaryAUROC   | 0     \n","------------------------------------------\n","2.4 K     Trainable params\n","0         Non-trainable params\n","2.4 K     Total params\n","0.010     Total estimated model params size (MB)\n","Epoch 0:   0% 0/407 [00:00<?, ?it/s] [rank: 1] Received SIGTERM: 15\n","[rank: 2] Received SIGTERM: 15\n","Exception in device=TPU:7: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Exception in device=TPU:3: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","Exception in device=TPU:6: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Exception in device=TPU:5: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","Exception in device=TPU:2: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Exception in device=TPU:4: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n","    fn(gindex, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 140, in _wrapping_function\n","    results = function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 581, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 990, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1036, in _run_stage\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n","    self.advance()\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n","    self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n","    self.advance(data_fetcher)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n","    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n","    self._optimizer_step(batch_idx, closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n","    call._call_lightning_module_hook(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/ddp.py\", line 263, in optimizer_step\n","    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n","RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'reduce': failed to connect to all addresses (14)\n","Traceback (most recent call last):\n","  File \"/content/tabnet.py\", line 525, in <module>\n","    cli_main()\n","  File \"/content/tabnet.py\", line 513, in cli_main\n","    cli = ArgsCLI(model_class=LightningTabNet,\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py\", line 386, in __init__\n","    self._run_subcommand(self.subcommand)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n","    fn(**fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 545, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n","    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/xla.py\", line 108, in launch\n","    while not process_context.join():\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 140, in join\n","    raise ProcessExitedException(\n","torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL\n"]}],"source":["!sh trainer.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R28jUmPkSd5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"mount_file_id":"1FlTETQ-ZBuYRL1EaCJcrEyIPM0c_uIYw","authorship_tag":"ABX9TyM/2yeHLlga41qX/FdK7c/l"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}