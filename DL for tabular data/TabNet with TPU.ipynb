{"cells":[{"cell_type":"markdown","metadata":{"id":"6x0QOAp4v7jd"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"VcMoz_CLlWcW"},"source":["nn module 에서 TabNet 사용 시 제 성능을 내지 못하는 이유를 찾기 위해 처음부터 훑어보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTo6loxkTyd9"},"outputs":[],"source":["%%capture\n","!pip install -U pytorch-tabnet lightning wandb wget jsonargparse[signatures]>=4.18.0\n","!pip install -U \\\n","    cloud-tpu-client==0.10 \\\n","    https://storage.googleapis.com/tpu-pytorch/tmp/colab_tmp_whl/torch_xla-2.0.0.dev20230516+colab-cp310-cp310-linux_x86_64.whl\n","!pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n","# !pip install -U \\\n","#     cloud-tpu-client==0.10 \\\n","#     https://storage.googleapis.com/tpu-pytorch/lsiyuan-experiment/wheel/torch_xla-2.1.0-cp310-cp310-linux_x86_64.whl"]},{"cell_type":"markdown","metadata":{"id":"PUhx1vM5jCme"},"source":["TabNet Tutorial 따라하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":3162,"status":"ok","timestamp":1699837370992,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6xFqRi5zbvqk","outputId":"08dca429-500b-4a0e-97ff-64b301b275e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0+cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","import torch\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import roc_auc_score\n","\n","import pandas as pd\n","import numpy as np\n","np.random.seed(0)\n","\n","import scipy\n","\n","import os\n","import wget\n","from pathlib import Path\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import os\n","import torch\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = f\"1\"\n","torch.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1699837442811,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"WetUcZlRjqnJ","outputId":"4557dfc3-c2f2-4a92-e1bf-f7aac24963b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading file...\n"]}],"source":["import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n","dataset_name = 'census-income'\n","out = Path(os.getcwd()+'/data/'+dataset_name+'.csv')\n","\n","out.parent.mkdir(parents=True, exist_ok=True)\n","if out.exists():\n","    print(\"File already exists.\")\n","else:\n","    print(\"Downloading file...\")\n","    wget.download(url, out.as_posix())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1699837447712,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"4YZTjMPkjxzx","outputId":"15c0007a-7579-4b6b-d319-f6dfced22cde"},"outputs":[{"output_type":"stream","name":"stdout","text":["32560\n","39 73\n"," State-gov 9\n"," Bachelors 16\n"," 13 16\n"," Never-married 7\n"," Adm-clerical 15\n"," Not-in-family 6\n"," White 5\n"," Male 2\n"," 2174 119\n"," 0 92\n"," 40 94\n"," United-States 42\n"," <=50K 2\n","Set 3\n"]}],"source":["train = pd.read_csv(out)\n","print(len(train))\n","target = ' <=50K'\n","if \"Set\" not in train.columns:\n","    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n","\n","train_indices = train[train.Set==\"train\"].index\n","valid_indices = train[train.Set==\"valid\"].index\n","test_indices = train[train.Set==\"test\"].index\n","\n","nunique = train.nunique()\n","types = train.dtypes\n","\n","categorical_columns = []\n","categorical_dims =  {}\n","for col in train.columns:\n","    if types[col] == 'object' or nunique[col] < 200:\n","        print(col, train[col].nunique())\n","        l_enc = LabelEncoder()\n","        train[col] = train[col].fillna(\"VV_likely\")\n","        train[col] = l_enc.fit_transform(train[col].values)\n","        categorical_columns.append(col)\n","        categorical_dims[col] = len(l_enc.classes_)\n","    else:\n","        train.fillna(train.loc[train_indices, col].mean(), inplace=True)\n","# check that pipeline accepts strings\n","# train.loc[train[target]==0, target] = \"wealthy\"\n","# train.loc[train[target]==1, target] = \"not_wealthy\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1699837448190,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6v-oLA6Mj36p","outputId":"4c6dad69-29d5-4ed3-e989-42437f6a2765"},"outputs":[{"output_type":"stream","name":"stdout","text":["             all features 14\n","             categoricals [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","   categorical dimensions [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42]\n"]}],"source":["unused_feat = ['Set']\n","\n","features = [ col for col in train.columns if col not in unused_feat+[target]]\n","# 카테고리 index\n","cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","# 카테고리별 class 개수\n","cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","\n","print(\"all features\".rjust(25,\" \"), len(features))\n","print(\"categoricals\".rjust(25,\" \"), cat_idxs)\n","print(\"categorical dimensions\".rjust(25,\" \") , cat_dims)\n","grouped_features = [[0, 1, 2], [8, 9, 10]]\n","\n","train.loc[train_indices, features+[target]].to_csv(\"train.csv\", index=False)\n","train.loc[valid_indices, features+[target]].to_csv(\"valid.csv\", index=False)\n","train.loc[test_indices, features+[target]].to_csv(\"test.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1699240880703,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"h_WpyooYj9EK","outputId":"88012d12-9bda-4380-d974-2b3923035aa6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]}],"source":["# steps, n_shared, n_independce 수정하지 않았음\n","tabnet_params = {\n","    \"cat_idxs\":cat_idxs,\n","    \"cat_dims\":cat_dims,\n","    \"cat_emb_dim\":2,\n","    \"n_d\": 16,\n","    \"n_a\": 16,\n","    \"n_independent\": 9,\n","    \"n_shared\": 4,\n","    \"n_steps\": 2,\n","    \"gamma\": 1.4690246460970766,\n","    \"lambda_sparse\": 0,\n","    \"optimizer_fn\": torch.optim.Adam,\n","    \"optimizer_params\":dict(lr=2e-2),\n","    \"scheduler_params\":{\n","        \"step_size\":50, # how to use learning rate scheduler\n","        \"gamma\":0.9},\n","    \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n","    \"mask_type\": 'entmax', # \"sparsemax\"\n","    \"grouped_features\" : grouped_features\n","   }\n","\n","clf = TabNetClassifier(**tabnet_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpJ4E0TUkHfN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699492130726,"user_tz":-540,"elapsed":280,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"446fc53b-4736-4b18-c348-0e4e5c5a1731"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3267, 14) (3267,)\n"]}],"source":["X_train = train[features].values[train_indices]\n","y_train = train[target].values[train_indices]\n","\n","X_valid = train[features].values[valid_indices]\n","y_valid = train[target].values[valid_indices]\n","\n","X_test = train[features].values[test_indices]\n","y_test = train[target].values[test_indices]\n","\n","print(X_test.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"_9_oHkNflf9R"},"source":["놀랍게도 agumentations를 지원하고 있었다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ-Nl1pikIg5"},"outputs":[],"source":["max_epochs = 50 if not os.getenv(\"CI\", False) else 2\n","\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","aug = ClassificationSMOTE(p=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617648,"status":"ok","timestamp":1698627658503,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"CT0D2yDCkNt-","outputId":"8cdeb641-f53d-459e-a399-589d962baf49"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:25s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:46s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:58s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:01:09s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:01:22s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:01:32s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:01:45s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:01:55s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:02:12s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:02:25s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:02:37s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:02:48s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:02:59s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:03:11s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:03:22s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:03:35s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:03:45s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:03:59s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:04:08s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:04:22s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:04:31s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:04:54s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:05:08s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:05:17s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:05:30s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:05:39s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:05:53s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:06:02s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:06:15s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:06:24s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:06:38s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:06:47s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:07:00s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:07:10s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:07:22s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:07:32s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:07:44s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:07:55s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:08:07s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:08:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:08:29s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:08:41s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:08:52s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:09:04s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:09:15s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:09:27s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:09:37s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:09:50s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:10:00s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:10:13s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the behaviour of the model's fit method using Compressed Sparse Row matrices\n","sparse_X_train = scipy.sparse.csr_matrix(X_train)  # Create a CSR matrix from X_train\n","sparse_X_valid = scipy.sparse.csr_matrix(X_valid)  # Create a CSR matrix from X_valid\n","\n","# Fitting the model\n","clf.fit(\n","    X_train=sparse_X_train, y_train=y_train,\n","    eval_set=[(sparse_X_train, y_train), (sparse_X_valid, y_valid)],\n","    eval_name=['train', 'valid'],\n","    eval_metric=['auc'],\n","    max_epochs=max_epochs , patience=20,\n","    batch_size=1024, virtual_batch_size=128,\n","    num_workers=0,\n","    weights=1,\n","    drop_last=False,\n","    augmentations=aug, #aug, None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":979090,"status":"ok","timestamp":1698628637591,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jTvaq1hVkRou","outputId":"bf6ffce5-f571-40a4-d2ce-a0b8822a404c"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:14s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:20s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:33s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:39s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:58s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:03s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:16s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:22s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:40s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:46s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:05s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:23s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:29s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:41s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:54s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:01s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:12s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:21s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:30s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:37s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:55s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:01s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:38s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:48s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:52s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:56s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:01:02s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:08s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:12s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:16s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:21s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:27s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:32s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:36s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:40s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:46s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:52s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:56s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:02:00s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:02:05s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:11s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:16s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:20s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:24s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:29s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:36s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:40s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:44s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:48s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:54s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:03:00s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:04s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:12s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:27s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:31s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:37s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:43s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:47s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:51s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:55s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:04:01s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:07s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:13s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:19s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:37s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:56s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:02s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:21s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:39s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:45s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:55s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:04s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:15s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:19s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:28s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:42s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:53s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:02s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:11s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:22s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:29s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:35s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:54s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:00s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:04s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:36s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:42s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:55s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:00s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:06s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:19s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:25s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:38s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:43s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:50s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:58s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:02s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:08s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:26s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:32s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:46s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:52s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:58s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:03s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:11s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:17s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:26s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:30s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:35s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:41s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:46s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:50s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:54s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:59s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:06s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the warm_start=False behaviour\n","save_history = []\n","\n","# Fitting the model without starting from a warm start nor computing the feature importance\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=False\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))\n","\n","save_history = []  # Resetting the list to show that it also works when computing feature importance\n","\n","# Fitting the model without starting from a warm start but with the computing of the feature importance activated\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=True # True by default so not needed\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uq7dwumBlr7z"},"outputs":[],"source":["# plot losses\n","plt.plot(clf.history['loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A52swQzcls_U"},"outputs":[],"source":["# plot auc\n","plt.plot(clf.history['train_auc'])\n","plt.plot(clf.history['valid_auc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qY0wTE3lt5k"},"outputs":[],"source":["# plot learning rates\n","plt.plot(clf.history['lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbZ5ydVIlvs4"},"outputs":[],"source":["preds = clf.predict_proba(X_test)\n","test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n","\n","\n","preds_valid = clf.predict_proba(X_valid)\n","valid_auc = roc_auc_score(y_score=preds_valid[:,1], y_true=y_valid)\n","\n","print(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")\n","# check that best weights are used\n","assert np.isclose(valid_auc, np.max(clf.history['valid_auc']), atol=1e-6)\n","clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBrjfLhbl02o"},"outputs":[],"source":["# save tabnet model\n","saving_path_name = \"./tabnet_model_test_1\"\n","saved_filepath = clf.save_model(saving_path_name)\n","\n","# define new model with basic parameters and load state dict weights\n","loaded_clf = TabNetClassifier()\n","loaded_clf.load_model(saved_filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9kfTTcAl4RY"},"outputs":[],"source":["loaded_preds = loaded_clf.predict_proba(X_test)\n","loaded_test_auc = roc_auc_score(y_score=loaded_preds[:,1], y_true=y_test)\n","\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_auc}\")\n","\n","assert(test_auc == loaded_test_auc)\n","loaded_clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IalieiADl8SZ"},"outputs":[],"source":["# Global explainability : feat importance summing to 1\n","clf.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3sio5i_l-XO"},"outputs":[],"source":["explain_matrix, masks = clf.explain(X_test)\n","fig, axs = plt.subplots(1, 3, figsize=(20,20))\n","\n","for i in range(3):\n","    axs[i].imshow(masks[i][:50])\n","    axs[i].set_title(f\"mask {i}\")\n","    axs[i].set_xticklabels(labels = features, rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"Xcqx5Qygjm_E"},"source":["Lightning 으로 네트워크 학습하기"]},{"cell_type":"code","execution_count":41,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1699851747191,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jizvAZvn1Ppo","outputId":"b91bdbe9-ecaa-4db4-eb71-42cd5d683f28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting tabnet.py\n"]}],"source":["%%writefile tabnet.py\n","# @title <b><i>Training 파일 저장<i/></b>\n","import argparse\n","import logging\n","import os\n","import gc\n","import sys\n","import time\n","import pickle\n","from typing import Any, Dict, Optional, Type\n","# import dgl\n","# import deepchem as dc\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.special import softmax\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics import F1Score, AUROC, Accuracy\n","from torch.optim import SGD, Adam, NAdam, RAdam, SparseAdam, LBFGS, RMSprop\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CyclicLR\n","from torchsummary import summary\n","\n","# from dgl.dataloading import GraphDataLoader\n","\n","from pytorch_tabnet.tab_network import TabNetPretraining, TabNet\n","from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n","from pytorch_tabnet.utils import create_group_matrix, create_explain_matrix\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","\n","import wandb\n","from lightning.fabric.utilities import apply_func\n","import lightning.pytorch as pl\n","from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n","from lightning.pytorch.callbacks import (\n","    TQDMProgressBar, EarlyStopping, LearningRateMonitor,\n","    ModelCheckpoint, StochasticWeightAveraging, DeviceStatsMonitor)\n","from lightning.pytorch.plugins.io import XLACheckpointIO\n","from lightning.pytorch.cli import LightningCLI, ArgsType\n","from lightning_utilities.core.imports import module_available\n","\n","sys.setrecursionlimit(10**7)\n","sys.set_int_max_str_digits(0)\n","isxla = module_available(\"torch_xla\")\n","print(f'Can Use Torch_XLA? {isxla}')\n","\n","if isxla:\n","    import torch_xla\n","    import torch_xla.core.xla_model as xm\n","    import torch_xla.distributed.xla_multiprocessing as xmp\n","    import torch_xla.utils.serialization as xser\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.WARNING)\n","\n","\n","class ArgsCLI(LightningCLI):\n","    def add_arguments_to_parser(self, parser):\n","        parser.add_argument('--train_file',\n","                            type=str,\n","                            default='dev_t.csv',\n","                            help='train file')\n","\n","        parser.add_argument('--test_file',\n","                            type=str,\n","                            default='dev_v.csv',\n","                            help='test file')\n","\n","        parser.add_argument('--accumulate_grad_batches',\n","                            type=int,\n","                            default=1,\n","                            help='accumulate_grad_batches')\n","\n","        parser.add_argument('--num_workers',\n","                            type=int,\n","                            default=2,\n","                            help='num of worker for dataloader')\n","\n","        parser.add_argument('--batch_size',\n","                            type=int,\n","                            default=32,\n","                            help='batch size for training (default: 96)')\n","\n","        parser.add_argument('--lr',\n","                            type=float,\n","                            default=5e-7,\n","                            help='The initial learning rate')\n","\n","        parser.add_argument('--warmup_ratio',\n","                            type=float,\n","                            default=0.1,\n","                            help='warmup ratio')\n","\n","        parser.add_argument('--checkpoint_path',\n","                            type=str,\n","                            help='checkpoint path')\n","\n","        parser.add_argument('--default_root_dir',\n","                            type=str,\n","                            help='default_root_dir')\n","\n","\n","class CensusDataset(Dataset):\n","    def __init__(self,\n","                 filepath: str,) -> None:\n","        super().__init__()\n","        self.data = pd.read_csv(filepath)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def update_attributes(self, features, idxs, dims):\n","        self.features=features\n","        self.cat_ids = idxs\n","        self.cat_dims = dims\n","\n","    def make_input_id_mask(self,\n","                           tokens):\n","        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = [1] * len(input_id)\n","        if len(input_id) < self.max_seq_len:\n","            while len(input_id) < self.max_seq_len:\n","                input_id += [self.tokenizer.pad_token_id]\n","                attention_mask += [0]\n","        else:\n","            input_id = input_id[:self.max_seq_len - 1] + [self.tokenizer.eos_token_id]\n","            attention_mask = attention_mask[:self.max_seq_len]\n","        return input_id, attention_mask\n","\n","    def masking(self,\n","                input,\n","                mask):\n","        input = torch.tensor([input])\n","        rand = torch.rand(input.shape)\n","        mask_arr = (rand < 0.15) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n","        mask_ids = torch.flatten((mask_arr[0]).nonzero()).tolist()\n","        input[0, mask_ids] = mask\n","        del rand, mask_arr\n","        return input.tolist()[0]\n","\n","    def _labeling(self,\n","                  label):\n","        tokens = [self.tokenizer.bos_token]+self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n","        label_ids = self.tokenizer.convert_tokens_to_ids(tokens[1:])\n","        if len(label_ids) < self.max_seq_len:\n","            while len(label_ids)<self.max_seq_len:\n","                label_ids+=[-100]\n","        else:\n","            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n","        del tokens\n","        return label_ids\n","\n","    def _scale_transform(self, values):\n","        return self.scaler.transform(values.reshape(1,-1))\n","\n","    def __getitem__(self,\n","                    index:int):\n","        assert self.features\n","        record = self.data.iloc[index]\n","        dod  = record[' <=50K']\n","        tabs = record[self.features].values\n","        return {\n","            'tabnet_input': tabs.astype('float32'),\n","            'labels': np.array(dod, dtype=np.float32)\n","        }\n","\n","\n","class CensusDataModule(pl.core.LightningDataModule):\n","    def __init__(self,\n","                 train_file: str,\n","                 valid_file: str,\n","                 test_file: str,\n","                 batch_size: int = 32,\n","                 num_workers: int = 8):\n","        super().__init__()\n","        self.batch_size = batch_size # batch_size\n","        self.train_file_path = train_file\n","        self.valid_file_path = valid_file\n","        self.test_file_path = test_file\n","        self.num_workers = num_workers\n","        self.prepare_data_pre_node = True\n","    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n","\n","    def setup(self,\n","              stage):\n","        # split dataset\n","        self.train = CensusDataset(self.train_file_path)\n","        self.valid = CensusDataset(self.valid_file_path)\n","        self.test = CensusDataset(self.test_file_path)\n","        features, idxs, dims = self.get_features()\n","        self.train.update_attributes(features, idxs, dims)\n","        self.valid.update_attributes(features, idxs, dims)\n","        self.test.update_attributes(features, idxs, dims)\n","\n","    def get_features(self):\n","        df = pd.concat([self.train.data, self.valid.data, self.test.data])\n","        target = ' <=50K'\n","        nunique = df.nunique()\n","        types = df.dtypes\n","        categorical_columns = []\n","        categorical_dims =  {}\n","        for col in df.columns:\n","            if types[col] == 'object' or nunique[col] < 200:\n","                l_enc = LabelEncoder()\n","                df[col] = df[col].fillna(\"VV_likely\")\n","                df[col] = l_enc.fit_transform(df[col].values)\n","                categorical_columns.append(col)\n","                categorical_dims[col] = len(l_enc.classes_)\n","            else:\n","                df.fillna(df.median(), inplace=True)\n","        unused_feat = ['Set']\n","        features = [ col for col in df.columns if col not in unused_feat+[target]]\n","        cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","        cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","        return features, cat_idxs, cat_dims\n","\n","    def train_dataloader(self):\n","        print(\"Train size: \", len(self.train))\n","        return DataLoader(self.train,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=True,\n","                          drop_last=True)\n","\n","    def val_dataloader(self):\n","        print(\"Valid size: \", len(self.valid))\n","        return DataLoader(self.valid,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","    def test_dataloader(self):\n","        print(\"Test size: \", len(self.test))\n","        return DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","\n","class Base(pl.core.LightningModule):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs) -> None:\n","        super().__init__()\n","        self.lr = lr\n","        self.warmup_ratio = warmup\n","        self.batch_size = batch_size\n","\n","    def decay_params(self, named_params):\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'BatchNorm1d.bias', 'BatchNorm1d.weight']\n","        return [\n","            {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","    def named_params(self, params):\n","        return [\n","            {\"params\": [p for p in params], \"name\": \"weights\"}\n","        ]\n","\n","    def configure_optimizers(self):\n","        # Prepare optimizer\n","\n","        # optimizer_grouped_parameters = self.network.parameters()\n","        optimizer_grouped_parameters = self.decay_params(self.network.named_parameters())\n","        optimizer_grouped_parameters = self.named_params(self.network.parameters())\n","        optimizer = Adam(params=optimizer_grouped_parameters,\n","                         lr=self.lr,)\n","        # optimizer = RMSprop(params=optimizer_grouped_parameters,\n","        #                    momentum=0.9,\n","        #                    lr=self.lr)\n","\n","        # warm up lr\n","        num_nodes = self.trainer.strategy.num_nodes if not \"Single\" in str(type(self.trainer.strategy)) else 1\n","        devices = len(self.trainer.strategy.parallel_devices) if \"parallel_devices\" in self.trainer.strategy.__dict__.keys() else 1\n","        num_devices = int(devices * num_nodes)\n","        data_len = len(self.trainer.fit_loop._data_source.instance.train)\n","       # data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n","        logging.warn(f'number of devices {num_devices}, data length {data_len}')\n","        num_train_steps = int((data_len // (self.batch_size) * num_devices) * self.trainer.max_epochs)\n","        logging.warn(f'num_train_steps : {num_train_steps}')\n","\n","        num_warmup_steps = int(num_train_steps * 0.01)\n","        logging.warn(f'num_warmup_steps : {num_warmup_steps}')\n","\n","        scheduler = CyclicLR(\n","                 optimizer=optimizer,\n","                 base_lr=1e-3,\n","                 max_lr=self.lr,\n","                 cycle_momentum=False,\n","                 step_size_up=num_warmup_steps,)\n","\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"interval\": \"step\",\n","               # \"name\": \"lr_log\",\n","               # \"monitor\": \"loss\",\n","               # \"frequency\": self.batch_size\n","                }\n","            }\n","\n","   # DEFAULT\n","   # def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n","   #     optimizer.zero_grad()\n","\n","   # def backward(self, loss):\n","   #    with torch.autograd.set_detect_anomaly(True):\n","   #         loss.requires_grad_().to(self.device)\n","   #         loss.backward()\n","\n","\n","class Tabnet(torch.nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 output_dim: int,\n","                 batch_size: int,\n","                 cat_idx:list,\n","                 cat_dim:list,\n","                 device: str = \"cpu\"):\n","        super(Tabnet, self).__init__()\n","        self.cat_idx = cat_idx\n","        self.cat_dim = cat_dim\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.batch_size = batch_size\n","        self.net = TabNet(\n","            cat_idxs=cat_idx,\n","            cat_dims=cat_dim,\n","            cat_emb_dim=[2]*len(cat_idx),\n","            input_dim=input_dim,\n","            output_dim=output_dim,\n","            n_a=2, # Dimension of the prediction layer\n","            n_d=2, # Dimension of the attention layer\n","            n_steps=3, # Output까지의 Attentive-Feature transformer layer\n","            n_independent=1, # Decision step dependent 단계 GLU layer\n","            n_shared=1, # Shared across decision steps 단계 GLU layer\n","            epsilon=1e-5,\n","            gamma=1.3,\n","            mask_type=\"entmax\", # sparsemax / entmax\n","            momentum=0.01,\n","            virtual_batch_size=128,\n","            group_attention_matrix=create_group_matrix(\n","                [[0, 1, 2,], [8, 9, 10,]],\n","                self.input_dim).to(device))\n","        self.reducing_matrix = create_explain_matrix(\n","                    self.net.input_dim,\n","                    self.net.cat_emb_dim,\n","                    self.net.cat_idxs,\n","                    self.net.post_embed_dim,)\n","        self.aug = ClassificationSMOTE(p=0.2)\n","\n","\n","    def _re_init(self, device, state_dict=None):\n","       # if \"xla\" in device:\n","       # del self.net, self.reducing_matrix\n","        self.__init__(\n","                input_dim=self.input_dim,\n","                output_dim=self.output_dim,\n","                batch_size=self.batch_size,\n","                cat_idx=self.cat_idx,\n","                cat_dim=self.cat_dim,\n","                device=device)\n","        if state_dict:\n","            self.load_state_dict(state_dict)\n","        self.to(device)\n","\n","    def forward(self, x):\n","        x, _ = self.aug(x, None)\n","       # for param in self.net.parameters():\n","       #         param.grad = None\n","        return self.net(x)\n","\n","    def parameters(self):\n","        return self.net.parameters()\n","\n","\n","class LightningTabNet(Base):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs):\n","        super().__init__(lr, warmup, batch_size, **kwargs)\n","        # self.automatic_optimization=False\n","\n","        model_dim = 2 # @param {type:\"integer\"}\n","        cat_input_dim = 14 # @param {type:\"integer\"}\n","\n","        self.network = Tabnet(\n","            input_dim=cat_input_dim,\n","            output_dim=model_dim,\n","            batch_size=batch_size,\n","            cat_idx = [ 0, 1,  3,  4, 5,  6, 7, 8, 9,  10, 11, 12, 13],\n","            cat_dim = [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42])\n","        self.loss = torch.nn.CrossEntropyLoss()\n","        self.f1 = F1Score(task=\"multiclass\", num_classes=2)\n","        self.acc = Accuracy(task=\"multiclass\", num_classes=2)\n","        self.roc = AUROC(task=\"multiclass\", num_classes=2)\n","       # self.concat = torch.nn.Bilinear(model_dim, model_dim, model_dim,)\n","\n","    def _loss(self, yhat, y, M_loss):\n","        pred = torch.softmax(yhat, dim=1).to(self.device)\n","        acc = self.acc(pred, y)\n","        f1 = self.f1(pred, y)\n","        roc = self.roc(pred, y.long())\n","        clf_loss = self.loss(yhat, y.long()) - (1.3 * M_loss)\n","        clf_loss.to(self.device).requires_grad_()\n","        return clf_loss, acc, f1, roc\n","\n","    def initialize(self, m):\n","        for layer in m.modules():\n","            if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.Bilinear):\n","                torch.nn.init.xavier_uniform_(layer.weight, gain=1.0)\n","\n","   # def teardown(self,\n","   #              trainer: pl.Trainer,\n","   #              pl_module: pl.LightningModule,\n","   #              stage: str) -> None:\n","   #     print(f\"{stage} Done\")\n","\n","    def on_fit_start(self,) -> None:\n","        self.network._re_init(self.device) # Multiprocessing 하는 경우 device에 올라가는 문제\n","        self.initialize(self.network.net)\n","        torch.set_grad_enabled(True)\n","        self.configure_optimizers()\n","        self.trainer.strategy.setup_optimizers(trainer=self.trainer)\n","        print(self.trainer.strategy._lightning_optimizers)\n","       # print(\"Training with Lightning Module\\n\")\n","\n","    def on_predict_start(self,) -> None:\n","        state_dict = self.network.state_dict()\n","       # self.tabnet._re_init(self.device, state_dict)\n","\n","   # def on_fit_end(self,) -> None:\n","   #     if xm.is_master_ordinal():\n","   #         print(\"\\nTrain Done\\n\")\n","   #         # save_hf_repo(self.model.cpu(), self.tokenizer)\n","\n","    def forward(self,\n","                inputs):\n","        return self.network(inputs[\"tabnet_input\"])\n","       # output = torch.sum(torch.nan_to_num(torch.stack(output)), dim=0)\n","\n","   # def on_train_epoch_start(self):\n","   #     self.lm.train()\n","   #     self.tabnet.train()\n","\n","    def training_step(self,\n","                      batch,\n","                      batch_idx: int):\n","       # opt = self.optimizers()\n","        outs, M_loss = self(batch)\n","        loss, acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_acc\", acc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","       # opt.zero_grad()\n","       # self.manual_backward(loss)\n","       # opt.step()\n","\n","        return loss\n","\n","    def validation_step(self,\n","                        batch,\n","                        batch_idx: int):\n","        outs, M_loss = self(batch)\n","        loss, acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"val_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","\n","\n","    # def on_training_batch_end(self,\n","    #                           outputs,\n","    #                           batch,\n","    #                           batch_idx) -> None:\n","    #     print(\"Train Batch End\")\n","\n","    # def on_training_epoch_end(self,) -> None:\n","    #     print(\"Train Epoch End\")\n","\n","    # def validation_batch_end(self,\n","    #                          outputs,\n","    #                          batch,\n","    #                          batch_idx,\n","    #                          dataloader_idx=0) -> None:\n","    #     print(\"Valid Batch End\")\n","\n","    # def validation_epoch_end(self,) -> None:\n","    #     print(\"Valid Epoch End\")\n","\n","    # def on_save_checkpoint(self,\n","    #                        trainer: pl.Trainer,\n","    #                        pl_module: pl.LightningModule=None,\n","    #                        checkpoint: Dict[str,Any]=None) -> None:\n","    #     checkpoint[\"lighitning_logs/checkpoint\"]=self\n","    #     if xm.is_master_ordinal():\n","    #         print(f\"Trainer save checkpoint\")\n","    #         save_hf_repo(self.model.cpu(), self.model.tokenizer)\n","    #         print(\"\\nCheckpoint saved\\n\")\n","\n","def sweep_agent():\n","    sweep_config = {\n","        'method': 'random',\n","        'name': 'first_sweep',\n","        'metric': {\n","            'goal': 'minimize',\n","            'name': 'training_loss'\n","        },\n","        'parameters': {\n","            'n_hidden': {'values': [2,3,5,10]},\n","            'lr': {'max': 1.0, 'min': 0.0001}\n","        }\n","    }\n","    sweep_id = wandb.sweep(sweep_config, project=\"Tabnet_XLA\")\n","    wandb.agent(sweep_id=sweep_id, function=cli_main, count=5)\n","\n","\n","def cli_main() -> None: # args:ArgsType=None\n","    callbacks = [ # StochasticWeightAveraging(swa_lrs=1e-2), # 23/04/24 기준 lightning에서 오류 있는 callback\n","        ModelCheckpoint(monitor=\"val_loss\",\n","                        dirpath=\"logs/\",\n","                        filename=\"best-checkpoint\",\n","                        verbose=True,\n","                        save_last=True,\n","                        mode=\"min\",\n","                        save_top_k=1),\n","        DeviceStatsMonitor(),\n","        EarlyStopping(monitor=\"val_loss\",\n","                      mode=\"min\",\n","                      stopping_threshold=1e-4,\n","                      min_delta=0.00,\n","                      patience=5,\n","                      divergence_threshold=9.0),\n","        LearningRateMonitor(logging_interval=\"step\"),\n","        TQDMProgressBar(refresh_rate=3)]\n","    logger = WandbLogger(project=\"Tabnet_XLA\", log_model=\"all\")\n","    cli = ArgsCLI(model_class=LightningTabNet,\n","                  datamodule_class=CensusDataModule,\n","                  save_config_kwargs={\"overwrite\": True},\n","                  trainer_defaults={\n","                    # \"plugins\": [XLACheckpointIO(),],\n","                      \"callbacks\": callbacks,\n","                    #  \"reload_dataloaders_every_n_epochs\": 1,\n","                      \"detect_anomaly\": True,\n","                      \"logger\": [logger]\n","                      },)\n","\n","\n","if __name__ == \"__main__\":\n","    pl.cli_lightning_logo()\n","    wandb.init(project=\"Tabnet_XLA\")\n","    cli_main()\n","\n","   # save_path = \"/content/drive/MyDrive/data_kevin/ChemGPT_finetuned.pth\"\n","   # torch.save(model.model.state_dict(),save_path)\n","    wandb.finish()\n","    print(\"\\nTraining Done...\\n\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1699841973557,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"ibzfCMq_Mhhe","outputId":"776e2594-f641-4669-e146-516421812a97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting trainer.sh\n"]}],"source":["%%writefile trainer.sh\n","TRAIN_FILE=\"train.csv\" #@param ['\"train.csv\"'] {type:\"raw\"}\n","VAL_FILE=\"valid.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","TEST_FILE=\"test.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","PERCISION=32 #@param [\"bf16-true\", \"16-true\", \"32\", \"bf16-mixed\"] {type:\"raw\", allow-input: true}\n","LEARNING_RATE=2e-02 #@param {type:\"raw\"}\n","GRADIENT_CLIP=1.0 #@param {type:\"raw\"}\n","ACCUMULATE_GRAD_BATCHES=1 #@param {type:\"raw\"}\n","BATCH_SIZE=128 #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"512\", \"4096\", \"32768\", \"262144\", \"2097152\", \"16777216\", \"134217728\", \"1073741824\", \"8589934592\", \"68719476736\"] {type:\"raw\", allow-input: true}\n","MAXEPOCHS=100 #@param {type:\"raw\"}\n","STRATEGY=auto #@param [\"auto\", \"bagua\", \"colossalai\", \"collaborative\", \"ddp\", \"ddp2\", \"ddp_find_unused_parameters_false\", \"ddp_fully_sharded\", \"ddp_sharded\", \"ddp_sharded_find_unused_parameters_false\", \"ddp_sharded_spawn\", \"ddp_sharded_spawn_find_unused_parameters_false\", \"ddp_spawn\", \"ddp_spawn_find_unused_parameters_false\", \"ddp_fork\" ,\"deepspeed\", \"deepspeed_stage_1\", \"deepspeed_stage_2\", \"deepspeed_stage_2_offload\", \"deepspeed_stage_3\", \"deepspeed_stage_3_offload\", \"deepspeed_stage_3_offload_nvme\", \"dp\", \"fsdp\", \"fsdp_native\", \"horovod\", \"hpu_parallel\", \"hpu_single\", \"ipu_strategy\", \"single_device\", \"single_tpu\", \"tpu_spawn\", \"tpu_spawn_debug\", \"xla\", \"xla_debug\"]{type:\"raw\", allow-input: false}\n","NUMWORKERS=2 #@param {type:\"raw\"}\n","WARMUP=0.1 #@param {type:\"raw\"}\n","PROFILE=simple #@param [\"simple\", \"advanced\", \"pytorch\", \"xla\"]  {type:\"raw\", allow-input: false}\n","FAST_DEV_RUN=false #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","SYNC_BATCHNORM=false #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","ACCELERATOR=cpu #@param [\"cpu\", \"gpu\", \"tpu\"] {type:\"raw\", allow-input: true}\n","CORES=1 #@param {type:\"raw\"}\n","\n","python tabnet.py fit\\\n","    --trainer.accumulate_grad_batches $ACCUMULATE_GRAD_BATCHES\\\n","    --trainer.gradient_clip_val $GRADIENT_CLIP\\\n","    --trainer.max_epochs $MAXEPOCHS\\\n","    --trainer.precision $PERCISION\\\n","    --trainer.accelerator $ACCELERATOR\\\n","    --trainer.devices $CORES\\\n","    --trainer.use_distributed_sampler false\\\n","    --trainer.sync_batchnorm $SYNC_BATCHNORM\\\n","    --trainer.profiler $PROFILE\\\n","    --trainer.strategy $STRATEGY\\\n","    --trainer.num_sanity_val_steps 3\\\n","    --trainer.log_every_n_steps 2\\\n","    --trainer.fast_dev_run $FAST_DEV_RUN\\\n","    --model.lr $LEARNING_RATE\\\n","    --model.batch_size $BATCH_SIZE\\\n","    --data.num_workers $NUMWORKERS\\\n","    --data.batch_size $BATCH_SIZE\\\n","    --model.warmup $WARMUP\\\n","    --data.train_file $TRAIN_FILE\\\n","    --data.valid_file $VAL_FILE\\\n","    --data.test_file $TEST_FILE\n","\n","exit 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRMpyxzTP6tn","outputId":"388a7864-3206-4e16-9201-1040e291850e"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:TPU system grpc://10.84.101.42:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"]},{"output_type":"stream","name":"stdout","text":["Can Use Torch_XLA? True\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevintb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","2023-11-13 05:03:03.051670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231113_050301-5m57qg7v\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlunar-dream-42\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/Tabnet_XLA\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/Tabnet_XLA/runs/5m57qg7v\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 3500102227\n","Seed set to 3500102227\n","You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n","GPU available: False, used: False\n","TPU available: True, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/setup.py:193: TPU available but not used. You can set it by doing `Trainer(accelerator='tpu')`.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","/usr/local/lib/python3.10/dist-packages/jsonargparse/_typehints.py:1223: JsonargparseWarning: \n","    Unable to serialize instance <lightning.pytorch.loggers.wandb.WandbLogger object at 0x7bcf2c463310>\n","\n","  warning(val)\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:634: Checkpoint directory /content/logs exists and is not empty.\n","/content/tabnet.py:288: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n","  logging.warn(f'number of devices {num_devices}, data length {data_len}')\n","WARNING:root:number of devices 1, data length 26072\n","/content/tabnet.py:290: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n","  logging.warn(f'num_train_steps : {num_train_steps}')\n","WARNING:root:num_train_steps : 26071900\n","/content/tabnet.py:293: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n","  logging.warn(f'num_warmup_steps : {num_warmup_steps}')\n","WARNING:root:num_warmup_steps : 260719\n","\n","  | Name    | Type               | Params\n","-----------------------------------------------\n","0 | network | Tabnet             | 1.7 K \n","1 | loss    | CrossEntropyLoss   | 0     \n","2 | f1      | MulticlassF1Score  | 0     \n","3 | acc     | MulticlassAccuracy | 0     \n","4 | roc     | MulticlassAUROC    | 0     \n","-----------------------------------------------\n","1.7 K     Trainable params\n","0         Non-trainable params\n","1.7 K     Total params\n","0.007     Total estimated model params size (MB)\n","WARNING:root:number of devices 1, data length 26072\n","WARNING:root:num_train_steps : 26071900\n","WARNING:root:num_warmup_steps : 260719\n","WARNING:root:number of devices 1, data length 26072\n","WARNING:root:num_train_steps : 26071900\n","WARNING:root:num_warmup_steps : 260719\n","[LightningAdam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    initial_lr: 0.001\n","    lr: 0.001\n","    maximize: False\n","    name: weights\n","    weight_decay: 0\n",")]\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]Valid size:  3221\n","Train size:  26072\n","Epoch 0: 100% 203/203 [00:33<00:00,  6.00it/s, v_num=qg7v, train_loss_step=1.850, train_acc_step=0.383, train_f1_step=0.383, train_roc_step=0.653]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 26.35it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 21.09it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.71it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 22.16it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 22.75it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 22.25it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 23.17it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.73it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 24.35it/s]\u001b[A\n","Epoch 0: 100% 203/203 [00:35<00:00,  5.78it/s, v_num=qg7v, train_loss_step=1.850, train_acc_step=0.383, train_f1_step=0.383, train_roc_step=0.653, val_f1_step=0.586, val_roc_step=0.621, val_loss=3.060, val_acc=0.562, val_f1_epoch=0.562, val_roc_epoch=0.532, train_loss_epoch=2.520, train_acc_epoch=0.283, train_f1_epoch=0.283, train_roc_epoch=0.543]Epoch 0, global step 203: 'val_loss' reached 3.05567 (best 3.05567), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 1: 100% 203/203 [00:19<00:00, 10.45it/s, v_num=qg7v, train_loss_step=1.080, train_acc_step=0.641, train_f1_step=0.641, train_roc_step=0.690, val_f1_step=0.586, val_roc_step=0.621, val_loss=3.060, val_acc=0.562, val_f1_epoch=0.562, val_roc_epoch=0.532, train_loss_epoch=2.520, train_acc_epoch=0.283, train_f1_epoch=0.283, train_roc_epoch=0.543]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 28.09it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 22.34it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.92it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 21.79it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 21.66it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 21.05it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 21.90it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 22.80it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 23.43it/s]\u001b[A\n","Epoch 1: 100% 203/203 [00:20<00:00,  9.77it/s, v_num=qg7v, train_loss_step=1.080, train_acc_step=0.641, train_f1_step=0.641, train_roc_step=0.690, val_f1_step=0.484, val_roc_step=0.460, val_loss=1.450, val_acc=0.521, val_f1_epoch=0.521, val_roc_epoch=0.537, train_loss_epoch=1.420, train_acc_epoch=0.455, train_f1_epoch=0.455, train_roc_epoch=0.576]Epoch 1, global step 406: 'val_loss' reached 1.44550 (best 1.44550), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 2: 100% 203/203 [00:19<00:00, 10.47it/s, v_num=qg7v, train_loss_step=0.897, train_acc_step=0.523, train_f1_step=0.523, train_roc_step=0.442, val_f1_step=0.484, val_roc_step=0.460, val_loss=1.450, val_acc=0.521, val_f1_epoch=0.521, val_roc_epoch=0.537, train_loss_epoch=1.420, train_acc_epoch=0.455, train_f1_epoch=0.455, train_roc_epoch=0.576]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 25.13it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 20.69it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 22.82it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 21.28it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 22.67it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 21.73it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 22.65it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.18it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 23.74it/s]\u001b[A\n","Epoch 2: 100% 203/203 [00:20<00:00,  9.80it/s, v_num=qg7v, train_loss_step=0.897, train_acc_step=0.523, train_f1_step=0.523, train_roc_step=0.442, val_f1_step=0.539, val_roc_step=0.527, val_loss=0.901, val_acc=0.548, val_f1_epoch=0.548, val_roc_epoch=0.552, train_loss_epoch=0.978, train_acc_epoch=0.543, train_f1_epoch=0.543, train_roc_epoch=0.560]Epoch 2, global step 609: 'val_loss' reached 0.90061 (best 0.90061), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 3: 100% 203/203 [00:18<00:00, 10.69it/s, v_num=qg7v, train_loss_step=0.792, train_acc_step=0.578, train_f1_step=0.578, train_roc_step=0.481, val_f1_step=0.539, val_roc_step=0.527, val_loss=0.901, val_acc=0.548, val_f1_epoch=0.548, val_roc_epoch=0.552, train_loss_epoch=0.978, train_acc_epoch=0.543, train_f1_epoch=0.543, train_roc_epoch=0.560]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 28.99it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 22.84it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 24.75it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 22.73it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 23.73it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 22.39it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 22.98it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.62it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 24.30it/s]\u001b[A\n","Epoch 3: 100% 203/203 [00:20<00:00, 10.00it/s, v_num=qg7v, train_loss_step=0.792, train_acc_step=0.578, train_f1_step=0.578, train_roc_step=0.481, val_f1_step=0.562, val_roc_step=0.469, val_loss=0.779, val_acc=0.658, val_f1_epoch=0.658, val_roc_epoch=0.550, train_loss_epoch=0.856, train_acc_epoch=0.600, train_f1_epoch=0.600, train_roc_epoch=0.555]Epoch 3, global step 812: 'val_loss' reached 0.77927 (best 0.77927), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 4: 100% 203/203 [00:19<00:00, 10.34it/s, v_num=qg7v, train_loss_step=0.720, train_acc_step=0.688, train_f1_step=0.688, train_roc_step=0.573, val_f1_step=0.562, val_roc_step=0.469, val_loss=0.779, val_acc=0.658, val_f1_epoch=0.658, val_roc_epoch=0.550, train_loss_epoch=0.856, train_acc_epoch=0.600, train_f1_epoch=0.600, train_roc_epoch=0.555]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 27.62it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 22.06it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 20.05it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 21.14it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 20.96it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 20.28it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 21.03it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 21.87it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 22.49it/s]\u001b[A\n","Epoch 4: 100% 203/203 [00:21<00:00,  9.64it/s, v_num=qg7v, train_loss_step=0.720, train_acc_step=0.688, train_f1_step=0.688, train_roc_step=0.573, val_f1_step=0.625, val_roc_step=0.506, val_loss=0.712, val_acc=0.678, val_f1_epoch=0.678, val_roc_epoch=0.537, train_loss_epoch=0.748, train_acc_epoch=0.659, train_f1_epoch=0.659, train_roc_epoch=0.554]Epoch 4, global step 1015: 'val_loss' reached 0.71211 (best 0.71211), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 5: 100% 203/203 [00:19<00:00, 10.52it/s, v_num=qg7v, train_loss_step=0.697, train_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.573, val_f1_step=0.625, val_roc_step=0.506, val_loss=0.712, val_acc=0.678, val_f1_epoch=0.678, val_roc_epoch=0.537, train_loss_epoch=0.748, train_acc_epoch=0.659, train_f1_epoch=0.659, train_roc_epoch=0.554]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 26.33it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 20.70it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.21it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 21.63it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 22.48it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 21.72it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 22.72it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.19it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 23.82it/s]\u001b[A\n","Epoch 5: 100% 203/203 [00:20<00:00,  9.82it/s, v_num=qg7v, train_loss_step=0.697, train_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.573, val_f1_step=0.672, val_roc_step=0.512, val_loss=0.706, val_acc=0.719, val_f1_epoch=0.719, val_roc_epoch=0.528, train_loss_epoch=0.718, train_acc_epoch=0.701, train_f1_epoch=0.701, train_roc_epoch=0.541]Epoch 5, global step 1218: 'val_loss' reached 0.70563 (best 0.70563), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 6: 100% 203/203 [00:21<00:00,  9.48it/s, v_num=qg7v, train_loss_step=0.707, train_acc_step=0.727, train_f1_step=0.727, train_roc_step=0.498, val_f1_step=0.672, val_roc_step=0.512, val_loss=0.706, val_acc=0.719, val_f1_epoch=0.719, val_roc_epoch=0.528, train_loss_epoch=0.718, train_acc_epoch=0.701, train_f1_epoch=0.701, train_roc_epoch=0.541]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 28.03it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 22.23it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.58it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 21.78it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 22.97it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 22.06it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 23.00it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.54it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 24.00it/s]\u001b[A\n","Epoch 6: 100% 203/203 [00:22<00:00,  8.91it/s, v_num=qg7v, train_loss_step=0.707, train_acc_step=0.727, train_f1_step=0.727, train_roc_step=0.498, val_f1_step=0.688, val_roc_step=0.508, val_loss=0.701, val_acc=0.743, val_f1_epoch=0.743, val_roc_epoch=0.509, train_loss_epoch=0.710, train_acc_epoch=0.726, train_f1_epoch=0.726, train_roc_epoch=0.522]Epoch 6, global step 1421: 'val_loss' reached 0.70076 (best 0.70076), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 7: 100% 203/203 [00:20<00:00,  9.79it/s, v_num=qg7v, train_loss_step=0.706, train_acc_step=0.742, train_f1_step=0.742, train_roc_step=0.528, val_f1_step=0.688, val_roc_step=0.508, val_loss=0.701, val_acc=0.743, val_f1_epoch=0.743, val_roc_epoch=0.509, train_loss_epoch=0.710, train_acc_epoch=0.726, train_f1_epoch=0.726, train_roc_epoch=0.522]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 27.08it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 19.99it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 21.56it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 20.05it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 20.85it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 19.90it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:01<00:00, 20.56it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 21.40it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 22.04it/s]\u001b[A\n","Epoch 7: 100% 203/203 [00:22<00:00,  9.14it/s, v_num=qg7v, train_loss_step=0.706, train_acc_step=0.742, train_f1_step=0.742, train_roc_step=0.528, val_f1_step=0.656, val_roc_step=0.462, val_loss=0.699, val_acc=0.749, val_f1_epoch=0.749, val_roc_epoch=0.506, train_loss_epoch=0.705, train_acc_epoch=0.739, train_f1_epoch=0.739, train_roc_epoch=0.518]Epoch 7, global step 1624: 'val_loss' reached 0.69856 (best 0.69856), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 8: 100% 203/203 [00:21<00:00,  9.27it/s, v_num=qg7v, train_loss_step=0.705, train_acc_step=0.719, train_f1_step=0.719, train_roc_step=0.482, val_f1_step=0.656, val_roc_step=0.462, val_loss=0.699, val_acc=0.749, val_f1_epoch=0.749, val_roc_epoch=0.506, train_loss_epoch=0.705, train_acc_epoch=0.739, train_f1_epoch=0.739, train_roc_epoch=0.518]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 25.74it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 21.93it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.48it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 22.41it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 23.36it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 22.83it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 23.76it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:00<00:00, 24.67it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:00<00:00, 25.42it/s]\u001b[A\n","Epoch 8: 100% 203/203 [00:23<00:00,  8.74it/s, v_num=qg7v, train_loss_step=0.705, train_acc_step=0.719, train_f1_step=0.719, train_roc_step=0.482, val_f1_step=0.688, val_roc_step=0.484, val_loss=0.699, val_acc=0.748, val_f1_epoch=0.748, val_roc_epoch=0.504, train_loss_epoch=0.703, train_acc_epoch=0.741, train_f1_epoch=0.741, train_roc_epoch=0.518]Epoch 8, global step 1827: 'val_loss' reached 0.69853 (best 0.69853), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 9: 100% 203/203 [00:20<00:00,  9.74it/s, v_num=qg7v, train_loss_step=0.706, train_acc_step=0.734, train_f1_step=0.734, train_roc_step=0.507, val_f1_step=0.688, val_roc_step=0.484, val_loss=0.699, val_acc=0.748, val_f1_epoch=0.748, val_roc_epoch=0.504, train_loss_epoch=0.703, train_acc_epoch=0.741, train_f1_epoch=0.741, train_roc_epoch=0.518]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 26.50it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 21.85it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 23.97it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 22.38it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 23.00it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 22.24it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 22.96it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.69it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 24.28it/s]\u001b[A\n","Epoch 9: 100% 203/203 [00:22<00:00,  9.13it/s, v_num=qg7v, train_loss_step=0.706, train_acc_step=0.734, train_f1_step=0.734, train_roc_step=0.507, val_f1_step=0.688, val_roc_step=0.499, val_loss=0.697, val_acc=0.752, val_f1_epoch=0.752, val_roc_epoch=0.508, train_loss_epoch=0.702, train_acc_epoch=0.745, train_f1_epoch=0.745, train_roc_epoch=0.513]Epoch 9, global step 2030: 'val_loss' reached 0.69667 (best 0.69667), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 10: 100% 203/203 [00:21<00:00,  9.45it/s, v_num=qg7v, train_loss_step=0.690, train_acc_step=0.742, train_f1_step=0.742, train_roc_step=0.537, val_f1_step=0.688, val_roc_step=0.499, val_loss=0.697, val_acc=0.752, val_f1_epoch=0.752, val_roc_epoch=0.508, train_loss_epoch=0.702, train_acc_epoch=0.745, train_f1_epoch=0.745, train_roc_epoch=0.513]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/25 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  12% 3/25 [00:00<00:00, 28.39it/s]\u001b[A\n","Validation DataLoader 0:  24% 6/25 [00:00<00:00, 22.10it/s]\u001b[A\n","Validation DataLoader 0:  36% 9/25 [00:00<00:00, 24.59it/s]\u001b[A\n","Validation DataLoader 0:  48% 12/25 [00:00<00:00, 22.41it/s]\u001b[A\n","Validation DataLoader 0:  60% 15/25 [00:00<00:00, 23.27it/s]\u001b[A\n","Validation DataLoader 0:  72% 18/25 [00:00<00:00, 21.89it/s]\u001b[A\n","Validation DataLoader 0:  84% 21/25 [00:00<00:00, 22.77it/s]\u001b[A\n","Validation DataLoader 0:  96% 24/25 [00:01<00:00, 23.34it/s]\u001b[A\n","Validation DataLoader 0: 100% 25/25 [00:01<00:00, 24.05it/s]\u001b[A\n","Epoch 10: 100% 203/203 [00:22<00:00,  8.87it/s, v_num=qg7v, train_loss_step=0.690, train_acc_step=0.742, train_f1_step=0.742, train_roc_step=0.537, val_f1_step=0.695, val_roc_step=0.497, val_loss=0.697, val_acc=0.766, val_f1_epoch=0.766, val_roc_epoch=0.517, train_loss_epoch=0.700, train_acc_epoch=0.750, train_f1_epoch=0.750, train_roc_epoch=0.514]Epoch 10, global step 2233: 'val_loss' reached 0.69654 (best 0.69654), saving model to '/content/logs/best-checkpoint-v9.ckpt' as top 1\n","Epoch 11:  77% 156/203 [00:15<00:04, 10.14it/s, v_num=qg7v, train_loss_step=0.705, train_acc_step=0.734, train_f1_step=0.734, train_roc_step=0.541, val_f1_step=0.695, val_roc_step=0.497, val_loss=0.697, val_acc=0.766, val_f1_epoch=0.766, val_roc_epoch=0.517, train_loss_epoch=0.700, train_acc_epoch=0.750, train_f1_epoch=0.750, train_roc_epoch=0.514]"]}],"source":["import os\n","import gc\n","from tensorflow.python.distribute.cluster_resolver import tpu_cluster_resolver\n","from tensorflow.python.profiler import profiler_client\n","import tensorflow as tf\n","\n","gc.collect()\n","os.environ[\"USE_TORCH\"] = \"ON\"\n","os.environ[\"TPU_LOG_DIR\"] = \"disabled\"\n","os.environ[\"_TPU_AVAILABLE\"] = \"1\"\n","os.environ[\"TPU_NUM_DEVICES\"] = \"8\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","os.environ[\"TRIM_GRAPH_SIZE\"] = \"1000000\"\n","# os.environ[\"MALLOC_MMAP_THRESHOLD_\"] = \"134961168\"\n","os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"1000000000\"\n","os.environ[\"PR_SET_PDEATHSIG\"] = \"1\"\n","os.environ[\"PL_RECONCILE_PROCESS\"] = \"1\"\n","os.environ[\"XLA_USE_BF16\"] = \"0\"\n","os.environ[\"XLA_DOWNCAST_BF16\"]=\"0\"\n","os.environ[\"XLA_USE_32BIT_LONG\"]=\"0\"\n","os.environ[\"XLA_IR_DEBUG\"] = \"0\"\n","os.environ[\"XLA_HLO_DEBUG\"] = \"0\"\n","os.environ[\"PT_XLA_DEBUG\"] = \"0\"\n","os.environ[\"XLA_SYNC_WAIT\"] = \"0\"\n","os.environ[\"XLA_GET_TENSORS_OPBYOP\"] = \"0\"\n","os.environ[\"XLA_SYNC_TENSORS_OPBYOP\"] = \"0\"\n","os.environ[\"XLA_CUDA\"] = \"0\"\n","os.environ[\"BUNDLE_LIBTPU\"] = \"1\"\n","os.environ[\"CURL_CA_BUNDLE\"] = \"\"\n","\n","tpu_name = os.environ.get(\"TPU_NAME\")\n","\n","compute_zone = os.environ.get(\"CLOUDSDK_COMPUTE_ZONE\")\n","core_project = os.environ.get(\"CLOUDSDK_CORE_PROJECT\")\n","\n","service_addr = tpu_cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR']).get_master()\n","\n","service_addr = service_addr.replace(\"grpc://\", \"\").replace(\":8470\", \":8466\")\n","\n","\n","result = profiler_client.monitor(service_addr, duration_ms=100, level=2)\n","tf.keras.backend.clear_session()\n","\n","# Set up the TPU\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","!sh trainer.sh"]},{"cell_type":"markdown","metadata":{"id":"p9aTSf8-oCQF"},"source":["<p>텐서보드 열기</p>\n","스크래치 셀에서 열어서 같이 보기 CTRL + ALT + \"N\"\n","\n","> xla profile default address => grpc://x.x.x.x:8466 \\\n"," xla client profile default address => localhost:9012\n","```shell\n","!echo [ $TPU_NAME ]\n","!export TPU_LOAD_LIBRARY=0\n","%load_ext tensorboard\n","%tensorboard --logdir=lightning_logs --load_fast=false --port 9001\n","```"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"1FlTETQ-ZBuYRL1EaCJcrEyIPM0c_uIYw","authorship_tag":"ABX9TyOsxK1SJdq/ESqzlLjfUgtz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}