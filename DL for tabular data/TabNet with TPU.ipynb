{"cells":[{"cell_type":"markdown","metadata":{"id":"6x0QOAp4v7jd"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"VcMoz_CLlWcW"},"source":["nn module 에서 TabNet 사용 시 제 성능을 내지 못하는 이유를 찾기 위해 처음부터 훑어보기"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":97302,"status":"ok","timestamp":1699404404393,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"nTo6loxkTyd9"},"outputs":[],"source":["%%capture\n","!pip install -U pytorch-tabnet lightning wandb wget jsonargparse[signatures]>=4.18.0\n","!pip install -U \\\n","    cloud-tpu-client==0.10 \\\n","    https://storage.googleapis.com/tpu-pytorch/tmp/colab_tmp_whl/torch_xla-2.0.0.dev20230516+colab-cp310-cp310-linux_x86_64.whl\n","!pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n","# !pip install -U \\\n","#     cloud-tpu-client==0.10 \\\n","#     https://storage.googleapis.com/tpu-pytorch/lsiyuan-experiment/wheel/torch_xla-2.1.0-cp310-cp310-linux_x86_64.whl"]},{"cell_type":"markdown","metadata":{"id":"PUhx1vM5jCme"},"source":["TabNet Tutorial 따라하기"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":3188,"status":"ok","timestamp":1699404407574,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6xFqRi5zbvqk","outputId":"425a24ac-d234-471c-be2b-ba609809fa33"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0+cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","import torch\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import roc_auc_score\n","\n","import pandas as pd\n","import numpy as np\n","np.random.seed(0)\n","\n","import scipy\n","\n","import os\n","import wget\n","from pathlib import Path\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import os\n","import torch\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = f\"1\"\n","torch.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1699404408159,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"WetUcZlRjqnJ","outputId":"d39c4a9a-4018-44b8-e4e6-be3ade7225a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading file...\n"]}],"source":["url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n","dataset_name = 'census-income'\n","out = Path(os.getcwd()+'/data/'+dataset_name+'.csv')\n","\n","out.parent.mkdir(parents=True, exist_ok=True)\n","if out.exists():\n","    print(\"File already exists.\")\n","else:\n","    print(\"Downloading file...\")\n","    wget.download(url, out.as_posix())"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1699404408591,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"4YZTjMPkjxzx","outputId":"f185c406-21a7-47b7-c899-ff84bf31c9c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["32560\n","39 73\n"," State-gov 9\n"," Bachelors 16\n"," 13 16\n"," Never-married 7\n"," Adm-clerical 15\n"," Not-in-family 6\n"," White 5\n"," Male 2\n"," 2174 119\n"," 0 92\n"," 40 94\n"," United-States 42\n"," <=50K 2\n","Set 3\n"]}],"source":["train = pd.read_csv(out)\n","print(len(train))\n","target = ' <=50K'\n","if \"Set\" not in train.columns:\n","    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n","\n","train_indices = train[train.Set==\"train\"].index\n","valid_indices = train[train.Set==\"valid\"].index\n","test_indices = train[train.Set==\"test\"].index\n","\n","nunique = train.nunique()\n","types = train.dtypes\n","\n","categorical_columns = []\n","categorical_dims =  {}\n","for col in train.columns:\n","    if types[col] == 'object' or nunique[col] < 200:\n","        print(col, train[col].nunique())\n","        l_enc = LabelEncoder()\n","        train[col] = train[col].fillna(\"VV_likely\")\n","        train[col] = l_enc.fit_transform(train[col].values)\n","        categorical_columns.append(col)\n","        categorical_dims[col] = len(l_enc.classes_)\n","    else:\n","        train.fillna(train.loc[train_indices, col].mean(), inplace=True)\n","# check that pipeline accepts strings\n","# train.loc[train[target]==0, target] = \"wealthy\"\n","# train.loc[train[target]==1, target] = \"not_wealthy\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699404408591,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"6v-oLA6Mj36p","outputId":"ec7887a2-fdcc-4960-8d1c-f4a90ec87eca"},"outputs":[{"output_type":"stream","name":"stdout","text":["             all features 14\n","             categoricals [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","   categorical dimensions [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42]\n"]}],"source":["unused_feat = ['Set']\n","\n","features = [ col for col in train.columns if col not in unused_feat+[target]]\n","# 카테고리 index\n","cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","# 카테고리별 class 개수\n","cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","\n","print(\"all features\".rjust(25,\" \"), len(features))\n","print(\"categoricals\".rjust(25,\" \"), cat_idxs)\n","print(\"categorical dimensions\".rjust(25,\" \") , cat_dims)\n","grouped_features = [[0, 1, 2], [8, 9, 10]]\n","\n","train.loc[train_indices, features+[target]].to_csv(\"train.csv\", index=False)\n","train.loc[valid_indices, features+[target]].to_csv(\"valid.csv\", index=False)\n","train.loc[test_indices, features+[target]].to_csv(\"test.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1699240880703,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"h_WpyooYj9EK","outputId":"88012d12-9bda-4380-d974-2b3923035aa6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]}],"source":["# steps, n_shared, n_independce 수정하지 않았음\n","tabnet_params = {\n","    \"cat_idxs\":cat_idxs,\n","    \"cat_dims\":cat_dims,\n","    \"cat_emb_dim\":2,\n","    \"n_d\": 16,\n","    \"n_a\": 16,\n","    \"n_independent\": 9,\n","    \"n_shared\": 4,\n","    \"n_steps\": 2,\n","    \"gamma\": 1.4690246460970766,\n","    \"lambda_sparse\": 0,\n","    \"optimizer_fn\": torch.optim.Adam,\n","    \"optimizer_params\":dict(lr=2e-2),\n","    \"scheduler_params\":{\n","        \"step_size\":50, # how to use learning rate scheduler\n","        \"gamma\":0.9},\n","    \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n","    \"mask_type\": 'entmax', # \"sparsemax\"\n","    \"grouped_features\" : grouped_features\n","   }\n","\n","clf = TabNetClassifier(**tabnet_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpJ4E0TUkHfN"},"outputs":[],"source":["X_train = train[features].values[train_indices]\n","y_train = train[target].values[train_indices]\n","\n","X_valid = train[features].values[valid_indices]\n","y_valid = train[target].values[valid_indices]\n","\n","X_test = train[features].values[test_indices]\n","y_test = train[target].values[test_indices]"]},{"cell_type":"markdown","metadata":{"id":"_9_oHkNflf9R"},"source":["놀랍게도 agumentations를 지원하고 있었다"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NQ-Nl1pikIg5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699404955860,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"2e273330-f858-44a7-bdae-730633c9d0ca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__call__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_set_seed',\n"," 'alpha',\n"," 'beta',\n"," 'device',\n"," 'p',\n"," 'seed']"]},"metadata":{},"execution_count":8}],"source":["max_epochs = 50 if not os.getenv(\"CI\", False) else 2\n","\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","aug = ClassificationSMOTE(p=0.2)\n","dir(aug)"]},{"cell_type":"code","source":["torch.randperm(64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lTc_7VWbzwc","executionInfo":{"status":"ok","timestamp":1699405076269,"user_tz":-540,"elapsed":261,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"0463a925-5e55-4beb-c3b5-7c1e945c70a7"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([49, 20, 35, 62, 32, 58, 25, 26,  8,  6, 42, 40,  5, 56, 33, 51, 34, 41,\n","        61, 38, 59,  1,  2, 48, 13, 47, 19, 12, 54, 43, 17, 57, 16, 36, 44, 63,\n","        15, 23, 39, 60, 27, 24,  9, 21,  0, 22, 31, 50, 46, 52, 37,  3, 14, 53,\n","        10, 55, 29,  7, 18, 30, 28, 45,  4, 11])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617648,"status":"ok","timestamp":1698627658503,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"CT0D2yDCkNt-","outputId":"8cdeb641-f53d-459e-a399-589d962baf49"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:25s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:46s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:58s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:01:09s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:01:22s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:01:32s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:01:45s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:01:55s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:02:12s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:02:25s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:02:37s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:02:48s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:02:59s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:03:11s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:03:22s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:03:35s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:03:45s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:03:59s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:04:08s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:04:22s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:04:31s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:04:54s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:05:08s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:05:17s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:05:30s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:05:39s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:05:53s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:06:02s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:06:15s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:06:24s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:06:38s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:06:47s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:07:00s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:07:10s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:07:22s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:07:32s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:07:44s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:07:55s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:08:07s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:08:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:08:29s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:08:41s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:08:52s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:09:04s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:09:15s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:09:27s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:09:37s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:09:50s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:10:00s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:10:13s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the behaviour of the model's fit method using Compressed Sparse Row matrices\n","sparse_X_train = scipy.sparse.csr_matrix(X_train)  # Create a CSR matrix from X_train\n","sparse_X_valid = scipy.sparse.csr_matrix(X_valid)  # Create a CSR matrix from X_valid\n","\n","# Fitting the model\n","clf.fit(\n","    X_train=sparse_X_train, y_train=y_train,\n","    eval_set=[(sparse_X_train, y_train), (sparse_X_valid, y_valid)],\n","    eval_name=['train', 'valid'],\n","    eval_metric=['auc'],\n","    max_epochs=max_epochs , patience=20,\n","    batch_size=1024, virtual_batch_size=128,\n","    num_workers=0,\n","    weights=1,\n","    drop_last=False,\n","    augmentations=aug, #aug, None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":979090,"status":"ok","timestamp":1698628637591,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jTvaq1hVkRou","outputId":"bf6ffce5-f571-40a4-d2ce-a0b8822a404c"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:14s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:20s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:33s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:39s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:58s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:03s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:16s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:22s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:40s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:46s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:05s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:23s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:29s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:41s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:54s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:01s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:12s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:21s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:30s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:37s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:55s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:01s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:38s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:48s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:52s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:56s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:01:02s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:08s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:12s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:16s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:21s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:27s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:32s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:36s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:40s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:46s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:52s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:56s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:02:00s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:02:05s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:11s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:16s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:20s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:24s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:29s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:36s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:40s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:44s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:48s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:54s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:03:00s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:04s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:12s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:18s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:27s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:31s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:37s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:43s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:47s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:51s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:55s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:04:01s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:07s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:03s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:07s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:13s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:19s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:23s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:37s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:43s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:56s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:02s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:07s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:21s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:27s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:39s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:45s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:51s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:55s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:59s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:04s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:10s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:15s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:19s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:28s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:34s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:42s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:47s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:53s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:02:58s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:02s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:06s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:11s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:17s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:22s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:25s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:29s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:35s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:41s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:45s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:49s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:54s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:00s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.64829 | train_auc: 0.77059 | valid_auc: 0.76213 |  0:00:04s\n","epoch 1  | loss: 0.50207 | train_auc: 0.85442 | valid_auc: 0.85818 |  0:00:08s\n","epoch 2  | loss: 0.47382 | train_auc: 0.86842 | valid_auc: 0.87363 |  0:00:12s\n","epoch 3  | loss: 0.4549  | train_auc: 0.88284 | valid_auc: 0.88606 |  0:00:18s\n","epoch 4  | loss: 0.43946 | train_auc: 0.8888  | valid_auc: 0.89114 |  0:00:24s\n","epoch 5  | loss: 0.42655 | train_auc: 0.8944  | valid_auc: 0.89693 |  0:00:27s\n","epoch 6  | loss: 0.42313 | train_auc: 0.90136 | valid_auc: 0.90255 |  0:00:31s\n","epoch 7  | loss: 0.41862 | train_auc: 0.9059  | valid_auc: 0.90625 |  0:00:36s\n","epoch 8  | loss: 0.41363 | train_auc: 0.90829 | valid_auc: 0.90865 |  0:00:42s\n","epoch 9  | loss: 0.40659 | train_auc: 0.91225 | valid_auc: 0.91197 |  0:00:47s\n","epoch 10 | loss: 0.40391 | train_auc: 0.91637 | valid_auc: 0.91706 |  0:00:51s\n","epoch 11 | loss: 0.39921 | train_auc: 0.91785 | valid_auc: 0.91749 |  0:00:55s\n","epoch 12 | loss: 0.38973 | train_auc: 0.92165 | valid_auc: 0.92229 |  0:01:00s\n","epoch 13 | loss: 0.39646 | train_auc: 0.92339 | valid_auc: 0.923   |  0:01:06s\n","epoch 14 | loss: 0.3871  | train_auc: 0.92385 | valid_auc: 0.92298 |  0:01:11s\n","epoch 15 | loss: 0.37618 | train_auc: 0.92586 | valid_auc: 0.92352 |  0:01:15s\n","epoch 16 | loss: 0.38287 | train_auc: 0.92615 | valid_auc: 0.92429 |  0:01:19s\n","epoch 17 | loss: 0.37443 | train_auc: 0.92639 | valid_auc: 0.92488 |  0:01:25s\n","epoch 18 | loss: 0.37641 | train_auc: 0.92879 | valid_auc: 0.92728 |  0:01:31s\n","epoch 19 | loss: 0.37322 | train_auc: 0.92808 | valid_auc: 0.92581 |  0:01:35s\n","epoch 20 | loss: 0.36986 | train_auc: 0.929   | valid_auc: 0.92708 |  0:01:38s\n","epoch 21 | loss: 0.36173 | train_auc: 0.92957 | valid_auc: 0.92718 |  0:01:43s\n","epoch 22 | loss: 0.37056 | train_auc: 0.9309  | valid_auc: 0.92795 |  0:01:50s\n","epoch 23 | loss: 0.37027 | train_auc: 0.93164 | valid_auc: 0.9281  |  0:01:54s\n","epoch 24 | loss: 0.36806 | train_auc: 0.93162 | valid_auc: 0.92712 |  0:01:58s\n","epoch 25 | loss: 0.36685 | train_auc: 0.9318  | valid_auc: 0.92688 |  0:02:02s\n","epoch 26 | loss: 0.36392 | train_auc: 0.93168 | valid_auc: 0.92757 |  0:02:08s\n","epoch 27 | loss: 0.36053 | train_auc: 0.93085 | valid_auc: 0.92738 |  0:02:14s\n","epoch 28 | loss: 0.36073 | train_auc: 0.9325  | valid_auc: 0.92707 |  0:02:18s\n","epoch 29 | loss: 0.36707 | train_auc: 0.93328 | valid_auc: 0.92766 |  0:02:22s\n","epoch 30 | loss: 0.36733 | train_auc: 0.93306 | valid_auc: 0.9278  |  0:02:26s\n","epoch 31 | loss: 0.35663 | train_auc: 0.93454 | valid_auc: 0.92881 |  0:02:32s\n","epoch 32 | loss: 0.35313 | train_auc: 0.93429 | valid_auc: 0.9288  |  0:02:38s\n","epoch 33 | loss: 0.35391 | train_auc: 0.93586 | valid_auc: 0.92874 |  0:02:46s\n","epoch 34 | loss: 0.35954 | train_auc: 0.93501 | valid_auc: 0.9277  |  0:02:52s\n","epoch 35 | loss: 0.35075 | train_auc: 0.93614 | valid_auc: 0.92656 |  0:02:58s\n","epoch 36 | loss: 0.35976 | train_auc: 0.93622 | valid_auc: 0.92686 |  0:03:03s\n","epoch 37 | loss: 0.35083 | train_auc: 0.93723 | valid_auc: 0.92864 |  0:03:07s\n","epoch 38 | loss: 0.35093 | train_auc: 0.93735 | valid_auc: 0.92861 |  0:03:11s\n","epoch 39 | loss: 0.35654 | train_auc: 0.93769 | valid_auc: 0.92701 |  0:03:17s\n","epoch 40 | loss: 0.34763 | train_auc: 0.93908 | valid_auc: 0.92782 |  0:03:23s\n","epoch 41 | loss: 0.3486  | train_auc: 0.93877 | valid_auc: 0.9281  |  0:03:26s\n","epoch 42 | loss: 0.35497 | train_auc: 0.94002 | valid_auc: 0.92788 |  0:03:30s\n","epoch 43 | loss: 0.34803 | train_auc: 0.94017 | valid_auc: 0.92638 |  0:03:35s\n","epoch 44 | loss: 0.34356 | train_auc: 0.93994 | valid_auc: 0.92704 |  0:03:41s\n","epoch 45 | loss: 0.34472 | train_auc: 0.93825 | valid_auc: 0.92729 |  0:03:46s\n","epoch 46 | loss: 0.3487  | train_auc: 0.94134 | valid_auc: 0.92615 |  0:03:50s\n","epoch 47 | loss: 0.34507 | train_auc: 0.94101 | valid_auc: 0.92766 |  0:03:54s\n","epoch 48 | loss: 0.33594 | train_auc: 0.94114 | valid_auc: 0.92662 |  0:03:59s\n","epoch 49 | loss: 0.34213 | train_auc: 0.94045 | valid_auc: 0.92489 |  0:04:06s\n","Stop training because you reached max_epochs = 50 with best_epoch = 31 and best_valid_auc = 0.92881\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# This illustrates the warm_start=False behaviour\n","save_history = []\n","\n","# Fitting the model without starting from a warm start nor computing the feature importance\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=False\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))\n","\n","save_history = []  # Resetting the list to show that it also works when computing feature importance\n","\n","# Fitting the model without starting from a warm start but with the computing of the feature importance activated\n","for _ in range(2):\n","    clf.fit(\n","        X_train=X_train, y_train=y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        eval_name=['train', 'valid'],\n","        eval_metric=['auc'],\n","        max_epochs=max_epochs , patience=20,\n","        batch_size=1024, virtual_batch_size=128,\n","        num_workers=0,\n","        weights=1,\n","        drop_last=False,\n","        augmentations=aug, #aug, None\n","        compute_importance=True # True by default so not needed\n","    )\n","    save_history.append(clf.history[\"valid_auc\"])\n","\n","assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uq7dwumBlr7z"},"outputs":[],"source":["# plot losses\n","plt.plot(clf.history['loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A52swQzcls_U"},"outputs":[],"source":["# plot auc\n","plt.plot(clf.history['train_auc'])\n","plt.plot(clf.history['valid_auc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qY0wTE3lt5k"},"outputs":[],"source":["# plot learning rates\n","plt.plot(clf.history['lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbZ5ydVIlvs4"},"outputs":[],"source":["preds = clf.predict_proba(X_test)\n","test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n","\n","\n","preds_valid = clf.predict_proba(X_valid)\n","valid_auc = roc_auc_score(y_score=preds_valid[:,1], y_true=y_valid)\n","\n","print(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")\n","# check that best weights are used\n","assert np.isclose(valid_auc, np.max(clf.history['valid_auc']), atol=1e-6)\n","clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBrjfLhbl02o"},"outputs":[],"source":["# save tabnet model\n","saving_path_name = \"./tabnet_model_test_1\"\n","saved_filepath = clf.save_model(saving_path_name)\n","\n","# define new model with basic parameters and load state dict weights\n","loaded_clf = TabNetClassifier()\n","loaded_clf.load_model(saved_filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9kfTTcAl4RY"},"outputs":[],"source":["loaded_preds = loaded_clf.predict_proba(X_test)\n","loaded_test_auc = roc_auc_score(y_score=loaded_preds[:,1], y_true=y_test)\n","\n","print(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_auc}\")\n","\n","assert(test_auc == loaded_test_auc)\n","loaded_clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IalieiADl8SZ"},"outputs":[],"source":["# Global explainability : feat importance summing to 1\n","clf.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3sio5i_l-XO"},"outputs":[],"source":["explain_matrix, masks = clf.explain(X_test)\n","fig, axs = plt.subplots(1, 3, figsize=(20,20))\n","\n","for i in range(3):\n","    axs[i].imshow(masks[i][:50])\n","    axs[i].set_title(f\"mask {i}\")\n","    axs[i].set_xticklabels(labels = features, rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"Xcqx5Qygjm_E"},"source":["Lightning 으로 네트워크 학습하기"]},{"cell_type":"code","execution_count":19,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1699407487848,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"jizvAZvn1Ppo","outputId":"66c4f74e-eaa2-4fe5-d35d-3229daa0296c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting tabnet.py\n"]}],"source":["%%writefile tabnet.py\n","# @title <b><i>Training 파일 저장<i/></b>\n","import argparse\n","import logging\n","import os\n","import gc\n","import sys\n","import time\n","import pickle\n","from typing import Any, Dict, Optional, Type\n","# import dgl\n","# import deepchem as dc\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.special import softmax\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics import F1Score, AUROC, Accuracy\n","from torch.optim import SGD, Adam, NAdam, RAdam, SparseAdam, LBFGS, RMSprop\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CyclicLR\n","from torchsummary import summary\n","\n","# from dgl.dataloading import GraphDataLoader\n","\n","from pytorch_tabnet.tab_network import TabNetPretraining, TabNet\n","from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n","from pytorch_tabnet.utils import create_group_matrix, create_explain_matrix\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","\n","import wandb\n","from lightning.fabric.utilities import apply_func\n","import lightning.pytorch as pl\n","from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n","from lightning.pytorch.callbacks import (\n","    TQDMProgressBar, EarlyStopping, LearningRateMonitor,\n","    ModelCheckpoint, StochasticWeightAveraging, DeviceStatsMonitor)\n","from lightning.pytorch.plugins.io import XLACheckpointIO\n","from lightning.pytorch.cli import LightningCLI, ArgsType\n","from lightning_utilities.core.imports import module_available\n","\n","sys.setrecursionlimit(10**7)\n","sys.set_int_max_str_digits(0)\n","isxla = module_available(\"torch_xla\")\n","print(f'Can Use Torch_XLA? {isxla}')\n","\n","if isxla:\n","    import torch_xla\n","    import torch_xla.core.xla_model as xm\n","    import torch_xla.distributed.xla_multiprocessing as xmp\n","    import torch_xla.utils.serialization as xser\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.WARNING)\n","\n","\n","class ArgsCLI(LightningCLI):\n","    def add_arguments_to_parser(self, parser):\n","        parser.add_argument('--train_file',\n","                            type=str,\n","                            default='dev_t.csv',\n","                            help='train file')\n","\n","        parser.add_argument('--test_file',\n","                            type=str,\n","                            default='dev_v.csv',\n","                            help='test file')\n","\n","        parser.add_argument('--accumulate_grad_batches',\n","                            type=int,\n","                            default=1,\n","                            help='accumulate_grad_batches')\n","\n","        parser.add_argument('--num_workers',\n","                            type=int,\n","                            default=2,\n","                            help='num of worker for dataloader')\n","\n","        parser.add_argument('--batch_size',\n","                            type=int,\n","                            default=32,\n","                            help='batch size for training (default: 96)')\n","\n","        parser.add_argument('--lr',\n","                            type=float,\n","                            default=5e-7,\n","                            help='The initial learning rate')\n","\n","        parser.add_argument('--warmup_ratio',\n","                            type=float,\n","                            default=0.1,\n","                            help='warmup ratio')\n","\n","        parser.add_argument('--checkpoint_path',\n","                            type=str,\n","                            help='checkpoint path')\n","\n","        parser.add_argument('--default_root_dir',\n","                            type=str,\n","                            help='default_root_dir')\n","\n","\n","class CensusDataset(Dataset):\n","    def __init__(self,\n","                 filepath: str,) -> None:\n","        super().__init__()\n","        self.data = pd.read_csv(filepath)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def update_attributes(self, features, idxs, dims):\n","        self.features=features\n","        self.cat_ids = idxs\n","        self.cat_dims = dims\n","\n","    def make_input_id_mask(self,\n","                           tokens):\n","        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = [1] * len(input_id)\n","        if len(input_id) < self.max_seq_len:\n","            while len(input_id) < self.max_seq_len:\n","                input_id += [self.tokenizer.pad_token_id]\n","                attention_mask += [0]\n","        else:\n","            input_id = input_id[:self.max_seq_len - 1] + [self.tokenizer.eos_token_id]\n","            attention_mask = attention_mask[:self.max_seq_len]\n","        return input_id, attention_mask\n","\n","    def masking(self,\n","                input,\n","                mask):\n","        input = torch.tensor([input])\n","        rand = torch.rand(input.shape)\n","        mask_arr = (rand < 0.15) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n","        mask_ids = torch.flatten((mask_arr[0]).nonzero()).tolist()\n","        input[0, mask_ids] = mask\n","        del rand, mask_arr\n","        return input.tolist()[0]\n","\n","    def _labeling(self,\n","                  label):\n","        tokens = [self.tokenizer.bos_token]+self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n","        label_ids = self.tokenizer.convert_tokens_to_ids(tokens[1:])\n","        if len(label_ids) < self.max_seq_len:\n","            while len(label_ids)<self.max_seq_len:\n","                label_ids+=[-100]\n","        else:\n","            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n","        del tokens\n","        return label_ids\n","\n","    def _scale_transform(self, values):\n","        return self.scaler.transform(values.reshape(1,-1))\n","\n","    def __getitem__(self,\n","                    index:int):\n","        assert self.features\n","        record = self.data.iloc[index]\n","        dod  = record[' <=50K']\n","        tabs = record[self.features].values\n","        return {\n","            'tabnet_input': tabs.astype('float32'),\n","            'labels': np.array(dod, dtype=np.float32)\n","        }\n","\n","\n","class CensusDataModule(pl.core.LightningDataModule):\n","    def __init__(self,\n","                 train_file: str,\n","                 valid_file: str,\n","                 test_file: str,\n","                 batch_size: int = 32,\n","                 num_workers: int = 8):\n","        super().__init__()\n","        self.batch_size = batch_size # batch_size\n","        self.train_file_path = train_file\n","        self.valid_file_path = valid_file\n","        self.test_file_path = test_file\n","        self.num_workers = num_workers\n","        self.prepare_data_pre_node = True\n","    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n","\n","    def setup(self,\n","              stage):\n","        # split dataset\n","        self.train = CensusDataset(self.train_file_path)\n","        self.valid = CensusDataset(self.valid_file_path)\n","        self.test = CensusDataset(self.test_file_path)\n","        features, idxs, dims = self.get_features()\n","        self.train.update_attributes(features, idxs, dims)\n","        self.valid.update_attributes(features, idxs, dims)\n","        self.test.update_attributes(features, idxs, dims)\n","\n","    def get_features(self):\n","        df = pd.concat([self.train.data, self.valid.data, self.test.data])\n","        target = ' <=50K'\n","        nunique = df.nunique()\n","        types = df.dtypes\n","        categorical_columns = []\n","        categorical_dims =  {}\n","        for col in df.columns:\n","            if types[col] == 'object' or nunique[col] < 200:\n","                l_enc = LabelEncoder()\n","                df[col] = df[col].fillna(\"VV_likely\")\n","                df[col] = l_enc.fit_transform(df[col].values)\n","                categorical_columns.append(col)\n","                categorical_dims[col] = len(l_enc.classes_)\n","            else:\n","                df.fillna(df.median(), inplace=True)\n","        unused_feat = ['Set']\n","        features = [ col for col in df.columns if col not in unused_feat+[target]]\n","        cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n","        cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","        return features, cat_idxs, cat_dims\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=True,\n","                          drop_last=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.valid,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers= self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False,\n","                          drop_last=True)\n","\n","\n","class Base(pl.core.LightningModule):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs) -> None:\n","        super().__init__()\n","        self.lr = lr\n","        self.warmup_ratio = warmup\n","        self.batch_size = batch_size\n","\n","    def decay_params(self, named_params):\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'BatchNorm1d.bias', 'BatchNorm1d.weight']\n","        return [\n","            {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","    def configure_optimizers(self):\n","        # Prepare optimizer\n","\n","        optimizer_grouped_parameters = self.network.parameters()\n","        # optimizer_grouped_parameters = self.decay_params(self.network.named_parameters())\n","\n","        optimizer = Adam(params=optimizer_grouped_parameters,\n","                        lr=self.lr,)\n","        # optimizer = RMSprop(params=optimizer_grouped_parameters,\n","        #                    momentum=0.9,\n","        #                    lr=self.lr)\n","\n","        # warm up lr\n","        num_nodes = self.trainer.strategy.num_nodes if not \"Single\" in str(type(self.trainer.strategy)) else 1\n","        devices = len(self.trainer.strategy.parallel_devices) if \"parallel_devices\" in self.trainer.strategy.__dict__.keys() else 1\n","        num_devices = int(devices) * int(num_nodes)\n","        data_len = len(self.trainer.fit_loop._data_source.instance.train)\n","       # data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n","        logging.info(f'number of devices {num_devices}, data length {data_len}')\n","        first = (data_len / (self.batch_size * num_devices))\n","        num_train_steps = int(first * self.trainer.max_epochs)\n","        logging.info(f'num_train_steps : {num_train_steps}')\n","\n","        num_warmup_steps = int(num_train_steps * 0.01)\n","        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n","\n","        scheduler = CyclicLR(\n","                 optimizer=optimizer,\n","                 base_lr=5e-5,\n","                 max_lr=self.lr,\n","                 cycle_momentum=False,\n","                 step_size_up=num_warmup_steps,)\n","\n","        self.lr_scheduler = {\n","            \"scheduler\": scheduler,\n","            \"name\": \"lr_log\",\n","            \"monitor\": \"loss\",\n","            \"interval\": \"step\",\n","            \"frequency\": self.batch_size}\n","        # return optimizer\n","        return [optimizer], [self.lr_scheduler]\n","\n","    def backward(self, loss):\n","        with torch.autograd.set_detect_anomaly(True):\n","            loss.requires_grad_().to(self.device)\n","            loss.backward()\n","\n","\n","\n","class Tabnet(torch.nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 output_dim: int,\n","                 batch_size: int,\n","                 cat_idx:list,\n","                 cat_dim:list,\n","                 device: str = \"cpu\"):\n","        super(Tabnet, self).__init__()\n","        self.cat_idx = cat_idx\n","        self.cat_dim = cat_dim\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.batch_size = batch_size\n","        self.net = TabNet(\n","            cat_idxs=cat_idx,\n","            cat_dims=cat_dim,\n","            cat_emb_dim=[3]*len(cat_idx),\n","            input_dim=input_dim,\n","            output_dim=output_dim,\n","            n_a=2, # Dimension of the prediction layer\n","            n_d=2, # Dimension of the attention layer\n","            n_steps=3, # Output까지의 Attentive-Feature transformer layer\n","            n_independent=1, # Decision step dependent 단계 GLU layer\n","            n_shared=1, # Shared across decision steps 단계 GLU layer\n","            epsilon=1e-5,\n","            gamma=1.3,\n","            mask_type=\"sparsemax\", # sparsemax / entmax\n","            momentum=0.01,\n","            virtual_batch_size=128,\n","            group_attention_matrix=create_group_matrix([[0, 1, 2,], [8, 9, 10,]], input_dim).to(device)\n","           # torch.eye(input_dim, device=device)\n","            )\n","        self.reducing_matrix = create_explain_matrix(\n","                    self.net.input_dim,\n","                    self.net.cat_emb_dim,\n","                    self.net.cat_idxs,\n","                    self.net.post_embed_dim,)\n","        self.aug = ClassificationSMOTE(p=0.2)\n","\n","\n","    def _re_init(self, device, state_dict=None):\n","       # if \"xla\" in device:\n","        del self.net, self.reducing_matrix\n","        self.__init__(\n","                input_dim=self.input_dim,\n","                output_dim=self.output_dim,\n","                batch_size=self.batch_size,\n","                cat_idx=self.cat_idx,\n","                cat_dim=self.cat_dim,\n","                device=device)\n","        if state_dict:\n","            self.load_state_dict(state_dict)\n","        self.to(device)\n","\n","    def forward(self, x):\n","        x, _ = self.aug(x, None)\n","        for param in self.net.parameters():\n","                param.grad = None\n","        return self.net(x)\n","\n","    def parameteres(self):\n","        return self.net.parameters()\n","\n","class LightningTabNet(Base):\n","    def __init__(self,\n","                 lr: float,\n","                 batch_size: int,\n","                 warmup: float = 0.1,\n","                 **kwargs):\n","        super().__init__(lr, warmup, batch_size, **kwargs)\n","        # self.automatic_optimization=False\n","\n","        model_dim = 2 # @param {type:\"integer\"}\n","        cat_input_dim = 14 # @param {type:\"integer\"}\n","\n","        self.network = Tabnet(\n","            input_dim=cat_input_dim,\n","            output_dim=model_dim,\n","            batch_size=batch_size,\n","            cat_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n","            cat_dim =  [73, 9, 16, 16, 7, 15, 6, 5, 2, 119, 92, 94, 42])\n","        self.loss = torch.nn.CrossEntropyLoss()\n","        self.f1 = F1Score(task=\"multiclass\", num_classes=2)\n","        self.acc = Accuracy(task=\"multiclass\", num_classes=2)\n","        self.roc = AUROC(task=\"multiclass\", num_classes=2)\n","       # self.concat = torch.nn.Bilinear(model_dim, model_dim, model_dim,)\n","\n","    def _loss(self, yhat, y, M_loss):\n","        pred = torch.softmax(yhat, dim=1).to(self.device)\n","        acc = self.acc(pred, y)\n","        f1 = self.f1(pred, y)\n","        roc = self.roc(pred, y.long())\n","        clf_loss = self.loss(pred, y.long()) - (1.3 * M_loss)\n","        clf_loss.requires_grad_()\n","        clf_loss.to(self.device)\n","        return clf_loss, acc, f1, roc\n","\n","    def initialize(self, m):\n","        for layer in m.modules():\n","            if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.Bilinear):\n","                torch.nn.init.xavier_uniform_(layer.weight, gain=1.0)\n","\n","   # def teardown(self,\n","   #              trainer: pl.Trainer,\n","   #              pl_module: pl.LightningModule,\n","   #              stage: str) -> None:\n","   #     print(f\"{stage} Done\")\n","\n","    def on_fit_start(self,) -> None:\n","        self.network._re_init(self.device) # Multiprocessing 하는 경우 device에 올라가는 문제\n","       # self.lm.init_weights()\n","       # self.initialize(self.tabnet.net)\n","       # print(\"Training with Lightning Module\\n\")\n","\n","    def on_predict_start(self,) -> None:\n","        state_dict = self.network.state_dict()\n","       # self.tabnet._re_init(self.device, state_dict)\n","\n","   # def on_fit_end(self,) -> None:\n","   #     if xm.is_master_ordinal():\n","   #         print(\"\\nTrain Done\\n\")\n","   #         # save_hf_repo(self.model.cpu(), self.tokenizer)\n","\n","    def forward(self,\n","                inputs):\n","        output, M_loss = self.network(inputs[\"tabnet_input\"])\n","       # output = torch.sum(torch.nan_to_num(torch.stack(output)), dim=0)\n","        return output, M_loss\n","\n","   # def on_train_epoch_start(self):\n","   #     self.lm.train()\n","   #     self.tabnet.train()\n","\n","    def training_step(self,\n","                      batch,\n","                      batch_idx: int):\n","       # opt = self.optimizers()\n","        outs, M_loss = self(batch)\n","        loss, dod_acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_dod_acc\", dod_acc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","\n","       # opt.zero_grad()\n","       # self.manual_backward(loss)\n","       # opt.step()\n","\n","        return loss\n","\n","    def validation_step(self,\n","                        batch,\n","                        batch_idx: int):\n","        outs, M_loss = self(batch)\n","        loss, dod_acc, f1, roc = self._loss(\n","            yhat=outs,\n","            y=batch[\"labels\"],\n","            M_loss=M_loss)\n","        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_dod_acc\", dod_acc, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"val_f1\", f1, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"val_roc\", roc, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","\n","\n","    # def on_training_batch_end(self,\n","    #                           outputs,\n","    #                           batch,\n","    #                           batch_idx) -> None:\n","    #     print(\"Train Batch End\")\n","\n","    # def on_training_epoch_end(self,) -> None:\n","    #     print(\"Train Epoch End\")\n","\n","    # def validation_batch_end(self,\n","    #                          outputs,\n","    #                          batch,\n","    #                          batch_idx,\n","    #                          dataloader_idx=0) -> None:\n","    #     print(\"Valid Batch End\")\n","\n","    # def validation_epoch_end(self,) -> None:\n","    #     print(\"Valid Epoch End\")\n","\n","    # def on_save_checkpoint(self,\n","    #                        trainer: pl.Trainer,\n","    #                        pl_module: pl.LightningModule=None,\n","    #                        checkpoint: Dict[str,Any]=None) -> None:\n","    #     checkpoint[\"lighitning_logs/checkpoint\"]=self\n","    #     if xm.is_master_ordinal():\n","    #         print(f\"Trainer save checkpoint\")\n","    #         save_hf_repo(self.model.cpu(), self.model.tokenizer)\n","    #         print(\"\\nCheckpoint saved\\n\")\n","\n","def sweep_agent():\n","    sweep_config = {\n","        'method': 'random',\n","        'name': 'first_sweep',\n","        'metric': {\n","            'goal': 'minimize',\n","            'name': 'training_loss'\n","        },\n","        'parameters': {\n","            'n_hidden': {'values': [2,3,5,10]},\n","            'lr': {'max': 1.0, 'min': 0.0001}\n","        }\n","    }\n","    sweep_id = wandb.sweep(sweep_config, project=\"Tabnet_XLA\")\n","    wandb.agent(sweep_id=sweep_id, function=cli_main, count=5)\n","\n","\n","def cli_main() -> None: # args:ArgsType=None\n","    callbacks = [ # StochasticWeightAveraging(swa_lrs=1e-2), # 23/04/24 기준 lightning에서 오류 있는 callback\n","        ModelCheckpoint(monitor=\"val_loss\",\n","                        dirpath=\"logs/\",\n","                        filename=\"best-checkpoint\",\n","                        verbose=True,\n","                        save_last=True,\n","                        mode=\"min\",\n","                        save_top_k=1),\n","        DeviceStatsMonitor(),\n","        EarlyStopping(monitor=\"val_loss\",\n","                      mode=\"min\",\n","                      stopping_threshold=1e-4,\n","                      min_delta=0.00,\n","                      patience=1,\n","                      divergence_threshold=9.0),\n","        LearningRateMonitor(logging_interval=\"step\"),\n","        TQDMProgressBar(refresh_rate=3)]\n","    logger = WandbLogger(project=\"Tabnet_XLA\", log_model=\"all\")\n","    cli = ArgsCLI(model_class=LightningTabNet,\n","                  datamodule_class=CensusDataModule,\n","                  save_config_kwargs={\"overwrite\": True},\n","                  trainer_defaults={\n","                    # \"plugins\": [XLACheckpointIO(),],\n","                      \"callbacks\": callbacks,\n","                      \"reload_dataloaders_every_n_epochs\": 1,\n","                      \"detect_anomaly\": True,\n","                      \"logger\": [logger]\n","                      },)\n","\n","\n","if __name__ == \"__main__\":\n","    pl.cli_lightning_logo()\n","    wandb.init(project=\"Tabnet_XLA\")\n","    cli_main()\n","\n","   # save_path = \"/content/drive/MyDrive/data_kevin/ChemGPT_finetuned.pth\"\n","   # torch.save(model.model.state_dict(),save_path)\n","    print(\"\\nMain Done...\\n\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1699405666163,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"ibzfCMq_Mhhe","outputId":"3b0b122a-61b2-4c92-c7d5-391fc3aaaa5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting trainer.sh\n"]}],"source":["%%writefile trainer.sh\n","TRAIN_FILE=\"train.csv\" #@param ['\"train.csv\"'] {type:\"raw\"}\n","VAL_FILE=\"valid.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","TEST_FILE=\"test.csv\" #@param ['\"test.csv\"'] {type:\"raw\"}\n","PERCISION=32 #@param [\"bf16-true\", \"16-true\", \"32\", \"bf16-mixed\"] {type:\"raw\", allow-input: true}\n","LEARNING_RATE=2e-02 #@param {type:\"raw\"}\n","GRADIENT_CLIP=1.0 #@param {type:\"raw\"}\n","ACCUMULATE_GRAD_BATCHES=1 #@param {type:\"raw\"}\n","BATCH_SIZE=16 #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"512\", \"4096\", \"32768\", \"262144\", \"2097152\", \"16777216\", \"134217728\", \"1073741824\", \"8589934592\", \"68719476736\"] {type:\"raw\", allow-input: true}\n","MAXEPOCHS=100 #@param {type:\"raw\"}\n","STRATEGY=auto #@param [\"auto\", \"bagua\", \"colossalai\", \"collaborative\", \"ddp\", \"ddp2\", \"ddp_find_unused_parameters_false\", \"ddp_fully_sharded\", \"ddp_sharded\", \"ddp_sharded_find_unused_parameters_false\", \"ddp_sharded_spawn\", \"ddp_sharded_spawn_find_unused_parameters_false\", \"ddp_spawn\", \"ddp_spawn_find_unused_parameters_false\", \"ddp_fork\" ,\"deepspeed\", \"deepspeed_stage_1\", \"deepspeed_stage_2\", \"deepspeed_stage_2_offload\", \"deepspeed_stage_3\", \"deepspeed_stage_3_offload\", \"deepspeed_stage_3_offload_nvme\", \"dp\", \"fsdp\", \"fsdp_native\", \"horovod\", \"hpu_parallel\", \"hpu_single\", \"ipu_strategy\", \"single_device\", \"single_tpu\", \"tpu_spawn\", \"tpu_spawn_debug\", \"xla\", \"xla_debug\"]{type:\"raw\", allow-input: false}\n","NUMWORKERS=2 #@param {type:\"raw\"}\n","WARMUP=0.1 #@param {type:\"raw\"}\n","PROFILE=pytorch #@param [\"simple\", \"advanced\", \"pytorch\", \"xla\"]  {type:\"raw\", allow-input: false}\n","FAST_DEV_RUN=false #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","SYNC_BATCHNORM=true #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n","ACCELERATOR=cpu #@param [\"cpu\", \"gpu\", \"tpu\"] {type:\"raw\", allow-input: true}\n","CORES=1 #@param {type:\"raw\"}\n","\n","python tabnet.py fit\\\n","    --trainer.accumulate_grad_batches $ACCUMULATE_GRAD_BATCHES\\\n","    --trainer.gradient_clip_val $GRADIENT_CLIP\\\n","    --trainer.max_epochs $MAXEPOCHS\\\n","    --trainer.precision $PERCISION\\\n","    --trainer.accelerator $ACCELERATOR\\\n","    --trainer.devices $CORES\\\n","    --trainer.use_distributed_sampler false\\\n","    --trainer.sync_batchnorm $SYNC_BATCHNORM\\\n","    --trainer.profiler $PROFILE\\\n","    --trainer.strategy $STRATEGY\\\n","    --trainer.num_sanity_val_steps 3\\\n","    --trainer.log_every_n_steps 2\\\n","    --trainer.fast_dev_run $FAST_DEV_RUN\\\n","    --model.lr $LEARNING_RATE\\\n","    --model.batch_size $BATCH_SIZE\\\n","    --data.num_workers $NUMWORKERS\\\n","    --data.batch_size $BATCH_SIZE\\\n","    --model.warmup $WARMUP\\\n","    --data.train_file $TRAIN_FILE\\\n","    --data.valid_file $VAL_FILE\\\n","    --data.test_file $TEST_FILE\n","\n","exit 0"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRMpyxzTP6tn","executionInfo":{"status":"ok","timestamp":1699414181605,"user_tz":-540,"elapsed":548185,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"4f723166-40f5-42d3-da6a-55e7a1f91e69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Can Use Torch_XLA? True\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevintb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","2023-11-08 03:20:41.054625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231108_032039-gqub3ro8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlogical-valley-10\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/Tabnet_XLA\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/Tabnet_XLA/runs/gqub3ro8\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 352322811\n","Seed set to 352322811\n","You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n","GPU available: False, used: False\n","TPU available: True, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/setup.py:193: TPU available but not used. You can set it by doing `Trainer(accelerator='tpu')`.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","/usr/local/lib/python3.10/dist-packages/jsonargparse/_typehints.py:1223: JsonargparseWarning: \n","    Unable to serialize instance <lightning.pytorch.loggers.wandb.WandbLogger object at 0x79df7c8edff0>\n","\n","  warning(val)\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /content/logs exists and is not empty.\n","\n","  | Name    | Type               | Params\n","-----------------------------------------------\n","0 | network | Tabnet             | 2.3 K \n","1 | loss    | CrossEntropyLoss   | 0     \n","2 | f1      | MulticlassF1Score  | 0     \n","3 | acc     | MulticlassAccuracy | 0     \n","4 | roc     | MulticlassAUROC    | 0     \n","-----------------------------------------------\n","2.3 K     Trainable params\n","0         Non-trainable params\n","2.3 K     Total params\n","0.009     Total estimated model params size (MB)\n","Sanity Checking DataLoader 0:   0% 0/3 [00:00<?, ?it/s]STAGE:2023-11-08 03:20:44 50268:50268 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n","Epoch 0:   0% 3/1629 [00:16<2:25:48,  5.38s/it, v_num=3ro8, train_loss_step=1.970, train_dod_acc_step=0.688, train_f1_step=0.688, train_roc_step=0.550]STAGE:2023-11-08 03:21:00 50268:50268 ActivityProfilerController.cpp:317] Completed Stage: Collection\n","STAGE:2023-11-08 03:21:00 50268:50268 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n","[W collection.cpp:496] Warning: Optimizer.step#Adam.step (function operator())\n","[W collection.cpp:496] Warning: [pl][profile][LightningModule]LightningTabNet.optimizer_step (function operator())\n","Epoch 0:   3% 57/1629 [00:23<10:55,  2.40it/s, v_num=3ro8, train_loss_step=1.790, train_dod_acc_step=0.812, train_f1_step=0.812, train_roc_step=0.795]/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n","Epoch 0: 100% 1629/1629 [03:02<00:00,  8.94it/s, v_num=3ro8, train_loss_step=1.950, train_dod_acc_step=0.812, train_f1_step=0.812, train_roc_step=0.792]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/201 [00:00<?, ?it/s]      \u001b[A\n","Validation DataLoader 0:   0% 0/201 [00:00<?, ?it/s]\u001b[ASTAGE:2023-11-08 03:23:46 50268:50268 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n","STAGE:2023-11-08 03:23:47 50268:50268 ActivityProfilerController.cpp:317] Completed Stage: Collection\n","STAGE:2023-11-08 03:23:47 50268:50268 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n","\n","Validation DataLoader 0:   1% 3/201 [00:00<00:18, 10.47it/s]\u001b[A\n","Validation DataLoader 0:   3% 6/201 [00:00<00:10, 18.03it/s]\u001b[A\n","Validation DataLoader 0:   4% 9/201 [00:00<00:08, 23.79it/s]\u001b[A\n","Validation DataLoader 0:   6% 12/201 [00:00<00:06, 28.26it/s]\u001b[A\n","Validation DataLoader 0:   7% 15/201 [00:00<00:05, 31.90it/s]\u001b[A\n","Validation DataLoader 0:   9% 18/201 [00:00<00:05, 34.95it/s]\u001b[A\n","Validation DataLoader 0:  10% 21/201 [00:00<00:04, 37.46it/s]\u001b[A\n","Validation DataLoader 0:  12% 24/201 [00:00<00:04, 39.63it/s]\u001b[A\n","Validation DataLoader 0:  13% 27/201 [00:00<00:04, 41.49it/s]\u001b[A\n","Validation DataLoader 0:  15% 30/201 [00:00<00:03, 43.07it/s]\u001b[A\n","Validation DataLoader 0:  16% 33/201 [00:00<00:03, 44.39it/s]\u001b[A\n","Validation DataLoader 0:  18% 36/201 [00:00<00:03, 45.68it/s]\u001b[A\n","Validation DataLoader 0:  19% 39/201 [00:00<00:03, 46.81it/s]\u001b[A\n","Validation DataLoader 0:  21% 42/201 [00:00<00:03, 47.84it/s]\u001b[A\n","Validation DataLoader 0:  22% 45/201 [00:00<00:03, 48.74it/s]\u001b[A\n","Validation DataLoader 0:  24% 48/201 [00:00<00:03, 49.56it/s]\u001b[A\n","Validation DataLoader 0:  25% 51/201 [00:01<00:02, 50.30it/s]\u001b[A\n","Validation DataLoader 0:  27% 54/201 [00:01<00:02, 51.00it/s]\u001b[A\n","Validation DataLoader 0:  28% 57/201 [00:01<00:02, 51.66it/s]\u001b[A\n","Validation DataLoader 0:  30% 60/201 [00:01<00:02, 52.26it/s]\u001b[A\n","Validation DataLoader 0:  31% 63/201 [00:01<00:02, 52.82it/s]\u001b[A\n","Validation DataLoader 0:  33% 66/201 [00:01<00:02, 53.34it/s]\u001b[A\n","Validation DataLoader 0:  34% 69/201 [00:01<00:02, 53.81it/s]\u001b[A\n","Validation DataLoader 0:  36% 72/201 [00:01<00:02, 54.26it/s]\u001b[A\n","Validation DataLoader 0:  37% 75/201 [00:01<00:02, 54.66it/s]\u001b[A\n","Validation DataLoader 0:  39% 78/201 [00:01<00:02, 55.04it/s]\u001b[A\n","Validation DataLoader 0:  40% 81/201 [00:01<00:02, 55.36it/s]\u001b[A\n","Validation DataLoader 0:  42% 84/201 [00:01<00:02, 55.71it/s]\u001b[A\n","Validation DataLoader 0:  43% 87/201 [00:01<00:02, 56.00it/s]\u001b[A\n","Validation DataLoader 0:  45% 90/201 [00:01<00:01, 56.29it/s]\u001b[A\n","Validation DataLoader 0:  46% 93/201 [00:01<00:01, 56.60it/s]\u001b[A\n","Validation DataLoader 0:  48% 96/201 [00:01<00:01, 56.85it/s]\u001b[A\n","Validation DataLoader 0:  49% 99/201 [00:01<00:01, 57.13it/s]\u001b[A\n","Validation DataLoader 0:  51% 102/201 [00:01<00:01, 57.39it/s]\u001b[A\n","Validation DataLoader 0:  52% 105/201 [00:01<00:01, 57.64it/s]\u001b[A\n","Validation DataLoader 0:  54% 108/201 [00:01<00:01, 57.88it/s]\u001b[A\n","Validation DataLoader 0:  55% 111/201 [00:01<00:01, 58.07it/s]\u001b[A\n","Validation DataLoader 0:  57% 114/201 [00:01<00:01, 58.20it/s]\u001b[A\n","Validation DataLoader 0:  58% 117/201 [00:02<00:01, 58.19it/s]\u001b[A\n","Validation DataLoader 0:  60% 120/201 [00:02<00:01, 58.37it/s]\u001b[A\n","Validation DataLoader 0:  61% 123/201 [00:02<00:01, 58.53it/s]\u001b[A\n","Validation DataLoader 0:  63% 126/201 [00:02<00:01, 58.72it/s]\u001b[A\n","Validation DataLoader 0:  64% 129/201 [00:02<00:01, 58.89it/s]\u001b[A\n","Validation DataLoader 0:  66% 132/201 [00:02<00:01, 59.04it/s]\u001b[A\n","Validation DataLoader 0:  67% 135/201 [00:02<00:01, 59.01it/s]\u001b[A\n","Validation DataLoader 0:  69% 138/201 [00:02<00:01, 59.17it/s]\u001b[A\n","Validation DataLoader 0:  70% 141/201 [00:02<00:01, 59.33it/s]\u001b[A\n","Validation DataLoader 0:  72% 144/201 [00:02<00:00, 59.48it/s]\u001b[A\n","Validation DataLoader 0:  73% 147/201 [00:02<00:00, 59.59it/s]\u001b[A\n","Validation DataLoader 0:  75% 150/201 [00:02<00:00, 59.67it/s]\u001b[A\n","Validation DataLoader 0:  76% 153/201 [00:02<00:00, 59.78it/s]\u001b[A\n","Validation DataLoader 0:  78% 156/201 [00:02<00:00, 59.90it/s]\u001b[A\n","Validation DataLoader 0:  79% 159/201 [00:02<00:00, 60.02it/s]\u001b[A\n","Validation DataLoader 0:  81% 162/201 [00:02<00:00, 60.13it/s]\u001b[A\n","Validation DataLoader 0:  82% 165/201 [00:02<00:00, 60.20it/s]\u001b[A\n","Validation DataLoader 0:  84% 168/201 [00:02<00:00, 60.31it/s]\u001b[A\n","Validation DataLoader 0:  85% 171/201 [00:02<00:00, 60.41it/s]\u001b[A\n","Validation DataLoader 0:  87% 174/201 [00:02<00:00, 60.51it/s]\u001b[A\n","Validation DataLoader 0:  88% 177/201 [00:02<00:00, 60.57it/s]\u001b[A\n","Validation DataLoader 0:  90% 180/201 [00:02<00:00, 60.67it/s]\u001b[A\n","Validation DataLoader 0:  91% 183/201 [00:03<00:00, 60.75it/s]\u001b[A\n","Validation DataLoader 0:  93% 186/201 [00:03<00:00, 60.84it/s]\u001b[A\n","Validation DataLoader 0:  94% 189/201 [00:03<00:00, 60.92it/s]\u001b[A\n","Validation DataLoader 0:  96% 192/201 [00:03<00:00, 61.02it/s]\u001b[A\n","Validation DataLoader 0:  97% 195/201 [00:03<00:00, 61.11it/s]\u001b[A\n","Validation DataLoader 0:  99% 198/201 [00:03<00:00, 61.19it/s]\u001b[A\n","Validation DataLoader 0: 100% 201/201 [00:03<00:00, 61.28it/s]\u001b[A\n","Epoch 0: 100% 1629/1629 [03:05<00:00,  8.77it/s, v_num=3ro8, train_loss_step=1.950, train_dod_acc_step=0.812, train_f1_step=0.812, train_roc_step=0.792, val_f1_step=0.875, val_roc_step=0.897, val_loss=2.080, val_dod_acc=0.685, val_f1_epoch=0.685, val_roc_epoch=0.526, train_loss_epoch=1.930, train_dod_acc_epoch=0.688, train_f1_epoch=0.688, train_roc_epoch=0.533]Epoch 0, global step 1629: 'val_loss' reached 2.08322 (best 2.08322), saving model to '/content/logs/best-checkpoint-v5.ckpt' as top 1\n","Epoch 1: 100% 1629/1629 [02:42<00:00, 10.02it/s, v_num=3ro8, train_loss_step=1.850, train_dod_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.462, val_f1_step=0.875, val_roc_step=0.897, val_loss=2.080, val_dod_acc=0.685, val_f1_epoch=0.685, val_roc_epoch=0.526, train_loss_epoch=1.930, train_dod_acc_epoch=0.688, train_f1_epoch=0.688, train_roc_epoch=0.533]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/201 [00:00<?, ?it/s]      \u001b[A\n","Validation DataLoader 0:   0% 0/201 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   1% 3/201 [00:00<00:04, 48.45it/s]\u001b[A\n","Validation DataLoader 0:   3% 6/201 [00:00<00:03, 54.73it/s]\u001b[A\n","Validation DataLoader 0:   4% 9/201 [00:00<00:03, 57.71it/s]\u001b[A\n","Validation DataLoader 0:   6% 12/201 [00:00<00:03, 59.38it/s]\u001b[A\n","Validation DataLoader 0:   7% 15/201 [00:00<00:03, 60.33it/s]\u001b[A\n","Validation DataLoader 0:   9% 18/201 [00:00<00:03, 60.94it/s]\u001b[A\n","Validation DataLoader 0:  10% 21/201 [00:00<00:02, 61.47it/s]\u001b[A\n","Validation DataLoader 0:  12% 24/201 [00:00<00:02, 62.17it/s]\u001b[A\n","Validation DataLoader 0:  13% 27/201 [00:00<00:02, 62.53it/s]\u001b[A\n","Validation DataLoader 0:  15% 30/201 [00:00<00:02, 63.05it/s]\u001b[A\n","Validation DataLoader 0:  16% 33/201 [00:00<00:02, 63.25it/s]\u001b[A\n","Validation DataLoader 0:  18% 36/201 [00:00<00:02, 63.50it/s]\u001b[A\n","Validation DataLoader 0:  19% 39/201 [00:00<00:02, 63.71it/s]\u001b[A\n","Validation DataLoader 0:  21% 42/201 [00:00<00:02, 64.01it/s]\u001b[A\n","Validation DataLoader 0:  22% 45/201 [00:00<00:02, 64.24it/s]\u001b[A\n","Validation DataLoader 0:  24% 48/201 [00:00<00:02, 64.59it/s]\u001b[A\n","Validation DataLoader 0:  25% 51/201 [00:00<00:02, 64.60it/s]\u001b[A\n","Validation DataLoader 0:  27% 54/201 [00:00<00:02, 64.92it/s]\u001b[A\n","Validation DataLoader 0:  28% 57/201 [00:00<00:02, 65.03it/s]\u001b[A\n","Validation DataLoader 0:  30% 60/201 [00:00<00:02, 65.17it/s]\u001b[A\n","Validation DataLoader 0:  31% 63/201 [00:00<00:02, 65.31it/s]\u001b[A\n","Validation DataLoader 0:  33% 66/201 [00:01<00:02, 65.33it/s]\u001b[A\n","Validation DataLoader 0:  34% 69/201 [00:01<00:02, 65.33it/s]\u001b[A\n","Validation DataLoader 0:  36% 72/201 [00:01<00:01, 65.38it/s]\u001b[A\n","Validation DataLoader 0:  37% 75/201 [00:01<00:01, 65.52it/s]\u001b[A\n","Validation DataLoader 0:  39% 78/201 [00:01<00:01, 65.67it/s]\u001b[A\n","Validation DataLoader 0:  40% 81/201 [00:01<00:01, 65.81it/s]\u001b[A\n","Validation DataLoader 0:  42% 84/201 [00:01<00:01, 65.93it/s]\u001b[A\n","Validation DataLoader 0:  43% 87/201 [00:01<00:01, 65.99it/s]\u001b[A\n","Validation DataLoader 0:  45% 90/201 [00:01<00:01, 66.05it/s]\u001b[A\n","Validation DataLoader 0:  46% 93/201 [00:01<00:01, 66.17it/s]\u001b[A\n","Validation DataLoader 0:  48% 96/201 [00:01<00:01, 66.27it/s]\u001b[A\n","Validation DataLoader 0:  49% 99/201 [00:01<00:01, 66.38it/s]\u001b[A\n","Validation DataLoader 0:  51% 102/201 [00:01<00:01, 66.42it/s]\u001b[A\n","Validation DataLoader 0:  52% 105/201 [00:01<00:01, 66.44it/s]\u001b[A\n","Validation DataLoader 0:  54% 108/201 [00:01<00:01, 66.18it/s]\u001b[A\n","Validation DataLoader 0:  55% 111/201 [00:01<00:01, 66.28it/s]\u001b[A\n","Validation DataLoader 0:  57% 114/201 [00:01<00:01, 66.37it/s]\u001b[A\n","Validation DataLoader 0:  58% 117/201 [00:01<00:01, 66.46it/s]\u001b[A\n","Validation DataLoader 0:  60% 120/201 [00:01<00:01, 66.49it/s]\u001b[A\n","Validation DataLoader 0:  61% 123/201 [00:01<00:01, 66.57it/s]\u001b[A\n","Validation DataLoader 0:  63% 126/201 [00:01<00:01, 66.67it/s]\u001b[A\n","Validation DataLoader 0:  64% 129/201 [00:01<00:01, 66.45it/s]\u001b[A\n","Validation DataLoader 0:  66% 132/201 [00:01<00:01, 66.28it/s]\u001b[A\n","Validation DataLoader 0:  67% 135/201 [00:02<00:00, 66.12it/s]\u001b[A\n","Validation DataLoader 0:  69% 138/201 [00:02<00:00, 66.08it/s]\u001b[A\n","Validation DataLoader 0:  70% 141/201 [00:02<00:00, 66.09it/s]\u001b[A\n","Validation DataLoader 0:  72% 144/201 [00:02<00:00, 66.10it/s]\u001b[A\n","Validation DataLoader 0:  73% 147/201 [00:02<00:00, 66.07it/s]\u001b[A\n","Validation DataLoader 0:  75% 150/201 [00:02<00:00, 66.07it/s]\u001b[A\n","Validation DataLoader 0:  76% 153/201 [00:02<00:00, 66.06it/s]\u001b[A\n","Validation DataLoader 0:  78% 156/201 [00:02<00:00, 66.07it/s]\u001b[A\n","Validation DataLoader 0:  79% 159/201 [00:02<00:00, 66.08it/s]\u001b[A\n","Validation DataLoader 0:  81% 162/201 [00:02<00:00, 66.14it/s]\u001b[A\n","Validation DataLoader 0:  82% 165/201 [00:02<00:00, 66.21it/s]\u001b[A\n","Validation DataLoader 0:  84% 168/201 [00:02<00:00, 66.24it/s]\u001b[A\n","Validation DataLoader 0:  85% 171/201 [00:02<00:00, 66.28it/s]\u001b[A\n","Validation DataLoader 0:  87% 174/201 [00:02<00:00, 66.32it/s]\u001b[A\n","Validation DataLoader 0:  88% 177/201 [00:02<00:00, 66.38it/s]\u001b[A\n","Validation DataLoader 0:  90% 180/201 [00:02<00:00, 66.44it/s]\u001b[A\n","Validation DataLoader 0:  91% 183/201 [00:02<00:00, 66.50it/s]\u001b[A\n","Validation DataLoader 0:  93% 186/201 [00:02<00:00, 66.54it/s]\u001b[A\n","Validation DataLoader 0:  94% 189/201 [00:02<00:00, 66.59it/s]\u001b[A\n","Validation DataLoader 0:  96% 192/201 [00:02<00:00, 66.65it/s]\u001b[A\n","Validation DataLoader 0:  97% 195/201 [00:02<00:00, 66.68it/s]\u001b[A\n","Validation DataLoader 0:  99% 198/201 [00:02<00:00, 66.74it/s]\u001b[A\n","Validation DataLoader 0: 100% 201/201 [00:03<00:00, 66.79it/s]\u001b[A\n","Epoch 1: 100% 1629/1629 [02:45<00:00,  9.82it/s, v_num=3ro8, train_loss_step=1.850, train_dod_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.462, val_f1_step=0.750, val_roc_step=0.462, val_loss=2.080, val_dod_acc=0.690, val_f1_epoch=0.690, val_roc_epoch=0.531, train_loss_epoch=1.930, train_dod_acc_epoch=0.688, train_f1_epoch=0.688, train_roc_epoch=0.529]Epoch 1, global step 3258: 'val_loss' reached 2.07916 (best 2.07916), saving model to '/content/logs/best-checkpoint-v5.ckpt' as top 1\n","Epoch 2: 100% 1629/1629 [02:42<00:00, 10.01it/s, v_num=3ro8, train_loss_step=1.940, train_dod_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.154, val_f1_step=0.750, val_roc_step=0.462, val_loss=2.080, val_dod_acc=0.690, val_f1_epoch=0.690, val_roc_epoch=0.531, train_loss_epoch=1.930, train_dod_acc_epoch=0.688, train_f1_epoch=0.688, train_roc_epoch=0.529]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/201 [00:00<?, ?it/s]      \u001b[A\n","Validation DataLoader 0:   0% 0/201 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   1% 3/201 [00:00<00:04, 47.44it/s]\u001b[A\n","Validation DataLoader 0:   3% 6/201 [00:00<00:03, 55.81it/s]\u001b[A\n","Validation DataLoader 0:   4% 9/201 [00:00<00:03, 59.15it/s]\u001b[A\n","Validation DataLoader 0:   6% 12/201 [00:00<00:03, 62.04it/s]\u001b[A\n","Validation DataLoader 0:   7% 15/201 [00:00<00:03, 61.17it/s]\u001b[A\n","Validation DataLoader 0:   9% 18/201 [00:00<00:03, 56.87it/s]\u001b[A\n","Validation DataLoader 0:  10% 21/201 [00:00<00:03, 57.11it/s]\u001b[A\n","Validation DataLoader 0:  12% 24/201 [00:00<00:03, 58.75it/s]\u001b[A\n","Validation DataLoader 0:  13% 27/201 [00:00<00:02, 60.26it/s]\u001b[A\n","Validation DataLoader 0:  15% 30/201 [00:00<00:02, 61.47it/s]\u001b[A\n","Validation DataLoader 0:  16% 33/201 [00:00<00:02, 62.62it/s]\u001b[A\n","Validation DataLoader 0:  18% 36/201 [00:00<00:02, 63.53it/s]\u001b[A\n","Validation DataLoader 0:  19% 39/201 [00:00<00:02, 64.33it/s]\u001b[A\n","Validation DataLoader 0:  21% 42/201 [00:00<00:02, 65.13it/s]\u001b[A\n","Validation DataLoader 0:  22% 45/201 [00:00<00:02, 65.72it/s]\u001b[A\n","Validation DataLoader 0:  24% 48/201 [00:00<00:02, 66.01it/s]\u001b[A\n","Validation DataLoader 0:  25% 51/201 [00:00<00:02, 66.52it/s]\u001b[A\n","Validation DataLoader 0:  27% 54/201 [00:00<00:02, 66.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 57/201 [00:00<00:02, 67.33it/s]\u001b[A\n","Validation DataLoader 0:  30% 60/201 [00:00<00:02, 67.44it/s]\u001b[A\n","Validation DataLoader 0:  31% 63/201 [00:00<00:02, 67.59it/s]\u001b[A\n","Validation DataLoader 0:  33% 66/201 [00:00<00:01, 67.75it/s]\u001b[A\n","Validation DataLoader 0:  34% 69/201 [00:01<00:01, 67.81it/s]\u001b[A\n","Validation DataLoader 0:  36% 72/201 [00:01<00:01, 67.71it/s]\u001b[A\n","Validation DataLoader 0:  37% 75/201 [00:01<00:01, 67.60it/s]\u001b[A\n","Validation DataLoader 0:  39% 78/201 [00:01<00:01, 67.63it/s]\u001b[A\n","Validation DataLoader 0:  40% 81/201 [00:01<00:01, 67.75it/s]\u001b[A\n","Validation DataLoader 0:  42% 84/201 [00:01<00:01, 67.94it/s]\u001b[A\n","Validation DataLoader 0:  43% 87/201 [00:01<00:01, 68.18it/s]\u001b[A\n","Validation DataLoader 0:  45% 90/201 [00:01<00:01, 68.45it/s]\u001b[A\n","Validation DataLoader 0:  46% 93/201 [00:01<00:01, 68.71it/s]\u001b[A\n","Validation DataLoader 0:  48% 96/201 [00:01<00:01, 68.76it/s]\u001b[A\n","Validation DataLoader 0:  49% 99/201 [00:01<00:01, 68.91it/s]\u001b[A\n","Validation DataLoader 0:  51% 102/201 [00:01<00:01, 69.03it/s]\u001b[A\n","Validation DataLoader 0:  52% 105/201 [00:01<00:01, 69.17it/s]\u001b[A\n","Validation DataLoader 0:  54% 108/201 [00:01<00:01, 69.15it/s]\u001b[A\n","Validation DataLoader 0:  55% 111/201 [00:01<00:01, 69.19it/s]\u001b[A\n","Validation DataLoader 0:  57% 114/201 [00:01<00:01, 69.30it/s]\u001b[A\n","Validation DataLoader 0:  58% 117/201 [00:01<00:01, 69.37it/s]\u001b[A\n","Validation DataLoader 0:  60% 120/201 [00:01<00:01, 69.37it/s]\u001b[A\n","Validation DataLoader 0:  61% 123/201 [00:01<00:01, 69.24it/s]\u001b[A\n","Validation DataLoader 0:  63% 126/201 [00:01<00:01, 69.34it/s]\u001b[A\n","Validation DataLoader 0:  64% 129/201 [00:01<00:01, 69.39it/s]\u001b[A\n","Validation DataLoader 0:  66% 132/201 [00:01<00:00, 69.32it/s]\u001b[A\n","Validation DataLoader 0:  67% 135/201 [00:01<00:00, 69.38it/s]\u001b[A\n","Validation DataLoader 0:  69% 138/201 [00:01<00:00, 69.38it/s]\u001b[A\n","Validation DataLoader 0:  70% 141/201 [00:02<00:00, 69.28it/s]\u001b[A\n","Validation DataLoader 0:  72% 144/201 [00:02<00:00, 69.43it/s]\u001b[A\n","Validation DataLoader 0:  73% 147/201 [00:02<00:00, 69.58it/s]\u001b[A\n","Validation DataLoader 0:  75% 150/201 [00:02<00:00, 69.73it/s]\u001b[A\n","Validation DataLoader 0:  76% 153/201 [00:02<00:00, 69.87it/s]\u001b[A\n","Validation DataLoader 0:  78% 156/201 [00:02<00:00, 69.95it/s]\u001b[A\n","Validation DataLoader 0:  79% 159/201 [00:02<00:00, 69.95it/s]\u001b[A\n","Validation DataLoader 0:  81% 162/201 [00:02<00:00, 69.96it/s]\u001b[A\n","Validation DataLoader 0:  82% 165/201 [00:02<00:00, 69.99it/s]\u001b[A\n","Validation DataLoader 0:  84% 168/201 [00:02<00:00, 70.01it/s]\u001b[A\n","Validation DataLoader 0:  85% 171/201 [00:02<00:00, 69.98it/s]\u001b[A\n","Validation DataLoader 0:  87% 174/201 [00:02<00:00, 69.88it/s]\u001b[A\n","Validation DataLoader 0:  88% 177/201 [00:02<00:00, 69.83it/s]\u001b[A\n","Validation DataLoader 0:  90% 180/201 [00:02<00:00, 69.73it/s]\u001b[A\n","Validation DataLoader 0:  91% 183/201 [00:02<00:00, 69.62it/s]\u001b[A\n","Validation DataLoader 0:  93% 186/201 [00:02<00:00, 69.58it/s]\u001b[A\n","Validation DataLoader 0:  94% 189/201 [00:02<00:00, 69.50it/s]\u001b[A\n","Validation DataLoader 0:  96% 192/201 [00:02<00:00, 69.51it/s]\u001b[A\n","Validation DataLoader 0:  97% 195/201 [00:02<00:00, 69.48it/s]\u001b[A\n","Validation DataLoader 0:  99% 198/201 [00:02<00:00, 69.53it/s]\u001b[A\n","Validation DataLoader 0: 100% 201/201 [00:02<00:00, 69.53it/s]\u001b[A\n","Epoch 2: 100% 1629/1629 [02:46<00:00,  9.81it/s, v_num=3ro8, train_loss_step=1.940, train_dod_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.154, val_f1_step=0.812, val_roc_step=0.538, val_loss=2.090, val_dod_acc=0.693, val_f1_epoch=0.693, val_roc_epoch=0.540, train_loss_epoch=1.930, train_dod_acc_epoch=0.691, train_f1_epoch=0.691, train_roc_epoch=0.533]Epoch 2, global step 4887: 'val_loss' was not in top 1\n","Epoch 2: 100% 1629/1629 [02:46<00:00,  9.81it/s, v_num=3ro8, train_loss_step=1.940, train_dod_acc_step=0.750, train_f1_step=0.750, train_roc_step=0.154, val_f1_step=0.812, val_roc_step=0.538, val_loss=2.090, val_dod_acc=0.693, val_f1_epoch=0.693, val_roc_epoch=0.540, train_loss_epoch=1.930, train_dod_acc_epoch=0.691, train_f1_epoch=0.691, train_roc_epoch=0.533]\n","FIT Profiler Report\n","Profile stats for: records\n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                          ProfilerStep*        12.35%       2.828ms       100.00%      22.898ms      22.898ms             1  \n","[pl][profile][Strategy]SingleDeviceStrategy.validati...        23.23%       5.319ms        70.08%      16.048ms      16.048ms             1  \n","                [pl][profile][_EvaluationLoop].val_next         0.38%      86.000us        14.10%       3.229ms       3.229ms             1  \n","enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        13.46%       3.083ms        13.73%       3.143ms       3.143ms             1  \n","                                      SparsemaxFunction        10.22%       2.340ms        13.27%       3.038ms       1.013ms             3  \n","[pl][module]torchmetrics.classification.auroc.Multic...         4.73%       1.083ms         9.06%       2.074ms       2.074ms             1  \n","[pl][module]torchmetrics.classification.accuracy.Mul...         3.14%     719.000us         4.69%       1.073ms       1.073ms             1  \n","[pl][module]torchmetrics.classification.f_beta.Multi...         2.54%     582.000us         3.82%     875.000us     875.000us             1  \n","                                       aten::batch_norm         0.18%      42.000us         3.19%     730.000us      60.833us            12  \n","                           aten::_batch_norm_impl_index         0.24%      56.000us         3.00%     688.000us      57.333us            12  \n","                                               aten::to         0.52%     118.000us         2.88%     659.000us       4.955us           133  \n","                                aten::native_batch_norm         2.32%     531.000us         2.64%     604.000us      50.333us            12  \n","                                         aten::_to_copy         1.21%     278.000us         2.47%     565.000us       6.348us            89  \n","                                           aten::linear         0.17%      40.000us         1.57%     359.000us      29.917us            12  \n","                                            aten::slice         1.25%     287.000us         1.47%     337.000us       3.304us           102  \n","                                              aten::sum         1.04%     239.000us         1.45%     333.000us      12.333us            27  \n","[pl][profile][Strategy]SingleDeviceStrategy.batch_to...         0.81%     186.000us         1.39%     318.000us     318.000us             1  \n","                                              aten::mul         0.97%     221.000us         1.34%     307.000us       6.977us            44  \n","                                            aten::index         0.83%     191.000us         1.31%     299.000us      18.688us            16  \n","                                           aten::matmul         0.20%      45.000us         1.21%     277.000us      18.467us            15  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 22.898ms\n","\n","\n","Main Done...\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:             DeviceStatsMonitor.on_train_batch_end/cpu_percent ▇▆█▇▆▇▆▆▆▇█▆▇▆▇▆▆▆▇▆▆▇▆▆▇▆▆▆▆▇▁▅▇▆▆▇▇▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_train_batch_end/cpu_swap_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          DeviceStatsMonitor.on_train_batch_end/cpu_vm_percent ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:           DeviceStatsMonitor.on_train_batch_start/cpu_percent ▅▁██▃▄▅▁▃▅█▃▆▅▅▅▂▂▆▂▂▅▂▅▅▂▁▂▂▅▁▂▅▆▃▆▄▃▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      DeviceStatsMonitor.on_train_batch_start/cpu_swap_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_train_batch_start/cpu_vm_percent ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_validation_batch_end/cpu_percent ▅▄▆▅▅▄▅▆▃▆▇█▆▆▆▆▇▆▆▅▇▂▅▃▆▃▂▅▃▃▄▅▅▄▄▃▃▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   DeviceStatsMonitor.on_validation_batch_end/cpu_swap_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     DeviceStatsMonitor.on_validation_batch_end/cpu_vm_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      DeviceStatsMonitor.on_validation_batch_start/cpu_percent ▅▅▅▄▁█▅▅█▄▁▄▅█▅▅▄▇██▅▁▄▄▅▁▄▅▄▄▄▅▅▁▅▄▄▅▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m: DeviceStatsMonitor.on_validation_batch_start/cpu_swap_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   DeviceStatsMonitor.on_validation_batch_start/cpu_vm_percent ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                         epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        lr_log ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                           train_dod_acc_epoch ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                            train_dod_acc_step ▆▂▇▃▁▅▄▇▇▅▅▅▅▅█▅▅▅▅▂▅▄▃▄▄▅█▅▅▃▄▄▅▃▅▆▃▇▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train_f1_epoch ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                 train_f1_step ▆▂▇▃▁▅▄▇▇▅▅▅▅▅█▅▅▅▅▂▅▄▃▄▄▅█▅▅▃▄▄▅▃▅▆▃▇▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                              train_loss_epoch ▁▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train_loss_step ▆▁▃▇█▆▇▄▄▅▆▆█▄▅▅▅▃▄▇▃▇▇▄▄▄▄▅▆▆▄▇▅█▆▆▆▄▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train_roc_epoch █▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train_roc_step ▆▄▅▃▄▅▃▆▅▄▂▆▂▁█▅▄▁▅▄▄▄▁▁▄▅▃▅▆▅▄▃▄▃▂▇▅█▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                           trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▁▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   val_dod_acc ▁▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  val_f1_epoch ▁▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   val_f1_step ▆██▆▃▆▅▅▆▁▄▅▅▇▆█▇▅▄▄▃▃▇▅▃▆▄▅▂▆▆▄▅▇▅▄▂█▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                      val_loss ▄▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                 val_roc_epoch ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  val_roc_step ▇▇▇▃▁▅▅▄▄▂▆▅▅▅▅▇▇▂▃▃▅▅█▅▄█▅▃▃▃▅▃▄▅▄▄▅▅▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:             DeviceStatsMonitor.on_train_batch_end/cpu_percent 23.4\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_train_batch_end/cpu_swap_percent 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:          DeviceStatsMonitor.on_train_batch_end/cpu_vm_percent 34.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:           DeviceStatsMonitor.on_train_batch_start/cpu_percent 22.8\n","\u001b[34m\u001b[1mwandb\u001b[0m:      DeviceStatsMonitor.on_train_batch_start/cpu_swap_percent 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_train_batch_start/cpu_vm_percent 34.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        DeviceStatsMonitor.on_validation_batch_end/cpu_percent 30.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:   DeviceStatsMonitor.on_validation_batch_end/cpu_swap_percent 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:     DeviceStatsMonitor.on_validation_batch_end/cpu_vm_percent 26.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:      DeviceStatsMonitor.on_validation_batch_start/cpu_percent 7.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: DeviceStatsMonitor.on_validation_batch_start/cpu_swap_percent 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:   DeviceStatsMonitor.on_validation_batch_start/cpu_vm_percent 26.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                         epoch 2\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        lr_log 5e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                           train_dod_acc_epoch 0.69053\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                            train_dod_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train_f1_epoch 0.69053\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                 train_f1_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                              train_loss_epoch 1.93283\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train_loss_step 1.93925\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train_roc_epoch 0.53296\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train_roc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                           trainer/global_step 4886\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   val_dod_acc 0.69279\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  val_f1_epoch 0.69279\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   val_f1_step 0.8125\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                      val_loss 2.08951\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                 val_roc_epoch 0.54021\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  val_roc_step 0.53846\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlogical-valley-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/Tabnet_XLA/runs/gqub3ro8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231108_032039-gqub3ro8/logs\u001b[0m\n"]}],"source":["import os\n","import gc\n","from tensorflow.python.distribute.cluster_resolver import tpu_cluster_resolver\n","from tensorflow.python.profiler import profiler_client\n","import tensorflow as tf\n","\n","gc.collect()\n","os.environ[\"USE_TORCH\"] = \"ON\"\n","os.environ[\"TPU_LOG_DIR\"] = \"disabled\"\n","os.environ[\"_TPU_AVAILABLE\"] = \"1\"\n","os.environ[\"TPU_NUM_DEVICES\"] = \"8\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","os.environ[\"TRIM_GRAPH_SIZE\"] = \"1000000\"\n","# os.environ[\"MALLOC_MMAP_THRESHOLD_\"] = \"134961168\"\n","os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"1000000000\"\n","os.environ[\"PR_SET_PDEATHSIG\"] = \"1\"\n","os.environ[\"PL_RECONCILE_PROCESS\"] = \"1\"\n","os.environ[\"XLA_USE_BF16\"] = \"0\"\n","os.environ[\"XLA_DOWNCAST_BF16\"]=\"0\"\n","os.environ[\"XLA_USE_32BIT_LONG\"]=\"0\"\n","os.environ[\"XLA_IR_DEBUG\"] = \"0\"\n","os.environ[\"XLA_HLO_DEBUG\"] = \"0\"\n","os.environ[\"PT_XLA_DEBUG\"] = \"0\"\n","os.environ[\"XLA_SYNC_WAIT\"] = \"0\"\n","os.environ[\"XLA_GET_TENSORS_OPBYOP\"] = \"0\"\n","os.environ[\"XLA_SYNC_TENSORS_OPBYOP\"] = \"0\"\n","os.environ[\"XLA_CUDA\"] = \"0\"\n","os.environ[\"BUNDLE_LIBTPU\"] = \"1\"\n","os.environ[\"CURL_CA_BUNDLE\"] = \"\"\n","\n","tpu_name = os.environ.get(\"TPU_NAME\")\n","\n","compute_zone = os.environ.get(\"CLOUDSDK_COMPUTE_ZONE\")\n","core_project = os.environ.get(\"CLOUDSDK_CORE_PROJECT\")\n","\n","\n","\n","service_addr = tpu_cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR']).get_master()\n","\n","service_addr = service_addr.replace(\"grpc://\", \"\").replace(\":8470\", \":8466\")\n","\n","\n","result = profiler_client.monitor(service_addr, duration_ms=100, level=2)\n","tf.keras.backend.clear_session()\n","\n","# Set up the TPU\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","!sh trainer.sh"]},{"cell_type":"markdown","metadata":{"id":"p9aTSf8-oCQF"},"source":["<p>텐서보드 열기</p>\n","스크래치 셀에서 열어서 같이 보기 CTRL + ALT + \"N\"\n","\n","> xla profile default address => grpc://x.x.x.x:8466 \\\n"," xla client profile default address => localhost:9012\n","```shell\n","!echo [ $TPU_NAME ]\n","!export TPU_LOAD_LIBRARY=0\n","%load_ext tensorboard\n","%tensorboard --logdir=lightning_logs --load_fast=false --port 9001\n","```"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"1FlTETQ-ZBuYRL1EaCJcrEyIPM0c_uIYw","authorship_tag":"ABX9TyPreJ5u8XdRyQxHMudxrEeN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}