{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hjJhdcbjAdCm"},"outputs":[],"source":["#필요라이브러리 import\n","import numpy as np\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import pyedflib\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import sys, math ,os\n","import pickle\n","from pywt import wavedec\n","import pywt\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRcmnqBVAdCr"},"outputs":[],"source":["#데이터셋 디렉터리 및 파일 수 확인\n","\n","dir_list = [os.getcwd() + \"/dataset/2_Test\",os.getcwd() + \"/dataset/1_Train+Val\"]\n","for i, dataset_dir in enumerate(dir_list):\n","    file_list = os.listdir(dataset_dir)\n","    file_list_py = [file for file in file_list if file.endswith(\".edf\")]\n","    file_list_py.sort()\n","    print(f\"{dir_list[i]} 파일 개수 : {len(file_list_py)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvxsjYboAdCs"},"outputs":[],"source":["#데이터셋 라벨링 OSA : 1 / Normal : 0\n","\n","file_labels = dict()\n","for filename in os.listdir(\"/home/maic-player/dataset/1_Train+Val\"):\n","    file_labels.update({filename:1 if \"OSA\" in filename else 0})\n","# print(file_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN1nmhqqAdCs"},"outputs":[],"source":["# 데이터셋 duration min,max,average 확인\n","\n","duration_list = list()\n","for f_name in file_list_py:\n","    edf = pyedflib.EdfReader(dataset_dir + \"/\"+  f_name)\n","    duration_list += [edf.getFileDuration()]\n","    edf.close()\n","plt.plot(duration_list,label=f_name)\n","plt.show()\n","print(\n","    f'''\n","    최소 시간: {min(duration_list)/3600}시간({min(duration_list)})\n","    최대 시간: {max(duration_list)/3600}시간({max(duration_list)})\n","    평균 시간: {np.mean(duration_list)/3600}시간({np.mean(duration_list)})''')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5ExkDSsAdCt"},"outputs":[],"source":["#edf 내부 label 확인\n","labels = edf.getSignalLabels()\n","print(len(labels))\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDyzIDy6AdCt"},"outputs":[],"source":["dataset_dir + \"/\"+  file_list_py[80]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZQ1GoMhAdCu"},"outputs":[],"source":["edf_File = file_list_py[40]\n","edf = pyedflib.EdfReader(f\"{os.getcwd()}/dataset/1_Train+Val/{edf_File}\")\n","print(edf_File)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf4dAW6XAdCu"},"outputs":[],"source":["print(\"Duaration:\"+str(edf.getFileDuration())) # 파일 Duration 확인\n","print(\"Freq.:\"+str(edf.getSampleFrequencies())) # 파일 Frequen\n","print(\"N-Sample(=Freq x Duaration):\"+str(edf.getNSamples()))\n","print(\"Date:\"+str(edf.getStartdatetime()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXpOQq-5AdCv"},"outputs":[],"source":["edf.readSignal(0).size"]},{"cell_type":"markdown","metadata":{"id":"OYVi2luQAdCv"},"source":["1000개 -> 10초간 측정된 데이터\n","\n","최소 시간: 1.25시간(4500)\n","\n","최대 시간: 12.25시간(44100)\n","\n","평균 시간: 6.291944444444445시간(22651.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdFAie3GAdCx"},"outputs":[],"source":["# xyz plot\n","ten_sec=1000 # 멜스펙토그램 기준 1000 -> 10초\n","for i in range(0,3):\n","    plt.plot(edf.readSignal(i)[10*ten_sec:20*ten_sec], # 100초 ~ 200초 사이 signal 데이터 읽어 보기\n","             label=labels[i])\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nN5PYHInAdCy"},"outputs":[],"source":["# ECG plot\n","index = 3 # index 3번 ECG 데이터\n","plt.plot(edf.readSignal(index)[10*ten_sec:20*ten_sec], # 100초 ~ 200초 사이 signal 데이터 읽어 보기\n","         label=labels[index])\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAenTEPDAdCy"},"outputs":[],"source":["# mel freq plot\n","for i in range(4, 24): # index 4~24 까지 멜스팩토그램 20개 채널\n","    plt.plot(edf.readSignal(i)[10*ten_sec:60*ten_sec], # 100초 ~ 200초 사이 signal 데이터 읽어 보기\n","             label=labels[i])\n","# plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5EG9BCJAdCy"},"outputs":[],"source":["print(\"Anotation:\"+str(edf.read_annotation()))\n","print(\"Technician:\"+str(edf.getTechnician()))\n","print(\"Header:\"+str(edf.getHeader()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErFDfZ4fAdCy"},"outputs":[],"source":["print(\"SIgnal-Header(CH-0):\"+str(edf.getSignalHeader(0))+\"¥n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvKybrMLAdCz"},"outputs":[],"source":["print(edf.file_info())\n","edf.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhkZCm14AdCz"},"outputs":[],"source":["# torch gpu 환경 테스트\n","print(torch.backends.cudnn.is_available())\n","print(torch.backends.cudnn.version())\n","print(torch.backends.cudnn.enabled)\n","print(torch.cuda.get_device_properties(\"cuda\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX5fLzWhAdCz"},"outputs":[],"source":["# ecg 심전도 데이터 노이즈 처리(레퍼런스 : https://www.kaggle.com/code/nelsonsharma/ecg-02-ecg-signal-pre-processing)\n","def denoise_signal(X, dwt_transform, dlevels, cutoff_low, cutoff_high):\n","    coeffs = wavedec(X, dwt_transform, level=dlevels)   # wavelet transform 'bior4.4'\n","    # scale 0 to cutoff_low\n","    for ca in range(0,cutoff_low):\n","        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n","    # scale cutoff_high to end\n","    for ca in range(cutoff_high, len(coeffs)):\n","        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n","    Y = pywt.waverec(coeffs, dwt_transform) # inverse wavelet transform\n","    return Y\n","\n","# edf 데이터 호출\n","def get_edf_data(default_path:str):\n","    data_dict = {\n","     \"mov\":list(),\n","     \"ecg\":list(),\n","     \"mel\":list(),\n","     \"label\":list()\n","    }\n","    file_list_py = os.listdir(default_path)\n","    for flp in tqdm(file_list_py):\n","        if flp.endswith(\".edf\"):\n","            edf = pyedflib.EdfReader(f\"{default_path}/{flp}\") #edf 데이터 읽기\n","            #print(file_list_py[k+71]\n","            X_axis = edf.readSignal(0) #액티그래피 x축 데이터 변수저장\n","            Y_axis = edf.readSignal(1) #액티그래피 y축 데이터 변수저장\n","            Z_axis = edf.readSignal(2) #액티그래피 z축 데이터 변수저장\n","            Mov_volume_list = [math.sqrt(math.pow(X_axis[j], 2) + math.pow(Y_axis[j], 2) + math.pow(Z_axis[j], 2)) \\\n","                               for j in range(len(X_axis))] #x,y,z 데이터 1차원의 움직임 값으로 변환공식\n","            freq = edf.getSampleFrequencies()[0]\n","            xyz_n_sample = edf.getNSamples()[0]\n","            xyz = list()\n","            for n in range(freq, xyz_n_sample, freq):\n","                xyz += [Mov_volume_list[n:n+freq]]\n","\n","            ECG_axis = edf.readSignal(3) #ECG 원천데이터 변수저장\n","            ECG_axis = denoise_signal(ECG_axis,'bior4.4', 9 , 1 , 7) #ECG 원천데이터 denoise 적용\n","            freq = edf.getSampleFrequencies()[3]\n","            ecg_n_sample = edf.getNSamples()[3]\n","            ecg = list()\n","            for n in range(freq, ecg_n_sample, freq):\n","                ecg += [ECG_axis[n:n+freq]]\n","\n","            MEL_axis = np.array([edf.readSignal(index) for index in range(4,9)]) #코골이 주파수 대역 멜스펙토그램 데이터 저장\n","            freq = edf.getSampleFrequencies()[4]\n","            mel_n_sample = edf.getNSamples()[4]\n","            mel = list()\n","            for n in range(freq, mel_n_sample, freq):\n","                mel += [MEL_axis[:,n:n+freq]]\n","            freq_list = list()\n","            freq_list = [[np.array(m)[:,idex].sum() for idex in range(freq)] for m in np.array(mel)]\n","            # for i in range(freq):\n","                # pcaed += [pca.fit_transform([m[:,i] for m in mel]).tolist()]\n","            edf.close()\n","\n","            Mov_volume_list = np.array(xyz) #액티그래피 1차원 움직임 List\n","            ECG_volume_list = np.array(ecg) #Denoise 된 ECG 데이터 List\n","            MEL_volume_list = np.array(freq_list) #멜 스펙토그램 List\n","            data_dict[\"mov\"] += [[Mov_volume_list[i,:].sum() for i in range(len(Mov_volume_list))]] #1차원 움직임 dict\n","            data_dict[\"ecg\"] += [[ECG_volume_list[i,:].sum() for i in range(len(Mov_volume_list))]] #Denoise ECG dict\n","            data_dict[\"mel\"] += [[MEL_volume_list[i,:].sum() for i in range(len(Mov_volume_list))]] #멜스펙토그램 dict\n","            data_dict[\"label\"] += [1 if \"OSA\" in flp else 0] #OSA : 1 / Normal : 0 라벨 Dict\n","    return data_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCyWxG14AdC0"},"outputs":[],"source":["# 1.Train+Val 디렉토리 edf 파일 읽어오기\n","df = get_edf_data(default_path = os.getcwd() + \"/dataset/1_Train+Val\")\n","# 2.Test 디렉토리 edf 파일 읽어오기\n","Test = get_edf_data(default_path = os.getcwd() + \"/dataset/2_Test\")\n","\n","# 새로운 dataframe 저장을 위한 dict형의 변수\n","data_dict = {\n"," \"mov\":list(),\n"," \"ecg\":list(),\n"," \"mel\":list(),\n"," \"label\":list()\n","}\n","dur = 450\n","# Train 데이터만 최소 duration 450 기준으로 split하여 301개 데이터에서 1888개로 확장\n","for index, row in tqdm(df.iterrows(),\n","                       total = len(df)):\n","    start = 0\n","    # 0에서 부터 duration에 해당하는 데이터까지 반복하여 새로운 데이터로 가져옴\n","    while len(row['mov'][start:start+dur]) >= dur:\n","        data_dict[\"mov\"] += [row['mov'][start:start+dur]]\n","        data_dict[\"ecg\"] += [row['ecg'][start:start+dur]]\n","        data_dict[\"mel\"] += [row['mel'][start:start+dur]]\n","        data_dict[\"label\"] += [row['label']]\n","        start += dur\n","df = pd.DataFrame(data_dict)\n","\n","# Test 데이터의 경우 450까지만 가져오도록 함\n","for index, row in tqdm(Test.iterrows(),\n","                       total = len(df)):\n","    start = 0\n","    row['mov'] = row['mov'][start:start+dur]\n","    row['ecg'] = row['ecg'][start:start+dur]\n","    row['mel'] = row['mel'][start:start+dur]\n","\n","# Train 80 : Valid 20의 비율로 data spilte\n","Train, Valid, _, _ = train_test_split(df,\n","                                      df.label,\n","                                      stratify=df.label,\n","                                      test_size=0.2,\n","                                      random_state=42)\n","\n","# Valid 데이터를 통한 정확한 성능 측정을 위해 Valid 데이터는 OSA와 Normal 비율을 50:50으로 맞춤\n","Valid = pd.concat([Valid[Valid.label==1].sample(Valid.label.value_counts()[0]),\n","                   Valid[Valid.label==0]], ignore_index=True)\n","\n","# 빠르게 데이터를 읽어오기 위해 DataFrame 형태 그대로 저장\n","with open(\"train.pk\",\"wb\") as f:\n","    pickle.dump(Train, f)\n","    f.close()\n","with open(\"valid.pk\",\"wb\") as f:\n","    pickle.dump(Valid, f)\n","    f.close()\n","with open(\"test.pk\",\"wb\") as f:\n","    pickle.dump(Test, f)\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvGpErjLAdC0"},"outputs":[],"source":["%%writefile model.py\n","from torch import nn\n","\n","# mov, ecg, mel 세 개의 데이터를 읽어와서 처리하는 CNN 구조의 모델 정의\n","class Model(nn.Module):\n","    def __init__(self,\n","                 device=\"cpu\"):\n","        super().__init__()\n","        self.device=device\n","        self.cnn = nn.Sequential(\n","                    nn.Conv2d(in_channels=1,\n","                              out_channels=10,\n","                              stride=3,\n","                              kernel_size=(3, 3)),\n","                    nn.Dropout(p=0.3),\n","                    nn.Conv2d(in_channels=10,\n","                              out_channels=20,\n","                              stride=2,\n","                              kernel_size=(1, 5)),\n","                    nn.Dropout(p=0.3),\n","                    nn.Conv2d(in_channels=20,\n","                              out_channels=20,\n","                              stride=2,\n","                              kernel_size=(1, 3)),\n","                    nn.Dropout(p=0.3),\n","                    nn.Conv2d(in_channels=20,\n","                              out_channels=10,\n","                              stride=3,\n","                              kernel_size=(1, 2)),\n","                    nn.ReLU(),\n","                    nn.BatchNorm2d(10),\n","                    nn.MaxPool2d((1, 1),\n","                                 stride=2),\n","                    nn.Dropout(p=0.3),\n","                    nn.Conv2d(in_channels=10,\n","                              out_channels=1,\n","                              stride=2,\n","                              kernel_size=(1, 1)),\n","                    nn.Flatten()\n","                    )\n","        self.mlp = nn.Sequential(\n","            nn.Linear(3, 2),\n","            nn.Linear(2, 2),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self,\n","                inputs,\n","                return_dict=False):\n","        outputs = self.cnn(inputs)\n","        return  self.mlp(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGF5H46oAdC1"},"outputs":[],"source":["%%writefile trainer.py\n","import argparse\n","import logging\n","import os\n","import gc\n","import numpy as np\n","import pandas as pd\n","import time\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import pytorch_lightning as pl\n","from pytorch_lightning import loggers as pl_loggers\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import (TQDMProgressBar,\n","                                         StochasticWeightAveraging,\n","                                         DeviceStatsMonitor)\n","from torchmetrics.classification import BinaryF1Score, BinaryAccuracy\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.optim import AdamW\n","from torchmetrics.text.bleu import BLEUScore\n","from tqdm.auto import tqdm\n","import pyedflib\n","import datetime\n","from model import Model\n","import pickle\n","import torch.distributed as dist\n","\n","# Use address of one of the machines\n","\n","# 주피터 환경에서 gpu 사용하는 경우 kernel에서 계속 gpu 메모리를 점유하고 반환하지 않기 때문에,\n","# 학습에 대한 다양한 접근을 해보기 어렵기 때문에 수행 종료후 메모리 반환을 하는\n","# .py 수행으로 학습을 진행\n","# cls level에서 하이퍼파라미터 입력을 가능하게 하기 위한 argparse\n","parser = argparse.ArgumentParser(description='OSA TRAINER')\n","# 기본 checkpoint 경로 지정\n","parser.add_argument('--checkpoint_path',\n","                    type=str,\n","                    help='checkpoint path')\n","# torch에서 gpu 인식되는 경우 gpu 사용. 아닌 경우 cpu로 연산\n","device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","\n","# 로거 지정\n","logger = logging.getLogger()\n","# INFO 수준의 log도 출력\n","logger.setLevel(logging.INFO)\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self,\n","                 is_train:bool = True,\n","                 device:str = \"cpu\"):\n","        self.device = device\n","        if is_train:\n","            # train 모드일 때 train 데이터 프레임 가져옴\n","            with open(\"train.pk\", \"rb\")as f:\n","                self.dataset = pickle.load(f)\n","        else:\n","            # valid 모드일 때 valid 데이터 프레임 가져옴\n","            with open(\"valid.pk\", \"rb\")as f:\n","                self.dataset = pickle.load(f)\n","\n","\n","    def __getitem__(self,\n","                    index: int):\n","        # item iteration 마다 수행하게 됨\n","        example = self.dataset.iloc[index,:]\n","        # list 형 데이터 numpy arry float 32 타입으로 수정\n","        data = np.array(example.data, dtype=np.float32)\n","        # int 형 라벨 numpy arry long 타입으로 로 수정\n","        label = np.array(example.label, dtype=np.int_)\n","        # 모델에 바로 입력하기 위해 torch tensor 형태로 반환\n","        return {\n","            \"data\":torch.tensor(data),\n","            \"label\":torch.tensor(label)\n","        }\n","\n","    def __len__(self) -> int:\n","        # dataset에 대한 길이 반환\n","        return len(self.dataset)\n","\n","\n","class ArgsBase():\n","    @staticmethod\n","    def add_model_specific_args(parent_parser):\n","        parser = argparse.ArgumentParser(\n","            parents=[parent_parser], add_help=False)\n","        # 기본 train data 경로\n","        parser.add_argument('--train_file',\n","                            type=str,\n","                            default='train.pk',\n","                            help='train file')\n","        # 기본 valid data 경로\n","        parser.add_argument('--test_file',\n","                            type=str,\n","                            default='valid.pk',\n","                            help='test file')\n","\n","        return parser\n","\n","\n","class OSADataModule(pl.LightningDataModule):\n","    def __init__(self,\n","                 batch_size=64,\n","                 num_workers=8):\n","        super().__init__()\n","        # argument에서 입력하는 batch size를 받아옴. 기본은 64\n","        self.batch_size = batch_size\n","        # argument에서 입력하는 dataloader workers 받아옴. 기본은 8\n","        self.num_workers = num_workers\n","        # 사용하는 gpu 노드 당 데이터를 load\n","        self.prepare_data_pre_node = True\n","    @staticmethod\n","    def add_model_specific_args(parent_parser):\n","        parser = argparse.ArgumentParser(\n","            parents=[parent_parser], add_help=False)\n","        parser.add_argument('--num_workers',\n","                            type=int,\n","                            default=8,\n","                            help='num of worker for dataloader')\n","        return parser\n","\n","    def setup(self, stage):\n","        # 모듈 수행 시 train 데이터에 대한 dataset 객체 생성\n","        self.train = CustomDataset(is_train=True,\n","                                   device=device)\n","        # 모듈 수행 시 validaion 데이터에 대한 dataset 객체 생성\n","        self.test = CustomDataset(is_train=False,\n","                                  device=device)\n","\n","    # train data loader 함수\n","    def train_dataloader(self):\n","        return DataLoader(self.train,\n","                          batch_size=self.batch_size, # 한번에 batch size만큼 데이터를 load\n","                          num_workers=self.num_workers, # data loader 수행하는 process 수\n","                          pin_memory=True, # 메모리에 dataloaer 값 미리 고정시킴\n","                          shuffle=True) # 데이터가져올 때 shuffle 수행\n","    # validation data loader 함수\n","    def val_dataloader(self):\n","        return DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False)\n","    # test data loader 함수\n","    def test_dataloader(self):\n","        return DataLoader(self.test,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          shuffle=False)\n","\n","\n","class Base(pl.LightningModule):\n","    def __init__(self, hparams, **kwargs) -> None:\n","        super(Base, self).__init__()\n","        self.save_hyperparameters(hparams)\n","\n","    @staticmethod\n","    def add_model_specific_args(parent_parser):\n","        # add model specific args\n","        parser = argparse.ArgumentParser(\n","            parents=[parent_parser], add_help=False)\n","        # 기본 batch size 파라미터 지정\n","        parser.add_argument('--batch-size',\n","                            type=int,\n","                            default=8,\n","                            help='batch size for training (default: 96)')\n","        # 기본 learning rate 파라미터 지정\n","        parser.add_argument('--lr',\n","                            type=float,\n","                            default=5e-7,\n","                            help='The initial learning rate')\n","        # 기본 warmup ratio 지정\n","        parser.add_argument('--warmup_ratio',\n","                            type=float,\n","                            default=0.2,\n","                            help='warmup ratio')\n","\n","        return parser\n","\n","    def configure_optimizers(self):\n","        # Optimizer에서 Backward 수행할 Parameter 지정\n","        param_optimizer = list(self.model.named_parameters())\n","        # AdamW optimizer에서 no_decay로 지정되어야 하는 항목들\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(\n","                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(\n","                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        # AdamW Optimizer 선언\n","        optimizer = AdamW(optimizer_grouped_parameters,\n","                          lr = self.hparams.lr)\n","        # Optimizing process 수\n","        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n","        # Dataloader에서 읽어오는 dataset 길이\n","        data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n","        logging.info(f'number of workers {num_workers}, data length {data_len}')\n","        # Warmup 및 CosineAnnealing에 따른 Learning rate 조절을 위한 전체 Training Step 계산\n","        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n","        logging.info(f'num_train_steps : {num_train_steps}')\n","        # Warmup Ratio에 따라 warmup을 수행할 step 수 계산\n","        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n","        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n","        # CosineAnnealing Learning Rate Scheduler 선언\n","        scheduler = CosineAnnealingLR(optimizer=optimizer,\n","                                      T_max=num_train_steps,\n","                                      eta_min=5e-8)\n","        lr_scheduler = {'scheduler': scheduler,\n","                        'monitor': 'train_loss',\n","                        'interval': 'step',\n","                        'frequency': 1}\n","        return [optimizer], [lr_scheduler]\n","\n","\n","class MyTrainer(Base):\n","    def __init__(self, hparams, **kwargs):\n","        super(MyTrainer, self).__init__(hparams, **kwargs)\n","        # 모델 정의\n","        self.model = Model()\n","        # 이진 분류 output 2개에 대한 Loss Function\n","        self.loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([.79,.67]),\n","                                                 reduction=\"mean\")\n","        # 성능 측정을 위한 F1 Metric과 Accuracy\n","        self.metric = BinaryF1Score()\n","        self.acc = BinaryAccuracy()\n","\n","\n","    def forward(self, inputs):\n","        # input에 대한 Model 연산\n","        outputs = self.model(inputs=inputs['data'])\n","        # ouput의 예측값에 대한 argmax\n","        preds = torch.argmax(outputs, dim=1)\n","        # label 과 preds 사이의 loss 계산\n","        loss = self.loss_fn(outputs, inputs['label'])\n","        return {\n","            \"loss\": loss,\n","            \"logits\": outputs,\n","            \"preds\": preds\n","        }\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        # batch 데이터 trainer에 넘기기\n","        outs = self(batch)\n","        # 예측값에 대한 f1 metric 계산\n","        metric = self.metric(outs[\"preds\"], batch['label'])\n","        # 예측값에 대한 accuracy 계산\n","        acc = self.acc(outs[\"preds\"], batch['label'])\n","        # loss, metric, acc log로 출력\n","        self.log('train_loss', outs[\"loss\"], prog_bar=True, on_step=True, on_epoch=True)\n","        self.log('train_acc', acc, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n","        self.log('train_f1', metric, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n","        # loss Backward를 위한 loss 값 반환\n","        return outs[\"loss\"]\n","\n","    def validation_step(self, batch, batch_idx):\n","        # batch 데이터 trainer에 넘기기\n","        outs = self(batch)\n","        # 예측값에 대한 f1 metric 계산\n","        metric = self.metric(outs[\"preds\"], batch['label'])\n","        # 예측값에 대한 accuracy 계산\n","        acc = self.acc(outs[\"preds\"], batch['label'])\n","        # loss, metric, acc log로 출력\n","        self.log('val_loss', outs[\"loss\"], prog_bar=True, on_step=False, on_epoch=True, logger=True)\n","        self.log('val_acc', acc, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n","        self.log('val_f1', metric,prog_bar=True, on_step=False, on_epoch=True, logger=True)\n","\n","\n","if __name__ == '__main__':\n","    # Pytorch GPU 사용을 위한 환경변수 수정\n","    os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n","    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n","    os.environ[\"NCCL_DEBUG\"] = \"WARNNING\"\n","    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","    os.environ[\"WORLD_SIZE\"] = \"3\"\n","    os.environ[\"LOCAL_RANK\"] = \"0\"\n","    # torch 랜덤 시드 값 고정\n","    torch.cuda.manual_seed_all(seed=42)\n","    # 기본 Trainer Argument 값 지정 및 추가\n","    parser = Base.add_model_specific_args(parser)\n","    parser = ArgsBase.add_model_specific_args(parser)\n","    parser = OSADataModule.add_model_specific_args(parser)\n","    # Trainer에서 Validation 데이터셋 F1 score 모니터링을 통한 Early Stopping callback 추가\n","    parser = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_f1\",\n","                                                 stopping_threshold=1e-4,\n","                                                 min_delta=0.00,\n","                                                 patience=10,\n","                                                 divergence_threshold=9.0),\n","                                   ]).add_argparse_args(parser)\n","    # argument 읽어오기\n","    args = parser.parse_args()\n","    # logging.info(args)\n","    model = MyTrainer(args)\n","    # Data Loader 모듈에 Batch size와 loader process 수 지정\n","    dm = OSADataModule(batch_size=args.batch_size,\n","                       num_workers=args.num_workers)\n","    # Validation 데이터 셋 F1 score 기준으로 best 모델 저장\n","    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_f1',\n","                                                       dirpath=args.default_root_dir,\n","                                                       filename='best-checkpoint',\n","                                                       verbose=True,\n","                                                       save_last=True,\n","                                                       mode='max',\n","                                                       save_top_k=1,)\n","    # 학습 모니터링을 위한 TensorBoard logger 선언\n","    tb_logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir,'tb_logs'),\n","                                             log_graph=False,)\n","    # 학습 중 모델의 학습에 대한 모니터링을 위한 WandB 플랫폼 logger 연결\n","    # wandb_logger = pl_loggers.WandbLogger(name=\"MAIC\",\n","    #                                       project=\"Sleep AI Challenge ver3\",\n","    #                                       log_model=\"all\")\n","    # LearningRate 모니터링 callback 추가\n","    lr_logger = pl.callbacks.LearningRateMonitor()\n","    # Trainer에 받아온 Argument값 넣기\n","    trainer = pl.Trainer.from_argparse_args(args,\n","                                            logger=[tb_logger,\n","                                                    # wandb_logger # training 중 모니터링을 위한 wandb 플랫폼 로그는 제출시 주석 처리 하였습니다.\n","                                                   ],\n","                                            callbacks=[checkpoint_callback,\n","                                                       lr_logger])\n","    # Trainer 모델 학습 수행\n","    trainer.fit(model, dm, )\n","    # 학습 종료 시점의 Model 추가 저장\n","    save_path = f'logs/osa{datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")}.pt'\n","    torch.save(model.model.state_dict(), save_path)\n","    print(f'### torch  model.pth has saved at {save_path} ###')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZ0Ddu7gAdC2"},"outputs":[],"source":["# 주피터 커널이 GPU를 점유하고 있어 python 파일로 실행할 수 있게 함\n","!python \\\n","    trainer.py\\\n","        --gradient_clip_val 1.0\\\n","        --max_epochs 50\\\n","        --lr 5e-3\\\n","        --batch-size 16\\\n","        --precision 32\\\n","        --default_root_dir logs\\\n","        --accelerator gpu\\\n","        --devices -1\\\n","        --strategy ddp\\\n","        --num_nodes 1\\\n","        --num_workers 1\\\n","        --auto_select_gpus true\\\n","        --replace_sampler_ddp true\\\n","        --sync_batchnorm true\\\n","        --move_metrics_to_cpu false\\\n","        --profiler simple\\\n","        --enable_model_summary true\\\n","        --amp_backend native\\\n","        --fast_dev_run false"]},{"cell_type":"markdown","metadata":{"id":"wnh5AdVPAdC3"},"source":["제출 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oT-zot7VAdC3"},"outputs":[],"source":["from model import Model\n","from trainer import *\n","import torch\n","import easydict\n","from tqdm.auto import tqdm\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","args = easydict.EasyDict({\n","    \"gradient_clip_val\": 1.0,\n","    \"max_epochs\": 50,\n","    \"lr\": 5e-4,\n","    \"batch_size\": 64,\n","    \"precision\": 32,\n","    \"default_root_dir\": \"logs\",\n","    \"num_workers\":0,\n","})\n","# Trainer 객체 선언을 위한 기본 argument 값 넣기\n","model = MyTrainer(args)\n","# 학습한 Trainer에 모델 state_dict 값 로드\n","model.load_state_dict(torch.load(\"BACKUP/logs/best-checkpoint-v50.ckpt\")[\"state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mALutPcxAdC3"},"outputs":[],"source":["# 테스트 Dataframe pickle 파일 로드\n","with open(\"test.pk\",\"rb\") as f:\n","    submit_df = pickle.load(f)\n","    f.close()\n","\n","# 제출용 답안 작성 파일명 Test_Answer_Form.csv 파일을 읽어서 모델을 통한 정답 마킹\n","a = pd.read_csv(\"Test_Answer_Form.csv\", names=[\"name\"])\n","for index,row in tqdm(submit_df.iterrows(),\n","                      total=len(submit_df),\n","                      desc=\"marking labels....\"):\n","    # 테스트 데이터 값을 입력 가능한 torch.tensor의 형태로 변환\n","    inputs = torch.tensor(np.array([row[\"data\"]], dtype=np.float32))\n","    # trainer의 모델을 통한 output 연산\n","    output = model.model.forward(inputs)\n","    # 25, 50, 75 인덱스에 대해 결과 연산에 문제가 없는지 확인\n","    if index in [25,50,75]:\n","        print(output)\n","    # [1,2] 사이즈의 tensor에서 argmax 연산을 통해 binary class 매핑\n","    label=\"OSA\" if output.argmax(1) else \"Normal\"\n","    a.loc[index,\"label\"] = label\n","\n","# 제출용 csv 파일로 저장\n","a.to_csv(\"Submissions.csv\",\n","         header=None,\n","         index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}