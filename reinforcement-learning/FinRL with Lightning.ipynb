{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Oe885BwyTllvI32f1NLasD0eN_o9XCKl","authorship_tag":"ABX9TyPRSlrA+//NxN/UtFUOkzFL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBlFwUkF65rB","executionInfo":{"status":"ok","timestamp":1699538528756,"user_tz":-540,"elapsed":11265,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"6275c524-a68a-469e-a4ab-5b3d62554c73"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package libgl1-mesa-glx:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package swig4.0.\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install -q condacolab wrds swig\n","!pip install lightning\n","!pip install cohere openai tiktoken\n","# import condacolab\n","# condacolab.install()\n","!pip install -U git+https://github.com/AI4Finance-Foundation/FinRL.git"],"metadata":{"id":"a0-GLiGB4VIn","executionInfo":{"status":"ok","timestamp":1699538713353,"user_tz":-540,"elapsed":184600,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","import lightning.pytorch as pl\n","import finrl\n","from finrl import config\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n","from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n","from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n","from finrl.agents.stablebaselines3.models import DRLAgent, DRLEnsembleAgent\n","from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n","from torch.utils.data.dataset import IterableDataset\n","from torch.utils.data import DataLoader\n","from typing import Iterator, List, Tuple\n","\n","class FinRLLightning(pl.LightningModule):\n","    def __init__(self, start_date, end_date, ticker_list, total_timesteps):\n","        super(FinRLLightning, self).__init__()\n","\n","        # Initialize and configure the custom environment\n","        self.df = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=ticker_list).fetch_data()\n","        self.fe = FeatureEngineer(\n","            use_technical_indicator=True,\n","            tech_indicator_list=config.INDICATORS,\n","            use_turbulence=False,\n","            user_defined_feature=False,\n","        )\n","        self.df = self.fe.preprocess_data(self.df)\n","\n","        # Split data into training and evaluation sets\n","        self.trade = data_split(self.df, start_date, end_date)\n","\n","        stock_dimension = len(self.trade.tic.unique())\n","        state_space = 1 + 2 * stock_dimension + len(config.INDICATORS) * stock_dimension\n","        self.env_kwargs = {\n","            \"hmax\": 100,\n","            \"num_stock_shares\": [0] * stock_dimension,\n","            \"initial_amount\": 1000000,\n","            \"buy_cost_pct\": 0.001,\n","            \"sell_cost_pct\": 0.001,\n","            \"state_space\": state_space,\n","            \"stock_dim\": stock_dimension,\n","            \"tech_indicator_list\": config.INDICATORS,\n","            \"action_space\": stock_dimension,\n","            \"reward_scaling\": 1e-4,\n","        }\n","        self.e_train_gym = StockTradingEnv(df=self.trade, **self.env_kwargs)\n","\n","        # Initialize and configure the DRL agent\n","        self.agent = DRLAgent(model_name=\"ddpg\", env=self.e_train_gym)\n","\n","        self.total_timesteps = total_timesteps\n","\n","    # def train_dataloader(self):\n","    #     # Implement your data loading logic here.\n","    #     # Load and preprocess the training data.\n","    #     train_data = self.trade  # Assuming 'self.train' contains your training data\n","    #     dataset = Dataset(train_data)\n","    #     dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    #     return dataloader\n","\n","    def forward(self, x):\n","        # Define forward pass (if needed)\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","        # Define the training logic\n","        trained_model = self.agent.train_model(\"ppo\", total_timesteps=self.total_timesteps)\n","        return None\n","\n","    def configure_optimizers(self):\n","        # You don't need an optimizer for reinforcement learning typically\n","        return []\n","\n","class RLDataset(IterableDataset):\n","    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n","\n","    Args:\n","        buffer: replay buffer\n","        sample_size: number of experiences to sample at a time\n","    \"\"\"\n","\n","    def __init__(self, buffer, sample_size: int = 1) -> None:\n","        self.buffer = buffer\n","        self.sample_size = sample_size\n","\n","    def __iter__(self) -> Iterator[Tuple]:\n","        states, actions, rewards, dones, new_states = self.buffer.sample(batch_size=self.sample_size)\n","        for i in range(len(dones)):\n","            yield states[i], actions[i], rewards[i], new_states[i], dones[i]\n","\n","# Create a lightning module to train the agent\n","class StockTradingModule(pl.LightningModule):\n","    def __init__(self, start_date, end_date, ticker_list, total_timesteps):\n","        super(StockTradingModule, self).__init__()\n","\n","        # Initialize and configure the custom environment\n","        self.df = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=ticker_list).fetch_data()\n","        self.fe = FeatureEngineer(\n","            use_technical_indicator=True,\n","            tech_indicator_list=config.INDICATORS,\n","            use_turbulence=False,\n","            user_defined_feature=False,\n","        )\n","        self.df = self.fe.preprocess_data(self.df)\n","\n","        # Split data into training and evaluation sets\n","        self.trade = data_split(self.df, start_date, end_date)\n","        stock_dimension = len(self.trade.tic.unique())\n","        state_space = 1 + 2 * stock_dimension + len(config.INDICATORS) * stock_dimension\n","        self.env_kwargs = {\n","            \"hmax\": 100,\n","            \"num_stock_shares\": [0] * stock_dimension,\n","            \"initial_amount\": 1000000,\n","            \"buy_cost_pct\": 0.001,\n","            \"sell_cost_pct\": 0.001,\n","            \"state_space\": state_space,\n","            \"stock_dim\": stock_dimension,\n","            \"tech_indicator_list\": config.INDICATORS,\n","            \"action_space\": stock_dimension,\n","            \"reward_scaling\": 1e-4,\n","        }\n","        self.e_train_gym = StockTradingEnv(df=self.trade, **self.env_kwargs)\n","\n","        # Initialize and configure the DRL agent\n","        self.agent = DRLAgent(env=self.e_train_gym)\n","        self.model = self.agent.get_model(model_name=\"ddpg\")\n","        self.total_timesteps = total_timesteps\n","        self.model.replay_buffer.full = True\n","\n","        self.automatic_optimization = False\n","\n","        display(dir(self.model))\n","\n","    def forward(self, state):\n","        # Return the action given the state\n","        return self.model.act(state)\n","\n","    def training_step(self, batch, batch_idx):\n","        # Perform one step of training\n","        state, action, reward, next_state, done = batch\n","        loss = self.model.learn(state, action, reward, next_state, done)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        # Return the optimizers for actor and critic networks\n","        return [self.model.actor.optimizer, self.model.critic.optimizer]\n","\n","    # def configure_optimizers(self):\n","    #     # You don't need an optimizer for reinforcement learning typically\n","    #     return []\n","\n","    def train_dataloader(self):\n","        # Return a dataloader for training\n","        return DataLoader(\n","            dataset=RLDataset(buffer=self.model.replay_buffer),\n","            batch_size=self.model.batch_size,\n","            num_workers=2,)\n","\n","\n","if __name__ == '__main__':\n","    # Set your parameters here\n","    start_date = \"2000-01-01\"\n","    end_date = \"2022-01-01\"\n","    ticker_list = [\"AAPL\", \"MSFT\"]\n","    total_timesteps = 20000\n","\n","    # Create the Lightning model and trainer\n","    # model = FinRLLightning(start_date, end_date, ticker_list, total_timesteps)\n","    # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1)\n","\n","    # Train the model\n","    # trainer.fit(model)\n","\n","    # Run backtest and evaluate the model\n","    # e_trade_gym = StockTradingEnv(df=model.trade, turbulence_threshold=250.0, **model.env_kwargs)\n","    # df_account_value, df_actions = DRLAgent.DRL_prediction(model=model.agent, environment=e_trade_gym)\n","    # Continue with backtesting analysis...\n","    # Create a trainer object\n","    trainer = pl.Trainer(max_epochs=10,  # number of epochs to train\n","                        accelerator=\"gpu\",  # number of GPUs to use\n","                        logger=TensorBoardLogger('logs/'),  # logger for tensorboard\n","                        callbacks=[pl.callbacks.ModelCheckpoint('checkpoints/'),\n","                                   pl.callbacks.TQDMProgressBar(refresh_rate=3)],  # checkpoint callback for saving models\n","                        )  # refresh rate of progress bar\n","\n","    # Train the agent\n","    trainer.fit(StockTradingModule(start_date, end_date, ticker_list, total_timesteps))"],"metadata":{"id":"niDk_4SCyCMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DPTGcjfGPTfC"},"execution_count":null,"outputs":[]}]}