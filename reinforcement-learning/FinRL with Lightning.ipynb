{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Oe885BwyTllvI32f1NLasD0eN_o9XCKl","authorship_tag":"ABX9TyMrT2Cl/peg9/ZI1dXIXOx/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBlFwUkF65rB","executionInfo":{"status":"ok","timestamp":1699410513470,"user_tz":-540,"elapsed":15383,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"7f84fc32-010f-4bc4-e677-4e8267f03c23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package libgl1-mesa-glx:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package swig4.0.\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install -q condacolab wrds swig\n","!pip install lightning\n","!pip install cohere openai tiktoken\n","# import condacolab\n","# condacolab.install()\n","!pip install -U git+https://github.com/AI4Finance-Foundation/FinRL.git"],"metadata":{"id":"a0-GLiGB4VIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import lightning.pytorch as pl\n","import finrl\n","from finrl import config\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n","from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n","from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n","from finrl.agents.stablebaselines3.models import DRLAgent, DRLEnsembleAgent\n","from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n","from torch.utils.data.dataset import IterableDataset\n","from torch.utils.data import DataLoader\n","from typing import Iterator, List, Tuple\n","\n","class FinRLLightning(pl.LightningModule):\n","    def __init__(self, start_date, end_date, ticker_list, total_timesteps):\n","        super(FinRLLightning, self).__init__()\n","\n","        # Initialize and configure the custom environment\n","        self.df = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=ticker_list).fetch_data()\n","        self.fe = FeatureEngineer(\n","            use_technical_indicator=True,\n","            tech_indicator_list=config.INDICATORS,\n","            use_turbulence=False,\n","            user_defined_feature=False,\n","        )\n","        self.df = self.fe.preprocess_data(self.df)\n","\n","        # Split data into training and evaluation sets\n","        self.trade = data_split(self.df, start_date, end_date)\n","\n","        stock_dimension = len(self.trade.tic.unique())\n","        state_space = 1 + 2 * stock_dimension + len(config.INDICATORS) * stock_dimension\n","        self.env_kwargs = {\n","            \"hmax\": 100,\n","            \"num_stock_shares\": [0] * stock_dimension,\n","            \"initial_amount\": 1000000,\n","            \"buy_cost_pct\": 0.001,\n","            \"sell_cost_pct\": 0.001,\n","            \"state_space\": state_space,\n","            \"stock_dim\": stock_dimension,\n","            \"tech_indicator_list\": config.INDICATORS,\n","            \"action_space\": stock_dimension,\n","            \"reward_scaling\": 1e-4,\n","        }\n","        self.e_train_gym = StockTradingEnv(df=self.trade, **self.env_kwargs)\n","\n","        # Initialize and configure the DRL agent\n","        self.agent = DRLAgent(model_name=\"ddpg\", env=self.e_train_gym)\n","\n","        self.total_timesteps = total_timesteps\n","\n","    # def train_dataloader(self):\n","    #     # Implement your data loading logic here.\n","    #     # Load and preprocess the training data.\n","    #     train_data = self.trade  # Assuming 'self.train' contains your training data\n","    #     dataset = Dataset(train_data)\n","    #     dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    #     return dataloader\n","\n","    def forward(self, x):\n","        # Define forward pass (if needed)\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","        # Define the training logic\n","        trained_model = self.agent.train_model(\"ppo\", total_timesteps=self.total_timesteps)\n","        return None\n","\n","    def configure_optimizers(self):\n","        # You don't need an optimizer for reinforcement learning typically\n","        return []\n","\n","class RLDataset(IterableDataset):\n","    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n","\n","    Args:\n","        buffer: replay buffer\n","        sample_size: number of experiences to sample at a time\n","    \"\"\"\n","\n","    def __init__(self, buffer, sample_size: int = 1) -> None:\n","        self.buffer = buffer\n","        self.sample_size = sample_size\n","\n","    def __iter__(self) -> Iterator[Tuple]:\n","        states, actions, rewards, dones, new_states = self.buffer.sample(batch_size=self.sample_size)\n","        for i in range(len(dones)):\n","            yield states[i], actions[i], rewards[i], new_states[i], dones[i]\n","\n","# Create a lightning module to train the agent\n","class StockTradingModule(pl.LightningModule):\n","    def __init__(self, start_date, end_date, ticker_list, total_timesteps):\n","        super(StockTradingModule, self).__init__()\n","\n","        # Initialize and configure the custom environment\n","        self.df = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=ticker_list).fetch_data()\n","        self.fe = FeatureEngineer(\n","            use_technical_indicator=True,\n","            tech_indicator_list=config.INDICATORS,\n","            use_turbulence=False,\n","            user_defined_feature=False,\n","        )\n","        self.df = self.fe.preprocess_data(self.df)\n","\n","        # Split data into training and evaluation sets\n","        self.trade = data_split(self.df, start_date, end_date)\n","        stock_dimension = len(self.trade.tic.unique())\n","        state_space = 1 + 2 * stock_dimension + len(config.INDICATORS) * stock_dimension\n","        self.env_kwargs = {\n","            \"hmax\": 100,\n","            \"num_stock_shares\": [0] * stock_dimension,\n","            \"initial_amount\": 1000000,\n","            \"buy_cost_pct\": 0.001,\n","            \"sell_cost_pct\": 0.001,\n","            \"state_space\": state_space,\n","            \"stock_dim\": stock_dimension,\n","            \"tech_indicator_list\": config.INDICATORS,\n","            \"action_space\": stock_dimension,\n","            \"reward_scaling\": 1e-4,\n","        }\n","        self.e_train_gym = StockTradingEnv(df=self.trade, **self.env_kwargs)\n","\n","        # Initialize and configure the DRL agent\n","        self.agent = DRLAgent(env=self.e_train_gym)\n","        self.model = self.agent.get_model(model_name=\"ddpg\")\n","        self.total_timesteps = total_timesteps\n","        self.model.replay_buffer.full = True\n","        display(dir(self.model))\n","\n","    def forward(self, state):\n","        # Return the action given the state\n","        return self.model.act(state)\n","\n","    def training_step(self, batch, batch_idx):\n","        # Perform one step of training\n","        state, action, reward, next_state, done = batch\n","        loss = self.model.learn(state, action, reward, next_state, done)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        # Return the optimizers for actor and critic networks\n","        return [self.model.actor_optimizer, self.model.critic_optimizer]\n","\n","    # def configure_optimizers(self):\n","    #     # You don't need an optimizer for reinforcement learning typically\n","    #     return []\n","\n","    def train_dataloader(self):\n","        # Return a dataloader for training\n","        self.train = DataLoader(\n","            dataset=RLDataset(buffer=self.model.replay_buffer),\n","            batch_size=self.model.batch_size)\n","        return self.train\n","\n","\n","if __name__ == '__main__':\n","    # Set your parameters here\n","    start_date = \"2000-01-01\"\n","    end_date = \"2022-01-01\"\n","    ticker_list = [\"AAPL\", \"MSFT\"]\n","    total_timesteps = 20000\n","\n","    # Create the Lightning model and trainer\n","    # model = FinRLLightning(start_date, end_date, ticker_list, total_timesteps)\n","    # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1)\n","\n","    # Train the model\n","    # trainer.fit(model)\n","\n","    # Run backtest and evaluate the model\n","    # e_trade_gym = StockTradingEnv(df=model.trade, turbulence_threshold=250.0, **model.env_kwargs)\n","    # df_account_value, df_actions = DRLAgent.DRL_prediction(model=model.agent, environment=e_trade_gym)\n","    # Continue with backtesting analysis...\n","    # Create a trainer object\n","    trainer = pl.Trainer(max_epochs=10,  # number of epochs to train\n","                        accelerator=\"gpu\",  # number of GPUs to use\n","                        logger=TensorBoardLogger('logs/'),  # logger for tensorboard\n","                        callbacks=[pl.callbacks.ModelCheckpoint('checkpoints/'),\n","                                   pl.callbacks.TQDMProgressBar(refresh_rate=3)],  # checkpoint callback for saving models\n","                        )  # refresh rate of progress bar\n","\n","    # Train the agent\n","    trainer.fit(StockTradingModule(start_date, end_date, ticker_list, total_timesteps))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"niDk_4SCyCMD","executionInfo":{"status":"error","timestamp":1699369535206,"user_tz":-540,"elapsed":8148,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"0ce5a2c8-2dc1-42dc-b43c-9d599e13ad09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","INFO: GPU available: True (cuda), used: True\n","INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO: TPU available: False, using: 0 TPU cores\n","INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO: IPU available: False, using: 0 IPUs\n","INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO: HPU available: False, using: 0 HPUs\n","INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","Shape of DataFrame:  (11072, 8)\n","Successfully added technical indicators\n","{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n","Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]},{"output_type":"display_data","data":{"text/plain":["['__abstractmethods__',\n"," '__annotations__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__slots__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_abc_impl',\n"," '_convert_train_freq',\n"," '_create_aliases',\n"," '_current_progress_remaining',\n"," '_custom_logger',\n"," '_dump_logs',\n"," '_episode_num',\n"," '_episode_storage',\n"," '_excluded_save_params',\n"," '_get_policy_from_name',\n"," '_get_torch_save_params',\n"," '_init_callback',\n"," '_last_episode_starts',\n"," '_last_obs',\n"," '_last_original_obs',\n"," '_n_updates',\n"," '_num_timesteps_at_start',\n"," '_on_step',\n"," '_sample_action',\n"," '_setup_learn',\n"," '_setup_lr_schedule',\n"," '_setup_model',\n"," '_stats_window_size',\n"," '_store_transition',\n"," '_total_timesteps',\n"," '_update_current_progress_remaining',\n"," '_update_info_buffer',\n"," '_update_learning_rate',\n"," '_vec_normalize_env',\n"," '_wrap_env',\n"," 'action_noise',\n"," 'action_space',\n"," 'actor',\n"," 'actor_batch_norm_stats',\n"," 'actor_batch_norm_stats_target',\n"," 'actor_target',\n"," 'batch_size',\n"," 'buffer_size',\n"," 'collect_rollouts',\n"," 'critic',\n"," 'critic_batch_norm_stats',\n"," 'critic_batch_norm_stats_target',\n"," 'critic_target',\n"," 'device',\n"," 'env',\n"," 'ep_info_buffer',\n"," 'ep_success_buffer',\n"," 'gamma',\n"," 'get_env',\n"," 'get_parameters',\n"," 'get_vec_normalize_env',\n"," 'gradient_steps',\n"," 'learn',\n"," 'learning_rate',\n"," 'learning_starts',\n"," 'load',\n"," 'load_replay_buffer',\n"," 'logger',\n"," 'lr_schedule',\n"," 'n_envs',\n"," 'num_timesteps',\n"," 'observation_space',\n"," 'optimize_memory_usage',\n"," 'policy',\n"," 'policy_aliases',\n"," 'policy_class',\n"," 'policy_delay',\n"," 'policy_kwargs',\n"," 'predict',\n"," 'replay_buffer',\n"," 'replay_buffer_class',\n"," 'replay_buffer_kwargs',\n"," 'save',\n"," 'save_replay_buffer',\n"," 'sde_sample_freq',\n"," 'seed',\n"," 'set_env',\n"," 'set_logger',\n"," 'set_parameters',\n"," 'set_random_seed',\n"," 'start_time',\n"," 'target_noise_clip',\n"," 'target_policy_noise',\n"," 'tau',\n"," 'tensorboard_log',\n"," 'train',\n"," 'train_freq',\n"," 'use_sde',\n"," 'use_sde_at_warmup',\n"," 'verbose']"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-f802e813031a>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStockTradingModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         )\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;31m# strategy will configure model and move it to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/single_device.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0m_optimizers_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36msetup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_init_optimizers_and_lr_schedulers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\u001b[0m in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0moptim_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"configure_optimizers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptim_conf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-f802e813031a>\u001b[0m in \u001b[0;36mconfigure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Return the optimizers for actor and critic networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# def configure_optimizers(self):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DDPG' object has no attribute 'actor_optimizer'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DPTGcjfGPTfC"},"execution_count":null,"outputs":[]}]}